"Document Title",Authors,"Author Affiliations","Publication Title",Date Added To Xplore,"Publication Year","Volume","Issue","Start Page","End Page","Abstract","ISSN",ISBNs,"DOI",Funding Information,PDF Link,"Author Keywords","IEEE Terms","Mesh_Terms",Article Citation Count,Patent Citation Count,"Reference Count","License",Online Date,Issue Date,"Meeting Date","Publisher",Document Identifier
"A Multi-Task Deep Learning Framework Coupling Semantic Segmentation and Image Reconstruction for Very High Resolution Imagery","M. Papadomanolaki; K. Karantzalos; M. Vakalopoulou","Remote Sensing Laboratory, National Technical University of Athens, Greece; Remote Sensing Laboratory, National Technical University of Athens, Greece; CVN, CentraleSupélec, Université Paris-Saclay and INRIA Saclay, France",IGARSS 2019 - 2019 IEEE International Geoscience and Remote Sensing Symposium,"14 Nov 2019","2019","","","1069","1072","Semantic segmentation, especially for very high-resolution satellite data, is one of the pillar problems in the remote sensing community. Lately, deep learning techniques are the ones that set the state-of-the-art for a number of benchmark datasets, however, there are still a lot of challenges that need to be addressed, especially in the case of limited annotations. To this end, in this paper, we propose a novel framework based on deep neural networks that is able to address concurrently semantic segmentation and image reconstruction in an end to end training. Under the proposed formulation, the image reconstruction acts as a regularization, constraining efficiently the solution in the entire image domain. This self-supervised component helps significantly the generalization of the network for the semantic segmentation, especially in cases of a low number of annotations. Experimental results and the performed quantitative evaluation on the publicly available ISPRS (WGIII/4) dataset indicate the great potential of the developed approach.","2153-7003","978-1-5386-9154-0","10.1109/IGARSS.2019.8898133","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8898133","Deep learning;Fully-convolutional networks;Feature representations;Autoencoders;Limited annotations","Semantics;Image reconstruction;Image segmentation;Deep learning;Vegetation;Buildings;Automobiles","","6","","11","IEEE","14 Nov 2019","","","IEEE","IEEE Conferences"
"Multi-Scale Convolutional Neural Network for SAR Image Semantic Segmentation","Y. Duan; X. Tao; C. Han; X. Qin; J. Lu","Department of Information Science and Technology, University of Science and Technology of China, Hefei, China; Department of Information Science and Technology, University of Science and Technology of China, Hefei, China; Department of Information Science and Technology, University of Science and Technology of China, Hefei, China; Department of Information Science and Technology, University of Science and Technology of China, Hefei, China; Department of Information Science and Technology, University of Science and Technology of China, Hefei, China",2018 IEEE Global Communications Conference (GLOBECOM),"21 Feb 2019","2018","","","1","6","Although the recent success of convolutional neural networks (CNNs) greatly advance the semantic segmentation of the natural images, few work has focused on the remote sensing images, especially the synthetic aperture radar (SAR) images. Specifically, the existing methods do not consider the speckle noise of the SAR images and the multi-scale characteristics contained in the SAR images. In this paper, we propose a multiscale convolutional neural network (CNN) model for SAR image semantic segmentation. The multi-scale CNN model includes noise removal stage, convolutional stage, feature concatenation stage and classification stage. In particular, we construct a sparse representation loss function to obtain a clear SAR image in noise removal stage. Then, the multi-scale convolutional stage is employed to learn the multi-scale deep features. The concatenation stage is used to connect the features with different scales and depths. Finally, softmax classifier is developed to obtain the labels of the SAR images with the multi-scale CNN model being trained in an end-to-end way. The experimental results on synthetic and real SAR images demonstrate the effectiveness of the proposed method.","2576-6813","978-1-5386-4727-1","10.1109/GLOCOM.2018.8647657","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8647657","","Synthetic aperture radar;Image segmentation;Semantics;Training;Testing;Speckle;Convolution","","11","","16","IEEE","21 Feb 2019","","","IEEE","IEEE Conferences"
"PointNest: Learning Deep Multiscale Nested Feature Propagation for Semantic Segmentation of 3-D Point Clouds","J. Wan; Z. Zeng; Q. Qiu; Z. Xie; Y. Xu","Key Laboratory of Geological and Evaluation of Ministry of Education, China University of Geosciences, Wuhan, China; State Key Laboratory of Information Engineering in Surveying, Mapping and Remote Sensing, Wuhan University, Wuhan, China; School of Computer Science, Wuhan, China; School of Computer Science, China University of Geosciences, Wuhan, China; School of Computer Science, Wuhan, China",IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing,"5 Oct 2023","2023","16","","9051","9066","3-D point cloud semantic segmentation is a fundamental task for scene understanding, but this task remains challenging due to the diverse scene classes, data defects, and occlusions. Most existing deep learning-based methods focus on new designs of feature extraction operators but neglect the importance of exploiting multiscale point information in the network, which is crucial for identifying objects under complex scenes. To tackle this limitation, we propose an innovative network called PointNest that efficiently learns multiscale point feature propagation for accurate point segmentation. PointNest employs a deep nested U-shape encoder–decoder architecture, where the encoder learns multiscale point features through nested feature aggregation units at different network depths and propagates local geometric contextual information with skip connections along horizontal and vertical directions. The decoder then receives multiscale nested features from the encoder to progressively recover geometric details of the abstracted decoding point features for pointwise semantic prediction. In addition, we introduce a deep supervision strategy to further promote multiscale information propagation in the network for efficient training and performance improvement. Experiments on three public benchmarks demonstrate that PointNest outperforms existing mainstream methods with the mean intersection over union scores of 68.8%, 74.7%, and 62.7% in S3DIS, Toronto-3D, and WHU-MLS datasets, respectively.","2151-1535","","10.1109/JSTARS.2023.3315557","National Key Research and Development Program of China(grant numbers:2022YFB3904200); Natural Science Foundation of Hubei Province of China(grant numbers:2022CFB640); Open Fund of Hubei Key Laboratory of Intelligent Vision Based Monitoring for Hydroelectric Engineering(grant numbers:2022SDSJ04); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10251503","3-D point cloud;deep supervision (DS);multiscale feature propagation;semantic segmentation","Point cloud compression;Feature extraction;Three-dimensional displays;Network architecture;Decoding;Semantic segmentation;Semantics;Deep learning","","1","","54","CCBYNCND","14 Sep 2023","","","IEEE","IEEE Journals"
"Multi-Task Deep Network for Semantic Segmentation of Building in Very High Resolution Imagery","K. Moghalles; H. -C. Li; Z. Al-Huda; E. A. Hezzam","School of Computer Science and Technology, Southwest Jiaotong University, Chengdu, China; School of Computer Science and Technology, Southwest Jiaotong University, Chengdu, China; School of Computer Science and Technology, Southwest Jiaotong University, Chengdu, China; Department of Information Systems, College of Computer Science and Engineering, Taibah University, Medina, Saudi Arabia","2021 International Conference of Technology, Science and Administration (ICTSA)","20 Apr 2021","2021","","","1","6","Building extraction from very high resolution (VHR) imagery plays an important role in urban planning, disaster management, navigation, updating geographic databases, and several other geospatial applications. The automatic generation of buildings from satellite images presents a considerable challenge due to the complexity of building shapes. Compared with the traditional building extraction approaches, deep learning networks have shown outstanding performance in this task by using both high-level and low-level feature maps. Recently, many deep networks derived from U-Net has been extensively used in various buildings segmentation tasks. However, in most of the cases, U-net produce coarse and non-smooth segmentations with lots of discontinuities. To improve and refine the performance of U-Net network, we propose a deep end-to-end network, which use a single encoder and two parallel decoders along with performing the mask predictions also perform distance map estimation. The distance map aid in ensuring smoothness in the segmentation predictions. We also propose a new joint loss function for the proposed architecture. Experimental results based on public international society for photogrammetry and remote sensing (ISPRS) datasets with only (RGB) images demonstrated that the proposed framework can significantly improve the quality of building segmentation.","","978-1-6654-2862-0","10.1109/ICTSA52017.2021.9406538","Fundamental Research Funds for the Central Universities; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9406538","Hierarchical Image Segmentation;Feature Extraction;Feature Selection;Classification;Graph Cut","Image segmentation;Shape;Architecture;Buildings;Urban planning;Feature extraction;Task analysis","","6","","35","IEEE","20 Apr 2021","","","IEEE","IEEE Conferences"
"Deep Multi - Resolution Network for Real- Time Semantic Segmentation in Street Scenes","Y. Wang; S. Chen; H. Bian; W. Li; Q. Lu","Shandong Academy of Sciences, School of Computer Science and Technology, Qilu University of Technology, Jinan, China; Shandong Academy of Sciences, School of Computer Science and Technology, Qilu University of Technology, Jinan, China; Shandong Academy of Sciences, School of Computer Science and Technology, Qilu University of Technology, Jinan, China; Information Technology Center, China Mobile Communication Group Co Ltd, Beijing, China; Shandong Academy of Sciences, School of Computer Science and Technology, Qilu University of Technology, Jinan, China",2023 International Joint Conference on Neural Networks (IJCNN),"2 Aug 2023","2023","","","01","08","Information at different resolutions plays distinct roles in computer vision tasks. Although the research on the utilization of different resolution information in semantic segmentation has made some achievements, the research on the utilization of different resolution information in real-time semantic segmentation needs to be improved. To address this issue, we propose Deep Multi-Resolution Network (DMRNet), a lightweight model using different resolution information for real-time semantic segmentation. This model consists of several branches with different resolutions, and information is fused between neighbouring branches after convolution operations. At the end of the lowest resolution branch, we designed an enhanced semantic information module, Amplify Aggregate Pyramid Pooling Module (AAPPM), to balance the extraction of semantic information with the speed of inference. In addition, at the end of all branches, we propose a multi-resolution fusion module (MRFM) to guide the information fusion in different branches, which helps to improve the problem of spatial details being covered by semantic information. On CityScapes and Camvid, the most widely-used datasets in the field of semantic segmentation, our method strikes a balance between network accuracy and inference speed. On a single 2080Ti GPU, DMRNet achieves 77.6 % and 74.7 % accuracy at inference speeds of 68.7 FPS and 91.6 FPS, respectively.","2161-4407","978-1-6654-8867-9","10.1109/IJCNN54540.2023.10191758","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10191758","Real-time;CNN;Lightweight Network;Semantic Segmentation","Convolution;Semantic segmentation;Roads;Semantics;Neural networks;Graphics processing units;Feature extraction","","2","","44","IEEE","2 Aug 2023","","","IEEE","IEEE Conferences"
