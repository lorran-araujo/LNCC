"Authors","Author full names","Author(s) ID","Title","Year","Source title","Volume","Issue","Art. No.","Page start","Page end","Page count","Cited by","DOI","Link","Abstract","Author Keywords","Document Type","Publication Stage","Open Access","Source","EID"
"Liu X.; Jiao L.; Li L.; Tang X.; Guo Y.","Liu, Xu (57209845337); Jiao, Licheng (7102491544); Li, Lingling (56327010200); Tang, Xu (57020306700); Guo, Yuwei (56434766900)","57209845337; 7102491544; 56327010200; 57020306700; 56434766900","Deep multi-level fusion network for multi-source image pixel-wise classification","2021","Knowledge-Based Systems","221","","106921","","","","45","10.1016/j.knosys.2021.106921","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85102972437&doi=10.1016%2fj.knosys.2021.106921&partnerID=40&md5=8c3467e82c23e49adbd8d3dccc685cbe","For multi-source image pixel-wise classification, each image information is different and complementary in the same area or scene. However, how to integrate them for decision-making is a difficult problem. In this paper, we focus on the characteristics of multi-source image and propose a novel pixel-wise classification method, named deep multi-level fusion network. The proposed method is to classify multi-sensor data including very high-resolution (VHR) RGB imagery, hyperspectral imagery (HSI) and multispectral light detection and ranging (MS-LiDAR) point cloud data. First, a deep spectral–spatial attention network is proposed to process HSI and MS-LiDAR images and get a learned classification map, which is based on feature level fusion. Next, a down-superpixel segmentation algorithm is proposed to get a segmentation result for VHR RGB imagery. Finally, the feature level fusion results are refinement by the down-superpixel segmentation results on the decision level, and get the final result. Extensive experiments and analyses on the data set grss_dfc_2018 demonstrate that the proposed multi-level fusion network can achieve a better result in the multi-source image pixel-wise classification. © 2021 Elsevier B.V.","Attention; Classification; Multi-level fusion; Segmentation","Article","Final","","Scopus","2-s2.0-85102972437"
"Zhang G.; Jiang W.","Zhang, Guangzhen (58719296200); Jiang, Wangyang (58719521800)","58719296200; 58719521800","Remote Sensing Image Semantic Segmentation Method Based on a Deep Convolutional Neural Network and Multiscale Feature Fusion","2023","International Journal on Semantic Web and Information Systems","19","1","","","","","2","10.4018/IJSWIS.333712","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85177745785&doi=10.4018%2fIJSWIS.333712&partnerID=40&md5=c91a055e9d7aa3428a72208d16e2e56b","There are many problems with remote sensing images, such as large data scales, complex illumination conditions, occlusion, and dense targets. The existing semantic segmentation methods for remote sensing images are not accurate enough for small and irregular target segmentation results, and the edge extraction results are poor. The authors propose a remote sensing image segmentation method based on a DCNN and multiscale feature fusion. Firstly, an end-to-end remote sensing image segmentation model using complete residual connection and multiscale feature fusion was designed based on a deep convolutional encoder–decoder network. Secondly, weighted high-level features were obtained using an attention mechanism, which better preserved the edges, texture, and other information of remote sensing images. The experimental results on ISPRS Potsdam and Urban Drone datasets show that compared with the comparison methods, this method has better segmentation effect on small and irregular objects and achieves the best segmentation performance while ensuring the computation speed. © 2023 IGI Global. All rights reserved.","Attention Mechanism; Deep Learning; Multiscale Feature Fusion; Remote Sensing Images; Semantic Segmentation","Article","Final","All Open Access; Bronze Open Access","Scopus","2-s2.0-85177745785"
"Wang L.; Zhang C.; Li R.; Duan C.; Meng X.; Atkinson P.M.","Wang, Libo (57222867791); Zhang, Ce (56605657100); Li, Rui (57061176600); Duan, Chenxi (57215424832); Meng, Xiaoliang (35729314000); Atkinson, Peter M. (7201906181)","57222867791; 56605657100; 57061176600; 57215424832; 35729314000; 7201906181","Scale-aware neural network for semantic segmentation of multi-resolution remote sensing images","2021","Remote Sensing","13","24","5015","","","","19","10.3390/rs13245015","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85121460626&doi=10.3390%2frs13245015&partnerID=40&md5=1923d40419382bb75f8e70ea4785a5d4","Assigning geospatial objects with specific categories at the pixel level is a fundamental task in remote sensing image analysis. Along with the rapid development of sensor technologies, remotely sensed images can be captured at multiple spatial resolutions (MSR) with information content manifested at different scales. Extracting information from these MSR images represents huge opportunities for enhanced feature representation and characterisation. However, MSR images suffer from two critical issues: (1) increased scale variation of geo-objects and (2) loss of detailed information at coarse spatial resolutions. To bridge these gaps, in this paper, we propose a novel scale-aware neural network (SaNet) for the semantic segmentation of MSR remotely sensed imagery. SaNet deploys a densely connected feature network (DCFFM) module to capture high-quality multi-scale context, such that the scale variation is handled properly and the quality of segmentation is increased for both large and small objects. A spatial feature recalibration (SFRM) module was further incorporated into the network to learn intact semantic content with enhanced spatial relationships, where the negative effects of information loss are removed. The combination of DCFFM and SFRM allows SaNet to learn scale-aware feature representation, which outperforms the existing multi-scale feature representation. Extensive experiments on three semantic segmentation datasets demonstrated the effectiveness of the proposed SaNet in cross-resolution segmentation. © 2021 by the authors. Licensee MDPI, Basel, Switzerland.","Deep convolutional neural network; Multiple spatial resolutions; Remote sensing; Scale-aware feature representation; Semantic segmentation","Article","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85121460626"
"Wan J.; Zeng Z.; Qiu Q.; Xie Z.; Xu Y.","Wan, Jie (57224205384); Zeng, Ziyin (57251475700); Qiu, Qinjun (57203591589); Xie, Zhong (36164790400); Xu, Yongyang (57095192900)","57224205384; 57251475700; 57203591589; 36164790400; 57095192900","PointNest: Learning Deep Multiscale Nested Feature Propagation for Semantic Segmentation of 3-D Point Clouds","2023","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","16","","","9051","9066","15","2","10.1109/JSTARS.2023.3315557","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85171797850&doi=10.1109%2fJSTARS.2023.3315557&partnerID=40&md5=5f38326ed41d936464cac19e95fe1521","3-D point cloud semantic segmentation is a fundamental task for scene understanding, but this task remains challenging due to the diverse scene classes, data defects, and occlusions. Most existing deep learning-based methods focus on new designs of feature extraction operators but neglect the importance of exploiting multiscale point information in the network, which is crucial for identifying objects under complex scenes. To tackle this limitation, we propose an innovative network called PointNest that efficiently learns multiscale point feature propagation for accurate point segmentation. PointNest employs a deep nested U-shape encoder-decoder architecture, where the encoder learns multiscale point features through nested feature aggregation units at different network depths and propagates local geometric contextual information with skip connections along horizontal and vertical directions. The decoder then receives multiscale nested features from the encoder to progressively recover geometric details of the abstracted decoding point features for pointwise semantic prediction. In addition, we introduce a deep supervision strategy to further promote multiscale information propagation in the network for efficient training and performance improvement. Experiments on three public benchmarks demonstrate that PointNest outperforms existing mainstream methods with the mean intersection over union scores of 68.8%, 74.7%, and 62.7% in S3DIS, Toronto-3D, and WHU-MLS datasets, respectively.  © 2008-2012 IEEE.","3-D point cloud; deep supervision (DS); multiscale feature propagation; semantic segmentation","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85171797850"
"Lyu Y.; Schiopu I.; Munteanu A.","Lyu, Y. (57208050022); Schiopu, I. (55970007300); Munteanu, A. (7005664010)","57208050022; 55970007300; 7005664010","Multi-modal neural networks with multi-scale RGB-T fusion for semantic segmentation","2020","Electronics Letters","56","18","","920","923","3","18","10.1049/el.2020.1635","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85091451959&doi=10.1049%2fel.2020.1635&partnerID=40&md5=758e13b5d4b263da317c38a330000214","A novel deep-learning-based method for semantic segmentation of RGB and thermal images is introduced. The proposed method employs a novel neural network design for multi-modal fusion based on multi-resolution patch processing. A novel decoder module is introduced to fuse the RGB and thermal features extracted by separate encoder streams. Experimental results on synthetic and real-world data demonstrate the efficiency of the proposed method compared with state-of-the-art methods.  © The Institution of Engineering and Technology 2020.","","Article","Final","","Scopus","2-s2.0-85091451959"
"Alavianmehr M.A.; Helfroush M.S.; Danyali H.; Tashk A.","Alavianmehr, M.A. (55499872600); Helfroush, M.S. (24528392900); Danyali, H. (6507150538); Tashk, A. (34881012000)","55499872600; 24528392900; 6507150538; 34881012000","Butterfly network: a convolutional neural network with a new architecture for multi-scale semantic segmentation of pedestrians","2023","Journal of Real-Time Image Processing","20","1","9","","","","2","10.1007/s11554-023-01273-z","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85147256487&doi=10.1007%2fs11554-023-01273-z&partnerID=40&md5=9f2cf5a3ab66600c889242a0e5101541","The detection of multi-scale pedestrians is one of the challenging tasks in pedestrian detection applications. Moreover, the task of small-scale pedestrian detection, i.e., accurate localization of pedestrians as low-scale target objects, can help solve the issue of occluded pedestrian detection as well. In this paper, we present a fully convolutional neural network with a new architecture and an innovative, fully detailed supervision for semantic segmentation of pedestrians. The proposed network has been named butterfly network (BF-Net) because of its architecture analogous to a butterfly. The proposed BF-Net preserves the ability of simplicity so that it can process static images with a real-time image processing rate. The sub-path blocks embedded in the architecture of the proposed BF-Net provides a higher accuracy for detecting multi-scale objective targets including the small ones. The other advantage of the proposed architecture is replacing common batch normalization with conditional one. In conclusion, the experimental results of the proposed method demonstrate that the proposed network outperform the other state-of-the-art networks such as U-Net + + , U-Net3 + , Mask-RCNN, and Deeplabv3 + for the semantic segmentation of the pedestrians. © 2023, The Author(s), under exclusive licence to Springer-Verlag GmbH Germany, part of Springer Nature.","Butterfly network (BF-Net); Convolutional neural network; Pedestrian detection; Semantic segmentation; State-of-the-art U-Nets","Article","Final","All Open Access; Bronze Open Access; Green Open Access","Scopus","2-s2.0-85147256487"
"Chen K.; Weinmann M.; Sun X.; Yan M.; Hinz S.; Jutzi B.; Weinmann M.","Chen, K. (57200269572); Weinmann, M. (36341884100); Sun, X. (34875643000); Yan, M. (57200852160); Hinz, S. (7004082496); Jutzi, B. (7801347466); Weinmann, M. (55818523800)","57200269572; 36341884100; 34875643000; 57200852160; 7004082496; 7801347466; 55818523800","SEMANTIC SEGMENTATION of AERIAL IMAGERY VIA MULTI-SCALE SHUFFLING CONVOLUTIONAL NEURAL NETWORKS with DEEP SUPERVISION","2018","ISPRS Annals of the Photogrammetry, Remote Sensing and Spatial Information Sciences","4","1","","29","36","7","16","10.5194/isprs-annals-IV-1-29-2018","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85056355458&doi=10.5194%2fisprs-annals-IV-1-29-2018&partnerID=40&md5=6f59f5f02c5a1c5c18ce201f4db98174","In this paper, we address the semantic segmentation of aerial imagery based on the use of multi-modal data given in the form of true orthophotos and the corresponding Digital Surface Models (DSMs). We present the Deeply-supervised Shuffling Convolutional Neural Network (DSCNN) representing a multi-scale extension of the Shuffling Convolutional Neural Network (SCNN) with deep supervision. Thereby, we take the advantage of the SCNN involving the shuffling operator to effectively upsample feature maps and then fuse multiscale features derived from the intermediate layers of the SCNN, which results in the Multi-scale Shuffling Convolutional Neural Network (MSCNN). Based on the MSCNN, we derive the DSCNN by introducing additional losses into the intermediate layers of the MSCNN. In addition, we investigate the impact of using different sets of hand-crafted radiometric and geometric features derived from the true orthophotos and the DSMs on the semantic segmentation task. For performance evaluation, we use a commonly used benchmark dataset. The achieved results reveal that both multi-scale fusion and deep supervision contribute to an improvement in performance. Furthermore, the use of a diversity of hand-crafted radiometric and geometric features as input for the DSCNN does not provide the best numerical results, but smoother and improved detections for several objects. © Authors 2018.","Aerial Imagery; CNN; Deep Supervision; Multi-Modal Data; Multi-Scale; Semantic Segmentation","Conference paper","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85056355458"
"Wang C.; Li L.","Wang, Chengyi (57129401700); Li, Lianfa (55939190800)","57129401700; 55939190800","Multi-scale residual deep network for semantic segmentation of buildings with regularizer of shape representation","2020","Remote Sensing","12","18","2932","","","","15","10.3390/RS12182932","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85091870784&doi=10.3390%2fRS12182932&partnerID=40&md5=c2d417214a0985752e3f90f8a41bed38","It is challenging for semantic segmentation of buildings based on high-resolution remote sensing images, given high variability of appearance and complicated backgrounds of the buildings and their images. In this communication, we proposed an ensemble multi-scale residual deep learning method with the regularizer of shape representation for semantic segmentation of buildings. Based on the U-Net architecture using residual connections and multi-scale ASPP (atrous spatial pyramid pooling) modules, our method introduced the regularizer of shape representation and ensemble learning of multi-scale models to enhance model training and reduce over-fitting. In our method, the shape representation was coded in an antoencoder that was used to encode and reconstruct the shape characteristics of the buildings. In prediction, we consider multi-scale trained models for different resolution inputs and side effects to obtain an optimal semantic segmentation. With the high-resolution image of the Changshan, an island county in China, we used two-thirds of the study region image to train the model and the remaining one-third for the independent test. We obtained the accuracy of 0.98-0.99, mean intersection over union (MIoU) of 0.91-0.93 and Jaccard coefficient of 0.89-0.92 in validation. In the independent test, our method achieved state-of-the-art performance (MIoU: 0.83; Jaccard index: 0.81). By comparing with the existing representative methods on four different data sets, the proposed method consistently improved the learning process and generalization. The study shows important contributions of ensemble learning of multi-scale residual models and regularizer of shape representation to semantic segmentation of buildings. © 2020 by the authors.","Multiple scales; Regularizer; Residual deep ensemble learning; Semantic segmentation of buildings; Shape representation","Article","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85091870784"
"Zhou Q.; Yang W.; Gao G.; Ou W.; Lu H.; Chen J.; Latecki L.J.","Zhou, Quan (56909174900); Yang, Wenbing (57201686354); Gao, Guangwei (55352183600); Ou, Weihua (55613280300); Lu, Huimin (57209823396); Chen, Jie (57255976300); Latecki, Longin Jan (7004123980)","56909174900; 57201686354; 55352183600; 55613280300; 57209823396; 57255976300; 7004123980","Multi-scale deep context convolutional neural networks for semantic segmentation","2019","World Wide Web","22","2","","555","570","15","114","10.1007/s11280-018-0556-3","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85045754940&doi=10.1007%2fs11280-018-0556-3&partnerID=40&md5=bd267fda757b9c9bb30fbc57c37f13ee","Recent years have witnessed the great progress for semantic segmentation using deep convolutional neural networks (DCNNs). This paper presents a novel fully convolutional network for semantic segmentation using multi-scale contextual convolutional features. Since objects in natural images tend to be with various scales and aspect ratios, capturing the rich contextual information is very critical for dense pixel prediction. On the other hand, when going deeper in convolutional layers, the convolutional feature maps of traditional DCNNs gradually become coarser, which may be harmful for semantic segmentation. According to these observations, we attempt to design a multi-scale deep context convolutional network (MDCCNet), which combines the feature maps from different levels of network in a holistic manner for semantic segmentation. The segmentation outputs of MDCCNets are further enhanced using dense connected conditional random fields (CRF). The proposed network allows us to fully exploit local and global contextual information, ranging from an entire scene to every single pixel, to perform pixel-wise label estimation. The experimental results demonstrate that our method outperforms or is comparable to state-of-the-art methods on PASCAL VOC 2012 and SIFTFlow semantic segmentation datasets. © 2018, Springer Science+Business Media, LLC, part of Springer Nature.","CRF; MDCNNs; Multi-scale context; Semantic segmentation","Article","Final","","Scopus","2-s2.0-85045754940"
"Audebert N.; Le Saux B.; Lefèvre S.","Audebert, Nicolas (57192692249); Le Saux, Bertrand (6506307016); Lefèvre, Sébastien (57203070803)","57192692249; 6506307016; 57203070803","Semantic segmentation of earth observation data using multimodal and multi-scale deep networks","2017","Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","10111 LNCS","","","180","196","16","173","10.1007/978-3-319-54181-5_12","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85016026118&doi=10.1007%2f978-3-319-54181-5_12&partnerID=40&md5=a8314c446dcf90d129c2653266d05d9b","This work investigates the use of deep fully convolutional neural networks (DFCNN) for pixel-wise scene labeling of Earth Observation images. Especially, we train a variant of the SegNet architecture on remote sensing data over an urban area and study different strategies for performing accurate semantic segmentation. Our contributions are the following: (1) we transfer efficiently a DFCNN from generic everyday images to remote sensing images; (2) we introduce a multi-kernel convolutional layer for fast aggregation of predictions at multiple scales; (3) we perform data fusion from heterogeneous sensors (optical and laser) using residual correction. Our framework improves state-of-the-art accuracy on the ISPRS Vaihingen 2D Semantic Labeling dataset. © Springer International Publishing AG 2017.","","Conference paper","Final","","Scopus","2-s2.0-85016026118"
"Duan Y.; Tao X.; Han C.; Qin X.; Lu J.","Duan, Yiping (57195315827); Tao, Xiaoming (57208894708); Han, Chaoyi (57202250817); Qin, Xiaowci (59038373800); Lu, Jianhua (7601561492)","57195315827; 57208894708; 57202250817; 59038373800; 7601561492","Multi-Scale Convolutional Neural Network for SAR Image Semantic Segmentation","2018","Proceedings - IEEE Global Communications Conference, GLOBECOM","","","8647657","","","","15","10.1109/GLOCOM.2018.8647657","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85063499279&doi=10.1109%2fGLOCOM.2018.8647657&partnerID=40&md5=5ef90f129433b4d85e9d4ce613e1c581","Although the recent success of convolutional neural networks (CNNs) greatly advance the semantic segmentation of the natural images, few work has focused on the remote sensing images, especially the synthetic aperture radar (SAR) images. Specifically, the existing methods do not consider the speckle noise of the SAR images and the multi-scale characteristics contained in the SAR images. In this paper, we propose a multiscale convolutional neural network (CNN) model for SAR image semantic segmentation. The multi-scale CNN model includes noise removal stage, convolutional stage, feature concatenation stage and classification stage. In particular, we construct a sparse representation loss function to obtain a clear SAR image in noise removal stage. Then, the multi-scale convolutional stage is employed to learn the multi-scale deep features. The concatenation stage is used to connect the features with different scales and depths. Finally, softmax classifier is developed to obtain the labels of the SAR images with the multi-scale CNN model being trained in an end-to-end way. The experimental results on synthetic and real SAR images demonstrate the effectiveness of the proposed method. © 2018 IEEE.","","Conference paper","Final","","Scopus","2-s2.0-85063499279"
"Zhao S.; Wang Z.; Huo Z.; Zhang F.","Zhao, Shan (35757184200); Wang, Zihao (59302509500); Huo, Zhanqiang (24921320800); Zhang, Fukai (57205648363)","35757184200; 59302509500; 24921320800; 57205648363","Semantic Segmentation Network Based on Adaptive Attention and Deep Fusion Utilizing a Multi-Scale Dilated Convolutional Pyramid","2024","Sensors","24","16","5305","","","","0","10.3390/s24165305","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85202440291&doi=10.3390%2fs24165305&partnerID=40&md5=3597f06cc3f731599b31dc5a1fcdb692","Deep learning has recently made significant progress in semantic segmentation. However, the current methods face critical challenges. The segmentation process often lacks sufficient contextual information and attention mechanisms, low-level features lack semantic richness, and high-level features suffer from poor resolution. These limitations reduce the model’s ability to accurately understand and process scene details, particularly in complex scenarios, leading to segmentation outputs that may have inaccuracies in boundary delineation, misclassification of regions, and poor handling of small or overlapping objects. To address these challenges, this paper proposes a Semantic Segmentation Network Based on Adaptive Attention and Deep Fusion with the Multi-Scale Dilated Convolutional Pyramid (SDAMNet). Specifically, the Dilated Convolutional Atrous Spatial Pyramid Pooling (DCASPP) module is developed to enhance contextual information in semantic segmentation. Additionally, a Semantic Channel Space Details Module (SCSDM) is devised to improve the extraction of significant features through multi-scale feature fusion and adaptive feature selection, enhancing the model’s perceptual capability for key regions and optimizing semantic understanding and segmentation performance. Furthermore, a Semantic Features Fusion Module (SFFM) is constructed to address the semantic deficiency in low-level features and the low resolution in high-level features. The effectiveness of SDAMNet is demonstrated on two datasets, revealing significant improvements in Mean Intersection over Union (MIOU) by 2.89% and 2.13%, respectively, compared to the Deeplabv3+ network. © 2024 by the authors.","adaptive attention; feature fusion; global average pooling; semantic segmentation","Article","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85202440291"
"He P.; Ma Z.; Fei M.; Liu W.; Guo G.; Wang M.","He, Peipei (56260849200); Ma, Zheng (57985046100); Fei, Meiqi (57985046200); Liu, Wenkai (56092148300); Guo, Guihai (57871637700); Wang, Mingwei (56542035800)","56260849200; 57985046100; 57985046200; 56092148300; 57871637700; 56542035800","A Multiscale Multi-Feature Deep Learning Model for Airborne Point-Cloud Semantic Segmentation","2022","Applied Sciences (Switzerland)","12","22","11801","","","","7","10.3390/app122211801","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85142837663&doi=10.3390%2fapp122211801&partnerID=40&md5=4bf40e5d1fe4a0008e5eec8f967b625a","In point-cloud scenes, semantic segmentation is the basis for achieving an understanding of a 3D scene. The disorderly and irregular nature of 3D point clouds makes it impossible for traditional convolutional neural networks to be applied directly, and most deep learning point-cloud models often suffer from an inadequate utilization of spatial information and of other related point-cloud features. Therefore, to facilitate the capture of spatial point neighborhood information and obtain better performance in point-cloud analysis for point-cloud semantic segmentation, a multiscale, multi-feature PointNet (MSMF-PointNet) deep learning point-cloud model is proposed in this paper. MSMF-PointNet is based on the classical point-cloud model PointNet, and two small feature-extraction networks called Mini-PointNets are added to operate in parallel with the modified PointNet; these additional networks extract multiscale, multi-neighborhood features for classification. In this paper, we use the spherical neighborhood method to obtain the local neighborhood features of the point cloud, and then we adjust the radius of the spherical neighborhood to obtain the multiscale point-cloud features. The obtained multiscale neighborhood feature point set is used as the input of the network. In this paper, a cross-sectional comparison analysis is conducted on the Vaihingen urban test dataset from the single-scale and single-feature perspectives. © 2022 by the authors.","airborne LiDAR; multiscale and multi-feature; PointNet; semantic segmentation","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85142837663"
"Zhao B.; Zhang X.; Li Z.; Hu X.","Zhao, Bonan (57210261738); Zhang, Xiaoshan (57210259571); Li, Zheng (57210261240); Hu, Xianliang (9235533700)","57210261738; 57210259571; 57210261240; 9235533700","A multi-scale strategy for deep semantic segmentation with convolutional neural networks","2019","Neurocomputing","365","","","273","284","11","20","10.1016/j.neucom.2019.07.078","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85070113876&doi=10.1016%2fj.neucom.2019.07.078&partnerID=40&md5=df0c825b6dff15dd288557ffc0c1c89e","A novel multi-scale scheme is proposed to improve the performance of deep semantic segmentation based on Convolutional Neural Networks (CNNs). The fundamental idea is to combine the information from different intermediate layers by introducing new multi-scale loss (mLoss) function. We also show that it can be calculated by three different modules. The advantage of mLoss functions is that the loss of all layers could be optimized in one-shot without additional modifications of the training algorithm. The proposed strategy is also applied to improve the performance of Unet and FCN, and the structures of multi-scale loss functions are presented as well. Numerical validations are performed on two datasets, including the benchmark Pascal VOC 2012 dataset and the PICC dataset from medical treatment. It is illustrated that our multi-scale approach yields faster learning convergence rate and better accuracy. © 2019 Elsevier B.V.","Convolutional neural network; Markov Random Field; Multi-scale loss function; PICC; Semantic segmentation","Article","Final","","Scopus","2-s2.0-85070113876"
"Wang L.; Yan C.","Wang, Longfei (57964142700); Yan, Chunman (36618437800)","57964142700; 36618437800","Semantic Segmentation of Road Scene Based on Multi-Scale Feature Extraction and Deep Supervision","2022","Proceedings of SPIE - The International Society for Optical Engineering","12342","","1234206","","","","1","10.1117/12.2644695","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85141869886&doi=10.1117%2f12.2644695&partnerID=40&md5=56e31b270cdf1dfb23b5a1bcb7e49df4","Aiming at the problems of inaccurate segmentation edges, poor adaptability to multi-scale road targets, prone to false segmentation and missing segmentation when segmenting road targets with various and changeable occlusions in the traditional U-Net model, a semantic segmentation model of road scene based on multi-scale feature extraction and deep supervision module is proposed. Firstly, the dual attention module is embedded in the U-Net encoder, which can make the model have the ability to capture the context information of channel dimension and spatial dimension in the global range, and enhance the road features; Secondly, before upsampling, the feature map containing high-level semantic information is input into ASPP module to obtain road features of different scales; Finally, the deep supervision module is introduced into the upsampling part to learn the feature representation at different levels and retain more road detail features. Experiments are carried out on CamVid dataset and Cityscapes dataset. The results show that our Network can effectively segment road targets with different scales, and the segmented road contour is more complete and clear, which improves the accuracy of semantic segmentation while ensuring a certain segmentation speed. © 2022 SPIE.","ASPP; Deep supervision; Dual attention module; Semantic segmentation; U-Net","Conference paper","Final","","Scopus","2-s2.0-85141869886"
"Wang Q.; Zhao Y.; Zhang Z.","Wang, Qixuan (58927992100); Zhao, Yangzheng (59390725300); Zhang, Zhuofan (59390523400)","58927992100; 59390725300; 59390523400","Convolutional Neural Network-Based Multi-scale Semantic Segmentation for Two-Dimensional Panoramic X-Rays of Teeth","2025","Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics) ","14623 LNCS","","","1","13","12","0","10.1007/978-3-031-72396-4_1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85207851492&doi=10.1007%2f978-3-031-72396-4_1&partnerID=40&md5=5394dd1339e0321a472aea9dfd6e0935","In this study, we propose a semantic segmentation framework based on convolutional neural networks (CNNs) aimed at segmenting teeth in 2D panoramic radiographs. To enhance the model’s performance, we adopt a data augmentation strategy based on the combination of Mosaic and multi-scale image scaling, which significantly enriches the training set samples. Additionally, we propose a post-processing strategy based on the model’s prediction probability map to address the issue of inaccurate points predicted by the model. The proposed framework is tested and evaluated on 2D panoramic radiographs, including 1000 unlabeled radiographs. The Dice coefficient, Intersection over Union (IoU), and 1-H(d) achieve 0.9341, 0.9814, and 0.0244, respectively, resulting in a total score of 0.9607, ranking fifth on the leaderboard. This demonstrates the superiority of the proposed framework in tooth segmentation of 2D panoramic radiographs. © The Author(s), under exclusive license to Springer Nature Switzerland AG 2025.","MICCAI challenge; Panoramic x-ray; Tooth segmentation","Conference paper","Final","","Scopus","2-s2.0-85207851492"
"Wang Y.; Chen S.; Bian H.; Li W.; Lu Q.","Wang, Yalun (58560566400); Chen, Shidong (57999887600); Bian, Huicong (58559935100); Li, Weixiao (58174506700); Lu, Qin (55231328800)","58560566400; 57999887600; 58559935100; 58174506700; 55231328800","Deep Multi - Resolution Network for Real- Time Semantic Segmentation in Street Scenes","2023","Proceedings of the International Joint Conference on Neural Networks","2023-June","","","","","","2","10.1109/IJCNN54540.2023.10191758","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85169606987&doi=10.1109%2fIJCNN54540.2023.10191758&partnerID=40&md5=917b9d2d66bd04b743d45957a756e5ec","Information at different resolutions plays distinct roles in computer vision tasks. Although the research on the utilization of different resolution information in semantic segmentation has made some achievements, the research on the utilization of different resolution information in real-time semantic segmentation needs to be improved. To address this issue, we propose Deep Multi-Resolution Network (DMRNet), a lightweight model using different resolution information for real-time semantic segmentation. This model consists of several branches with different resolutions, and information is fused between neighbouring branches after convolution operations. At the end of the lowest resolution branch, we designed an enhanced semantic information module, Amplify Aggregate Pyramid Pooling Module (AAPPM), to balance the extraction of semantic information with the speed of inference. In addition, at the end of all branches, we propose a multi-resolution fusion module (MRFM) to guide the information fusion in different branches, which helps to improve the problem of spatial details being covered by semantic information. On CityScapes and Camvid, the most widely-used datasets in the field of semantic segmentation, our method strikes a balance between network accuracy and inference speed. On a single 2080Ti GPU, DMRNet achieves 77.6 % and 74.7 % accuracy at inference speeds of 68.7 FPS and 91.6 FPS, respectively. © 2023 IEEE.","CNN; Lightweight Network; Real-time; Semantic Segmentation","Conference paper","Final","","Scopus","2-s2.0-85169606987"
"Eguchi R.R.; Huang P.-S.","Eguchi, Raphael R. (57215864763); Huang, Po-Ssu (8690115900)","57215864763; 8690115900","Multi-scale structural analysis of proteins by deep semantic segmentation","2020","Bioinformatics","36","6","","1740","1749","9","8","10.1093/bioinformatics/btz650","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85082147932&doi=10.1093%2fbioinformatics%2fbtz650&partnerID=40&md5=ca8cb7e22085504e3798c4a030bbb68a","Motivation: Recent advances in computational methods have facilitated large-scale sampling of protein structures, leading to breakthroughs in protein structural prediction and enabling de novo protein design. Establishing methods to identify candidate structures that can lead to native folds or designable structures remains a challenge, since few existing metrics capture high-level structural features such as architectures, folds and conformity to conserved structural motifs. Convolutional Neural Networks (CNNs) have been successfully used in semantic segmentation—a sub-field of image classification in which a class label is predicted for every pixel. Here, we apply semantic segmentation to protein structures as a novel strategy for fold identification and structure quality assessment. Results: We train a CNN that assigns each residue in a multi-domain protein to one of 38 architecture classes designated by the CATH database. Our model achieves a high per-residue accuracy of 90.8% on the test set (95.0% average per-class accuracy; 87.8% average per-structure accuracy). We demonstrate that individual class probabilities can be used as a metric that indicates the degree to which a randomly generated structure assumes a specific fold, as well as a metric that highlights non-conformative regions of a protein belonging to a known class. These capabilities yield a powerful tool for guiding structural sampling for both structural prediction and design. © The Author(s) 2019. Published by Oxford University Press. All rights reserved.","","Article","Final","All Open Access; Bronze Open Access; Green Open Access","Scopus","2-s2.0-85082147932"
"van Rijthoven M.; Balkenhol M.; Siliņa K.; van der Laak J.; Ciompi F.","van Rijthoven, Mart (57210814101); Balkenhol, Maschenka (57202417370); Siliņa, Karina (23567210700); van der Laak, Jeroen (6701833644); Ciompi, Francesco (32667506900)","57210814101; 57202417370; 23567210700; 6701833644; 32667506900","HookNet: Multi-resolution convolutional neural networks for semantic segmentation in histopathology whole-slide images","2021","Medical Image Analysis","68","","101890","","","","126","10.1016/j.media.2020.101890","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85096825471&doi=10.1016%2fj.media.2020.101890&partnerID=40&md5=181c142ef12968adc351bc8be4efe7d7","We propose HookNet, a semantic segmentation model for histopathology whole-slide images, which combines context and details via multiple branches of encoder-decoder convolutional neural networks. Concentric patches at multiple resolutions with different fields of view, feed different branches of HookNet, and intermediate representations are combined via a hooking mechanism. We describe a framework to design and train HookNet for achieving high-resolution semantic segmentation and introduce constraints to guarantee pixel-wise alignment in feature maps during hooking. We show the advantages of using HookNet in two histopathology image segmentation tasks where tissue type prediction accuracy strongly depends on contextual information, namely (1) multi-class tissue segmentation in breast cancer and, (2) segmentation of tertiary lymphoid structures and germinal centers in lung cancer. We show the superiority of HookNet when compared with single-resolution U-Net models working at different resolutions as well as with a recently published multi-resolution model for histopathology image segmentation. We have made HookNet publicly available by releasing the source code1 as well as in the form of web-based applications2,3 based on the grand-challenge.org platform. © 2020 The Authors","Computational pathology; Deep learning; Multi-resolution; Semantic segmentation","Article","Final","All Open Access; Green Open Access; Hybrid Gold Open Access","Scopus","2-s2.0-85096825471"
