"Document Title",Authors,"Author Affiliations","Publication Title",Date Added To Xplore,"Publication Year","Volume","Issue","Start Page","End Page","Abstract","ISSN",ISBNs,"DOI",Funding Information,PDF Link,"Author Keywords","IEEE Terms","Mesh_Terms",Article Citation Count,Patent Citation Count,"Reference Count","License",Online Date,Issue Date,"Meeting Date","Publisher",Document Identifier
"Intelligent Marine Survey: Lightweight Multi-Scale Attention Adaptive Segmentation Framework for Underwater Target Detection of AUV","Q. Wang; Y. Zhang; B. He","School of Information Science and Engineering, Ocean University of China, Qingdao, Shandong, China; School of Information Science and Engineering, Ocean University of China, Qingdao, Shandong, China; School of Information Science and Engineering, Ocean University of China, Qingdao, Shandong, China",IEEE Transactions on Automation Science and Engineering,"4 Feb 2025","2025","22","","1913","1927","Accurate and automatic underwater target recognition is a compelling challenge for autonomous underwater vehicles (AUVs) in intelligent marine surveys. This study proposed a seabed target correction model based on side-scan sonar (SSS) images and combined the navigation information of AUV to achieve pixel-level geocoding. Moreover, a lightweight multi-level attention adaptive segmentation framework ( ${\mathrm{M}}{{\mathrm{A}}^{\mathrm{2}}}{\mathrm{Net}}$ ) was proposed to achieve fine-grained recognition. It contains three new modules: 1) The lightweight attention network (LAN) is designed as the baseline to obtain dense feature maps and focus on interesting features based on a balanced attention mechanism. 2) the multi-scale feature pyramid (MASPP) was then constructed to capture the context of SSS images and extract rich semantic information at high levels. 3) Finally, the adaptive feature fusion module (AFF) effectively incorporates feature maps of MASPP and spatial information to improve the learned representations further. Extensive experiments are verified on six SSS categories and show the remarkable performance of the  ${\mathrm{M}}{{\mathrm{A}}^{\mathrm{2}}}{\mathrm{Net}}$  compared with state-of-the-art methods. Furthermore, real sea trials were conducted by deploying  ${\mathrm{M}}{{\mathrm{A}}^{\mathrm{2}}}{\mathrm{Net}}$  to the autonomous target recognition (ATR) system of AUV, which can achieve 29.7 fps and 81.23% MIoU for a ( $512\times 512$ ) input on a single Nvidia Jetson Xavier. Note to Practitioners—This paper aims to provide a real-time semantic segmentation model for the autonomous target detection of AUV, which is suitable for the autonomous detection of underwater targets by underwater robots (ROV, AUV, ARV, et al). This paper proposes a lightweight, multi-scale attention-adaptive segmentation framework ( ${\mathrm{M}}{{\mathrm{A}}^{\mathrm{2}}}{\mathrm{Net}}$ ) incorporating pixel-level seabed targets rectification methods. The algorithm has high segmentation accuracy and fast operation speed. It can identify seabed targets in high-resolution sonar images online and realize precise positioning of small seabed targets, which is conducive to improving the intelligence level of marine survey unmanned equipment. This paper details the design of  ${\mathrm{M}}{{\mathrm{A}}^{\mathrm{2}}}{\mathrm{Net}}$  and the hardware structure of the autonomous target recognition system (ATR). Plenty of simulation experiments and sea trials have proved the efficiency and practicability of the method for the autonomous detection of different seabed targets (sand waves, coral reefs, metal balls, threads, and artificial reefs). Future research will verify the generalization of the algorithm in more seabed targets.","1558-3783","","10.1109/TASE.2024.3371963","National Key Research and Development Program of China(grant numbers:2016YFC0301400); Natural Science Foundation of Shandong Province, China(grant numbers:ZR2020MF079); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10464247","Autonomous underwater vehicle;underwater targets recognition;side-scan sonar;real-time semantic segmentation","Feature extraction;Sonar;Real-time systems;Surveys;Semantics;Target recognition;Semantic segmentation","","5","","37","IEEE","14 Mar 2024","","","IEEE","IEEE Journals"
"The remote sensing image segmentation of land cover based on multi-scale attention features","H. Hu; L. Yang; J. Chen; S. Luo","School of Big Data, Yunnan Agricultural University, Kunming, China; School of Big Data, Yunnan Agricultural University, Kunming, China; School of Big Data, Yunnan Agricultural University, Kunming, China; School of Big Data, Yunnan Agricultural University, Kunming, China",2023 IEEE 35th International Conference on Tools with Artificial Intelligence (ICTAI),"20 Dec 2023","2023","","","429","436","Segmentation of land cover in remote sensing images is a task that involves interpreting remote sensing data using machine vision. Satisfying segmentation results in agriculture and forestry regions can guide land resource management, natural environment protection, urban construction, and the distribution of agricultural products. However, the performance of the widely used deep learning segmentation model on high-resolution remote sensing segmentation datasets in agriculture and forestry regions needs to be improved. To solve the problems of poor accuracy and loss of context information in remote sensing image semantic segmentation, this paper proposes an improved semantic segmentation network architecture. The model utilizes multi-scale feature extraction, deploys a multi-layer attention feature fusion module and an up-sampling fusion module to capture high-quality multi-scale context information, correctly handle scale changes, and help narrow the semantic gap between different levels. Finally, the proposed MLP decoder refers to the dynamic up-sampling operator to aggregate the information at different levels to achieve pixel segmentation. To verify the effectiveness of our proposed model, the researchers conducted experiments on two land cover segmentation datasets. The training process specifically designs data augmentation strategies for remote sensing segmentation tasks to enhance the model’s generalization ability. The final model achieved an mIoU (mean Intersection over Union) of 65.05% on the self-built rural land cover datasets, surpassing the benchmark network UPerNet by 5.92%. On the LoveDA dataset, our model achieved state-of-the-art performance with an mIoU of 53.39%, demonstrating its versatility.","2375-0197","979-8-3503-4273-4","10.1109/ICTAI59109.2023.00069","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10356532","land cover;remote sensing image;neural network;attention mechanism;multi-scale feature","Training;Semantic segmentation;Semantics;Object segmentation;Forestry;Feature extraction;Decoding","","","","40","IEEE","20 Dec 2023","","","IEEE","IEEE Conferences"
"Omni-Seg: A Scale-Aware Dynamic Network for Renal Pathological Image Segmentation","R. Deng; Q. Liu; C. Cui; T. Yao; J. Long; Z. Asad; R. M. Womick; Z. Zhu; A. B. Fogo; S. Zhao; H. Yang; Y. Huo","Department of Computer Science, Vanderbilt University, USA; Department of Computer Science, Vanderbilt University, USA; Department of Computer Science, Vanderbilt University, USA; Department of Computer Science, Vanderbilt University, USA; Big Data Institute, Central South University, China; Department of Computer Science, Vanderbilt University, USA; Department of Computer Science, The University of North Carolina at Chapel Hill, USA; Department of Computer Science, Vanderbilt University, USA; Department of Pathology, Microbiology and Immunology, Vanderbilt University Medical Center, USA; Department of Pathology, Microbiology and Immunology, Vanderbilt University Medical Center, USA; Department of Pathology, Microbiology and Immunology, Vanderbilt University Medical Center, USA; Department of Computer Science, Vanderbilt University, Nashville, TN, USA",IEEE Transactions on Biomedical Engineering,"30 Aug 2023","2023","70","9","2636","2644","Comprehensive semantic segmentation on renal pathological images is challenging due to the heterogeneous scales of the objects. For example, on a whole slide image (WSI), the cross-sectional areas of glomeruli can be 64 times larger than that of the peritubular capillaries, making it impractical to segment both objects on the same patch, at the same scale. To handle this scaling issue, prior studies have typically trained multiple segmentation networks in order to match the optimal pixel resolution of heterogeneous tissue types. This multi-network solution is resource-intensive and fails to model the spatial relationship between tissue types. In this article, we propose the Omni-Seg network, a scale-aware dynamic neural network that achieves multi-object (six tissue types) and multi-scale (5× to 40× scale) pathological image segmentation via a single neural network. The contribution of this article is three-fold: (1) a novel scale-aware controller is proposed to generalize the dynamic neural network from single-scale to multi-scale; (2) semi-supervised consistency regularization of pseudo-labels is introduced to model the inter-scale correlation of unannotated tissue types into a single end-to-end learning paradigm; and (3) superior scale-aware generalization is evidenced by directly applying a model trained on human kidney images to mouse kidney images, without retraining. By learning from 150,000 human pathological image patches from six tissue types at three different resolutions, our approach achieved superior segmentation performance according to human visual assessment and evaluation of image-omics (i.e., spatial transcriptomics).","1558-2531","","10.1109/TBME.2023.3260739","NIH(grant numbers:R01DK135597(Huo)); NIH; National Institute of Diabetes and Digestive and Kidney Diseases(grant numbers:DK56942(ABF)); NIH NIDDK(grant numbers:DK56942); NIH(grant numbers:R01DK135597); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10079171","Renal pathology;image segmentation;multi-label;multi-scale;semi-supervised learning","Image segmentation;Pathology;Neural networks;Task analysis;Head;Kidney;Image coding","Humans;Animals;Mice;Kidney;Neural Networks, Computer;Image Processing, Computer-Assisted","7","","51","IEEE","23 Mar 2023","","","IEEE","IEEE Journals"
"A Multi-Modal, Discriminative and Spatially Invariant CNN for RGB-D Object Labeling","U. Asif; M. Bennamoun; F. A. Sohel","IBM Research, Melbourne, Vic, Australia; University of Western Australia, Crawley, WA, Australia; Murdoch University, Murdoch, WA, Australia",IEEE Transactions on Pattern Analysis and Machine Intelligence,"3 Aug 2018","2018","40","9","2051","2065","While deep convolutional neural networks have shown a remarkable success in image classification, the problems of inter-class similarities, intra-class variances, the effective combination of multi-modal data, and the spatial variability in images of objects remain to be major challenges. To address these problems, this paper proposes a novel framework to learn a discriminative and spatially invariant classification model for object and indoor scene recognition using multi-modal RGB-D imagery. This is achieved through three postulates: 1) spatial invariance-this is achieved by combining a spatial transformer network with a deep convolutional neural network to learn features which are invariant to spatial translations, rotations, and scale changes, 2) high discriminative capability-this is achieved by introducing Fisher encoding within the CNN architecture to learn features which have small inter-class similarities and large intra-class compactness, and 3) multi-modal hierarchical fusion-this is achieved through the regularization of semantic segmentation to a multi-modal CNN architecture, where class probabilities are estimated at different hierarchical levels (i.e., imageand pixel-levels), and fused into a Conditional Random Field (CRF)-based inference hypothesis, the optimization of which produces consistent class labels in RGB-D images. Extensive experimental evaluations on RGB-D object and scene datasets, and live video streams (acquired from Kinect) show that our framework produces superior object and scene classification results compared to the state-of-the-art methods.","1939-3539","","10.1109/TPAMI.2017.2747134","Australian Research Council(grant numbers:DP150100294,DE120102960); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8022892","RGB-D object recognition;3D scene labeling;semantic segmentation","Three-dimensional displays;Labeling;Solid modeling;Proposals;Semantics;Image reconstruction;Computational modeling","","41","","45","IEEE","30 Aug 2017","","","IEEE","IEEE Journals"
"GrNT: Gate-Regularized Network Training for Improving Multi-Scale Fusion in Medical Image Segmentation","Y. Zhang; P. Gu; Y. Zhang; C. Wang; D. Z. Chen","Nanjing University of Science and Technology, Nanjing, Jiangsu, China; Department of Computer Science and Engineering, University of Notre Dame, Notre Dame, IN, USA; Department of Computer Science and Engineering, University of Notre Dame, Notre Dame, IN, USA; Department of Computer Science and Engineering, University of Notre Dame, Notre Dame, IN, USA; Department of Computer Science and Engineering, University of Notre Dame, Notre Dame, IN, USA",2023 IEEE 20th International Symposium on Biomedical Imaging (ISBI),"1 Sep 2023","2023","","","1","5","Multi-scale fusion is a key for semantic segmentation of medical images. Recent deep learning methods added network complexity to achieve better multi-scale fusion results. However, given the already very expressive architectures (e.g., U-Net), an interesting question is whether additional complexity is necessary for achieving more robust multi-scale fusion performance. In this paper, we proposed a new method for improving the multi-scale fusion performance of a medical image segmentation model. We create a set of binary gates at fusing locations that allow us to control forward and backward flows in training. A gate on-off schedule is imposed so that high-scale (level) features could drive the generation of segmentation, while low-scale features serve as complimentary for reconstructing shape details. Our method is effective, easy to implement, and occurs no extra cost when deploying the trained model. Experiments show that our gate-regularized network training (GrNT) improves widely adopted models (e.g., U-Net, Attention U-Net, and DenseVoxNet) on three segmentation datasets (2017 ISIC Skin Lesion segmentation (2D), MM-WHS CT (3D), and 2016 HVSMR (3D)).","1945-8452","978-1-6654-7358-3","10.1109/ISBI53787.2023.10230431","National Natural Science Foundation of China; Natural Science Foundation of Jiangsu Province; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10230431","","Training;Image segmentation;Solid modeling;Three-dimensional displays;Shape;Semantic segmentation;Logic gates","","2","","24","IEEE","1 Sep 2023","","","IEEE","IEEE Conferences"
"Robust Prototypical Few-Shot Organ Segmentation With Regularized Neural-ODEs","P. Pandey; M. Chasmai; T. Sur; B. Lall","Department of Electrical Engineering and the Department of Computer Science, IIT Delhi, New Delhi, India; Department of Electrical Engineering and the Department of Computer Science, IIT Delhi, New Delhi, India; Chennai Mathematical Institute, Chennai, India; Department of Electrical Engineering and the Department of Computer Science, IIT Delhi, New Delhi, India",IEEE Transactions on Medical Imaging,"31 Aug 2023","2023","42","9","2490","2501","Despite the tremendous progress made by deep learning models in image semantic segmentation, they typically require large annotated examples, and increasing attention is being diverted to problem settings like Few-Shot Learning (FSL) where only a small amount of annotation is needed for generalisation to novel classes. This is especially seen in medical domains where dense pixel-level annotations are expensive to obtain. In this paper, we propose Regularized Prototypical Neural Ordinary Differential Equation (R-PNODE), a method that leverages intrinsic properties of Neural-ODEs, assisted and enhanced by additional cluster and consistency losses to perform Few-Shot Segmentation (FSS) of organs. R-PNODE constrains support and query features from the same classes to lie closer in the representation space thereby improving the performance over the existing Convolutional Neural Network (CNN) based FSS methods. We further demonstrate that while many existing Deep CNN-based methods tend to be extremely vulnerable to adversarial attacks, R-PNODE exhibits increased adversarial robustness for a wide array of these attacks. We experiment with three publicly available multi-organ segmentation datasets in both in-domain and cross-domain FSS settings to demonstrate the efficacy of our method. In addition, we perform experiments with seven commonly used adversarial attacks in various settings to demonstrate R-PNODE’s robustness. R-PNODE outperforms the baselines for FSS by significant margins and also shows superior performance for a wide array of attacks varying in intensity and design.","1558-254X","","10.1109/TMI.2023.3258069","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10073652","Few-shot segmentation;Neural-ODEs;adversarial robustness;medical image segmentation","Image segmentation;Robustness;Perturbation methods;Medical diagnostic imaging;Training;Feature extraction;Annotations","Image Processing, Computer-Assisted;Neural Networks, Computer;Semantics","5","","61","IEEE","16 Mar 2023","","","IEEE","IEEE Journals"
"Delineation of agricultural fields using Psi-Net from high-resolution remote sensing images","J. Long; M. Li; M. Cha; X. Wang; H. Huang","The Academy of Digital China (Fujian), Fuzhou University, Fuzhou, China; The Academy of Digital China (Fujian), Fuzhou University, Fuzhou, China; The Academy of Digital China (Fujian), Fuzhou University, Fuzhou, China; The Academy of Digital China (Fujian), Fuzhou University, Fuzhou, China; Fujian Geologic Surveying and Mapping Institute, Fuzhou, China",2022 10th International Conference on Agro-geoinformatics (Agro-Geoinformatics),"23 Aug 2022","2022","","","1","5","Boundary information of agricultural fields is essential to many agricultural applications, particularly at the field level. This paper investigates the use of high-resolution remote sensing images to delineate the boundaries of agricultural fields. We consider the delineation task a multi-task semantic segmentation problem and use a recent deep neural network, i.e., Psi-Net, to do the semantic segmentation. The structure of a Psi-Net consists of an encoder and three decoders. The decoders learn three parallel tasks, corresponding to a primary task (i.e., mask prediction) and two additional tasks (i.e., contour detection and distance map estimation). The additional tasks are used to regularize the mask prediction to produce a refined mask with smooth boundaries. We conducted experiments on a GF1 PMS satellite image (2m) acquired in the 21st regiment of the 2nd agricultural division of Xinjiang. To evaluate the effectiveness of the proposed method, we compared it with existing single task semantic segmentation using UNet. Our results show that the proposed method using Psi-Net performed better than the existing method from the perspective of geometric and attribute accuracies. We conclude that the proposed Psi-Net method has a high potential for extracting field boundaries from high-resolution remote sensing images.","","978-1-6654-7078-0","10.1109/Agro-Geoinformatics55649.2022.9858952","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9858952","agricultural fields delineation;high-resolution remote sensing images;multi-task semantic segmentation;Psi-Net","Learning systems;Image segmentation;Satellites;Image resolution;Semantics;Neural networks;Multitasking","","","","11","IEEE","23 Aug 2022","","","IEEE","IEEE Conferences"
"R2I: A Deep Fault Diagnosis Semantic Segmentation System Based on Knowledge Distillation in Photovoltaic Panels","X. Lei; K. Ma; J. Zhao","Department of Electrical and Electronic Engineering, University of Liverpool, Liverpool, United Kingdom; Department of Electrical and Electronic Engineering, University of Liverpool, Liverpool, United Kingdom; Department of Computer Science, Xi'an Jiaotong-liverpool University, Suzhou, China","2023 3rd International Conference on Energy, Power and Electrical Engineering (EPEE)","25 Dec 2023","2023","","","1062","1066","With the growing adoption of photovoltaic power systems, ensuring the reliable and efficient operation of photovoltaic panels is critical. However, defects such as cracks, dirt, and failures are common during long-term use. Manual inspection is subjective and resource intensive. Recent computer vision and machine learning techniques present new opportunities for automated diagnosis. This paper reviews photovoltaic panel defect detection using deep learning. Two main approaches exist: visible light and infrared imaging. Earlier works employed object detection with lower precision. Newer methods use semantic segmentation for pixel-level accuracy. However, most focus on single modalities and datasets. Transfer learning across diverse data could improve generalization. Few works estimate defect areas, needed for automated analysis. This paper proposes a dual-modality semantic segmentation approach with knowledge transfer learning. It shares representations across visible and infrared datasets via distillation. A lightweight network reduces parameters and computations for deployment. Soft targets provide latent features transferable to new data. An algorithm estimates defect area proportions to assess hazard levels. The multi-modal transferable learning system aims to advance automated, accurate photovoltaic panel defect diagnosis. Key innovations include tackling insufficient data via shared knowledge, reducing model complexity, improving generalization, and pioneering defect severity assessment. This can facilitate intelligent photovoltaic inspection to ensure system health, safety and longevity.","","979-8-3503-1818-0","10.1109/EPEE59859.2023.10351967","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10351967","component;Power System Fault Diagnosis;Power System Security;Smart Grid;Digital Power System","Photovoltaic systems;Training;Technological innovation;Semantic segmentation;Transfer learning;Object detection;Manuals","","","","13","IEEE","25 Dec 2023","","","IEEE","IEEE Conferences"
"Algorithm for Crack Segmentation of Airport Runway Pavement under Complex Background based on Encoder and Decoder","H. Li; P. Jing; R. Huang; Z. Gui","CS Department, Civil Aviation University of China, China; CS Department, Civil Aviation University of China, China; CS Department, Civil Aviation University of China, China; Shanghai Guimu Robot Co. Ltd., China",2021 IEEE International Conference on Robotics and Biomimetics (ROBIO),"28 Mar 2022","2021","","","1706","1711","Automatic detection and segmentation of airport pavement cracks has always been the focus of attention of the field management department. Due to the different background, shape, color and size of cracks, traditional methods cannot accurately extract crack information from the road surface image with complex background. Therefore, this paper proposes a deep learning-based image detection method for cracks pixel-level segmentation. The proposed network is an encoder-decoder network structure. The encoder uses VGG19 as the backbone network to extract crack features. A spatial pyramid pooling module is introduced between the encoder and decoder to obtain the global crack information. The hole convolution and multi-loss supervision function are introduced to obtain a larger receptive field and improve the segmentation effect of small cracks. This model can be used for efficient multi-scale feature extraction, aggregation and resolution reconstruction, thereby greatly enhancing the fracture segmentation capability of the network. Compared with traditional image processing and other deep learning-based crack segmentation methods, this algorithm has higher accuracy and generalization ability on complex background airport pavements, making the automatic detection and monitoring of airport pavement more efficient.","","978-1-6654-0535-5","10.1109/ROBIO54168.2021.9739462","Research and Development; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9739462","","Image segmentation;Convolution;Shape;Roads;Feature extraction;Airports;Real-time systems","","2","","14","IEEE","28 Mar 2022","","","IEEE","IEEE Conferences"
"GraphRegNet: Deep Graph Regularisation Networks on Sparse Keypoints for Dense Registration of 3D Lung CTs","L. Hansen; M. P. Heinrich","Institute of Medical Informatics, Universitát zu Lübeck, Lübeck, Germany; Institute of Medical Informatics, Universitát zu Lübeck, Lübeck, Germany",IEEE Transactions on Medical Imaging,"31 Aug 2021","2021","40","9","2246","2257","In the last two years learning-based methods have started to show encouraging results in different supervised and unsupervised medical image registration tasks. Deep neural networks enable (near) real time applications through fast inference times and have tremendous potential for increased registration accuracies by task-specific learning. However, estimation of large 3D deformations, for example present in inhale to exhale lung CT or interpatient abdominal MRI registration, is still a major challenge for the widely adopted U-Net-like network architectures. Even when using multi-level strategies, current state-of-the-art DL registration results do not yet reach the high accuracy of conventional frameworks. To overcome the problem of large deformations for deep learning approaches, in this work, we present GraphRegNet, a sparse keypoint-based geometric network for dense deformable medical image registration. Similar to the successful 2D optical flow estimation of FlowNet or PWC-Net we leverage discrete dense displacement maps to facilitate the registration process. In order to cope with enormously increasing memory requirements when working with displacement maps in 3D medical volumes and to obtain a well-regularised and accurate deformation field we 1) formulate the registration task as the prediction of displacement vectors on a sparse irregular grid of distinctive keypoints and 2) introduce our efficient GraphRegNet for displacement regularisation, a combination of convolutional and graph neural network layers in a unified architecture. In our experiments on exhale to inhale lung CT registration we demonstrate substantial improvements (TRE below 1.4 mm) over other deep learning methods. Our code is publicly available at https://github.com/multimodallearning/graphregnet.","1558-254X","","10.1109/TMI.2021.3073986","German Research Foundation (DFG)(grant numbers:320997906 (HE 7364/2-1)); German Federal Ministry for Economic Affairs and Energy as part of the AI Space for Intelligence Healthcare Systems (KI SIGS) Consortium(grant numbers:01MK20012B); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9406964","Deformable registration;graph learning;thoracic CT","Lung;Computed tomography;Three-dimensional displays;Biomedical imaging;Feature extraction;Strain;Estimation","Image Processing, Computer-Assisted;Lung;Magnetic Resonance Imaging;Neural Networks, Computer;Tomography, X-Ray Computed","42","","60","IEEE","19 Apr 2021","","","IEEE","IEEE Journals"
"ASC-TRANS: A Hybrid Transformer Network Based on Adaptive Semantic Connection For Desert Boundary Segmentation","Z. Wang; Y. Lv; Y. Zhang","School of Information Science and Engineering, Lanzhou University, Lanzhou, China; School of Information Science and Engineering, Lanzhou University, Lanzhou, China; Northwest Institute of Eco-Environment and Resources, CAS, Lanzhou, China",IGARSS 2024 - 2024 IEEE International Geoscience and Remote Sensing Symposium,"5 Sep 2024","2024","","","8232","8236","Desertification is a serious land degradation process that poses various hazards to economic development and environmental security. Satellite remote sensing images have the characteristics of wide coverage and high resolution. Therefore, using deep neural networks and remote sensing image technology to extract desert boundaries is of great significance for scientific research and sustainable development. Inspired by this, a hybrid network is proposed based on adaptive semantic connections, which enables the model to better capture global context information and texture information while reducing the semantic gap between the encoder and the decoder. Secondly, to address computational complexity and improve model generalization capabilities, a dynamic feature scaling multi-Head self-Attention is introduced. Additionally, convolutional block attention modules with residuals are incorporated into the decoder to facilitate the model in learning relevant features. Finally, a differentiable boundary metric is used as the loss function leads to better performance and more accurate boundary segmentation. We conduct experiments on the Landsat 8 dataset. Through visual interpretation of the experimental results and calculation of evaluation indicators, it can be seen that our model has good results for desert boundary extraction.","2153-7003","979-8-3503-6032-5","10.1109/IGARSS53475.2024.10642585","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10642585","Deep learning;semantic segmentation;remote sensing;adaptive semantic connection","Measurement;Adaptation models;Accuracy;Satellites;Laser radar;Computational modeling;Semantics","","","","14","IEEE","5 Sep 2024","","","IEEE","IEEE Conferences"
"TransDUNet:Image segmentation of artificial joint based on TransUNet","J. Tang","College of Computer Science, Sichuan University, Chengdu, China",2023 8th International Conference on Intelligent Computing and Signal Processing (ICSP),"19 Sep 2023","2023","","","1755","1758","Traditional convolution has limited ability to capture long-range dependencies of images, which is crucial for semantic segmentation in medical images, where spatial relationships between different anatomical structures are compound and non-local. The long-range dependencies in medical images can be captured by multi-head self-attention mechanisms, which is essential for complex and non-local spatial relationships between different anatomical structures. The receptive field of standard up-sampling convolutions is limited, which can affect the accuracy of segmentation predictions. Dilated convolutions can expand the receptive field and capture multi-scale contextual information. This paper proposes a new deep learning architecture called TransDUNet for semantic segmentation of artificial joint implants in medical images. Based on TransUNet, We replace the traditional convolution in the encoder up-sampling module of TransUNet with the module consisting of multiple dilated convolutions with different dilation rates to Incorporating multi-scale information, Dense skip connections are utilized to enhance feature reuse and gradient flow, thereby improving the performance and generalization ability of the model. The proposed algorithm’s segmentation performance outperforms recently developed networks, according to experiments, which demonstrate that its mDSC and mIoU are 94.1% and 88.68%, respectively.","","979-8-3503-0245-5","10.1109/ICSP58490.2023.10248944","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10248944","component;;images of artificial joint prosthesis;dense skip connection;dilated convolution;Transformer;UNet","Convolution;Semantic segmentation;Signal processing algorithms;Deep architecture;Anatomical structure;Implants;Transformers","","","","13","IEEE","19 Sep 2023","","","IEEE","IEEE Conferences"
"Multi-Scale Regularized Deep Network for Retinal Vessel Segmentation","V. Cherukuri; V. K. B.G.; R. Bala; V. Monga","Dept. of Electrical Engineering, Pennsylvania State University, University Park, PA, USA; Palo Alto Research Center, CA; Palo Alto Research Center, Palo Alto, CA, USA; Dept. of Electrical Engineering, Pennsylvania State University, University Park, PA, USA",2019 IEEE International Conference on Image Processing (ICIP),"26 Aug 2019","2019","","","824","828","Vessel segmentation of retinal images is a key diagnostic capability in ophthalmology. Early approaches addressing this problem employed hand-crafted filters to capture vessel structures, accompanied by morphological processing. More recently, deep learning techniques have been employed to significantly enhance segmentation accuracy. We propose a novel domain enriched deep network that consists of two components: 1) a representation network which learns geometric (specifically curvilinear) features that are tailored to retinal images, followed by 2) a task network that utilizes the features obtained from the representation layer to perform pixel-level segmentation. The representation and task networks are learned jointly for any given training set. To obtain effective representation filters, we develop a new orientation constraint that enables geometric diversity of curvilinear features. A multi-scale extension is further developed to enhance segmentation of thin vessels. Experiments performed on two challenging benchmark databases reveal that the proposed regularized deep network can outperform state of the art alternatives as measured by common evaluation metrics. Further, the proposed method exhibits a more graceful decay in performance as training data is reduced.","2381-8549","978-1-5386-6249-6","10.1109/ICIP.2019.8803762","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8803762","segmentation;deep learning;priors.","Image segmentation;Task analysis;Deep learning;Feature extraction;Retina;Training;Measurement","","4","","32","IEEE","26 Aug 2019","","","IEEE","IEEE Conferences"
"ToolNet: Holistically-nested real-time segmentation of robotic surgical tools","L. C. García-Peraza-Herrera; W. Li; L. Fidon; C. Gruijthuijsen; A. Devreker; G. Attilakos; J. Deprest; E. V. Poorten; D. Stoyanov; T. Vercauteren; S. Ourselin","Dept. Med. Phys. Biomed. Eng., CMIC, UK; Dept. Med. Phys. Biomed. Eng., CMIC, UK; Dept. Med. Phys. Biomed. Eng., CMIC, UK; Katholieke Universiteit, Leuven, Belgium; Katholieke Universiteit, Leuven, Belgium; University College London Hospitals, UK; Universitair Ziekenhuis, Leuven, Belgium; Katholieke Universiteit Leuven, Leuven, Flanders, BE; Dept. Computer Science, CMIC, UK; Dept. Med. Phys. Biomed. Eng., CMIC, UK; Dept. Med. Phys. Biomed. Eng., CMIC, UK",2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),"14 Dec 2017","2017","","","5717","5722","Real-time tool segmentation from endoscopic videos is an essential part of many computer-assisted robotic surgical systems and of critical importance in robotic surgical data science. We propose two novel deep learning architectures for automatic segmentation of non-rigid surgical instruments. Both methods take advantage of automated deep-learning-based multi-scale feature extraction while trying to maintain an accurate segmentation quality at all resolutions. The two proposed methods encode the multi-scale constraint inside the network architecture. The first proposed architecture enforces it by cascaded aggregation of predictions and the second proposed network does it by means of a holistically-nested architecture where the loss at each scale is taken into account for the optimization process. As the proposed methods are for real-time semantic labeling, both present a reduced number of parameters. We propose the use of parametric rectified linear units for semantic labeling in these small architectures to increase the regularization of the network while maintaining the segmentation accuracy. We compare the proposed architectures against state-of-the-art fully convolutional networks. We validate our methods using existing benchmark datasets, including ex vivo cases with phantom tissue and different robotic surgical instruments present in the scene. Our results show a statistically significant improved Dice Similarity Coefficient over previous instrument segmentation methods. We analyze our design choices and discuss the key drivers for improving accuracy.","2153-0866","978-1-5386-2682-5","10.1109/IROS.2017.8206462","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8206462","","Instruments;Robots;Image segmentation;Tools;Surgery;Real-time systems;Computer architecture","","95","","27","IEEE","14 Dec 2017","","","IEEE","IEEE Conferences"
"Coal-Rock Image Recognition Method for Complex and Harsh Environment in Coal Mine Using Deep Learning Models","S. Chuanmeng; L. Xinyu; C. Jiaxin; W. Zhibo; L. Yong","State Key Laboratory of Dynamic Measurement Technology, North University of China, Taiyuan, China; State Key Laboratory of Dynamic Measurement Technology, North University of China, Taiyuan, China; State Key Laboratory of Dynamic Measurement Technology, North University of China, Taiyuan, China; State Key Laboratory of Dynamic Measurement Technology, North University of China, Taiyuan, China; State Key Laboratory of Coal Mine Disaster Dynamics and Control, Chongqing University, Chongqing, China",IEEE Access,"7 Aug 2023","2023","11","","80794","80805","The unfavorable factors of underground coal such as dark light, uneven illumination, band shadowing greatly make it difficult to recognize the coal rock at the mining workface accurately. To solve this problem, this paper proposes the fuse attention mechanism’s coal rock full-scale network (FAM-CRFSN) model. The deep extraction of coal rock semantic features is achieved by a multi-channel residual attention mechanism and a full-scale connection structure. Meanwhile, the balance between “deep” stacking and error back propagation is achieved by structures such as dilated convolution and Res2Block. Besides, a multi-dimensional loss function consisting of the cross-entropy loss, intersection over union, and multiscale structure similarity loss with pixel-level, area-level, and image-level expressions is established. Finally, the performance of the FAM-CRFSN network is tested with RGB coal rock images collected from an underground coal mining workface and superimposed with different proportions of gaussian noise and salt & pepper noise. The experimental results show that the FAM-CRFSN model can segment the coal rock regions accurately; at a noise intensity of 0.09, it achieves an MIOU of 85.77% and an MPA of 92.12%. Also, it achieves better accuracy and generalization performance than the mainstream semantic segmentation models. This study provides an important theoretical basis for promotes the unmanned and intelligent mining workface.","2169-3536","","10.1109/ACCESS.2023.3300243","National Key Research and Development Program of China(grant numbers:2022YFC2905700); Fundamental Research Program of Shanxi Province(grant numbers:202203021221106); Shanxi Science and Technology Achievement Transformation Guidance Special Project(grant numbers:202104021301061); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10197425","Intelligent mining;automatic recognition of coal rock;semantic segmentation;low-illuminance image segmentation","Feature extraction;Rocks;Convolution;Coal;Semantics;Interference;Image segmentation;Mining industry;Intelligent systems;Coal mining;Semantic segmentation;Lighting control","","5","","39","CCBYNCND","31 Jul 2023","","","IEEE","IEEE Journals"
"MRFP: Learning Generalizable Semantic Segmentation from Sim-2-Real with Multi-Resolution Feature Perturbation","S. Udupa; P. Gurunath; A. Sikdar; S. Sundaram","Department of Aerospace Engineering, Indian Institute of Science, Bengaluru, India; Department of Aerospace Engineering, Indian Institute of Science, Bengaluru, India; Robert Bosch Centre for Cyber-Physical Systems, Indian Institute of Science, Bengaluru, India; Department of Aerospace Engineering, Indian Institute of Science, Bengaluru, India",2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),"16 Sep 2024","2024","","","5904","5914","Deep neural networks have shown exemplary performance on semantic scene understanding tasks on source domains, but due to the absence of style diversity during training, enhancing performance on unseen target domains using only single source domain data remains a challenging task. Generation of simulated data is a feasible alternative to retrieving large style-diverse real-world datasets as it is a cumbersome and budget-intensive process. However, the large domain-specific inconsistencies between simulated and real-world data pose a significant generalization challenge in semantic segmentation. In this work, to alleviate this problem, we propose a novel Multi-Resolution Feature Perturbation (MRFP) technique to randomize domain-specific fine-grained features and perturb style of coarse features. Our experimental results on various urban-scene segmentation datasets clearly indicate that, along with the perturbation of style-information, perturbation of fine-feature components is paramount to learn domain invariant robust feature maps for semantic segmentation models. MRFP is a simple and computationally efficient, transferable module with no additional learnable parameters or objective functions, that helps state-of-the-art deep neural networks to learn robust domain invariant features for simulation-to-real semantic segmentation. Code is available at https://github.com/airl-iisc/MRFP.","2575-7075","979-8-3503-5300-6","10.1109/CVPR52733.2024.00564","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10657982","","Training;Computer vision;Perturbation methods;Semantic segmentation;Computational modeling;Semantics;Artificial neural networks","","1","","56","IEEE","16 Sep 2024","","","IEEE","IEEE Conferences"
"Application of CSGE-PSPnet Remote Sensing Image Semantic Segmentation Technology in Transmission and Transformation Engineering Design","B. Li; K. Sun; Y. Lang; L. Guan; F. Lu; Y. Zhu","Design and Consulting Ceter State Grid Economic and Technological Research Institute CO., LTD, Beijing, China; Engineering and Degital Certer Powerchina Sichuan Electric Power Engineering Co., LTD., Chengdu, China; Geographic Information Center Powerchina Sichuan Electric Power Engineering Co., LTD., Chengdu, China; Design and Consulting Ceter State Grid Economic and Technological Research Institute CO., LTD, Beijing, China; Design and Consulting Ceter State Grid Economic and Technological Research Institute CO., LTD, Beijing, China; Planning and Operations Department, Economic and Technological, Research Institute of State Grid Shandong Electric Power Company, Jinan, China",2022 Asian Conference on Frontiers of Power and Energy (ACFPE),"29 Nov 2022","2022","","","85","89","With the breakthrough development of remote sensing technology and deep learning, remote sensing technology also has a broad application prospect in the field of transmission and transformation engineering design. Semantic segmentation, as a key link in remote sensing image processing, is the basis for improving the intelligence of transmission and substation engineering, for siting, construction, maintenance, and monitoring of transmission and substation engineering by accurately identifying important facilities, sensitive locations and environmental changes. Therefore, this paper proposes a remote sensing image semantic segmentation model(CSGE-PSPnet), which is based on PSPNet(Pyramid Scene Parsing Network) and SGE(Spatial Group-Wise Enhance Attention). CSGE-PSPnet could enhance local significance features while extracting global features, realizes the refined extraction of key information, and designs CSGE Block(Spatial-Channel Group-Wise Enhance Attention) to extract channel and spatial local information at the same time to enhance local significance features. Multi-level features are fused through dense connection structures to retain feature information of small-scale targets. Subsequently, the deep global features are mined by PSPNet on the feature map after fusion, and the contextual information of different regions is fused to obtain the segmentation results. In addition, one auxiliary loss layer and one major loss layer further improve the model's generalization ability. The experimental results show that CSGE-PSPnet is superior to other methods and can accurately identify small targets, and CSGE Block significantly enhances the feature representation.","","978-1-6654-7084-1","10.1109/ACFPE56003.2022.9952233","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9952233","power transmission and transformation engineering;site selection;semantic segmentation of remote sensing images;mix domain attention;PSPNet","Substations;Target recognition;Semantic segmentation;Image edge detection;Power transmission;Maintenance engineering;Feature extraction","","","","23","IEEE","29 Nov 2022","","","IEEE","IEEE Conferences"
"A Method Based on Multi-Scale Consistency Regularization and Color-Spatial Constraints for Road Segmentation with Noisy Labels","Y. Shen; N. Su; C. Zhao; W. Hou; B. Huang; Y. Yan","College of Information and Communication Engineering, Harbin Engineering University, Harbin, P. R. China; College of Information and Communication Engineering, Harbin Engineering University, Harbin, P. R. China; College of Information and Communication Engineering, Harbin Engineering University, Harbin, P. R. China; Harbin Space Star Data System Technology Co., Ltd., Harbin, P. R. China; Jushri Technologies, Inc., Shanghai, P. R. China; College of Information and Communication Engineering, Harbin Engineering University, Harbin, P. R. China",IGARSS 2023 - 2023 IEEE International Geoscience and Remote Sensing Symposium,"20 Oct 2023","2023","","","5033","5036","Road segmentation methods based on (Convolutional Neural Networks, CNN) commonly require accurate pixel-level labels. However, accurate labels are hard to obtain in some cases. To address this issue, a method based on multi-scale consistency regularization and color-spatial constraints is proposed for road segmentation with noisy labels. First, a multi-scale consistency regularization is devised to improve the multi-scale aggregation capability and noise immunity of model. In addition, we utilize the color and spatial information of input images to constrain the model predictions, thereby giving the model a more reliable learning objective. In the experimental section, the accuracy and visualization results obtained from two road datasets provide compelling evidence of the efficacy of our method in addressing the challenges posed by noisy labels in road segmentation tasks.","2153-7003","979-8-3503-2010-7","10.1109/IGARSS52108.2023.10282699","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10282699","Road segmentation;CNN;noisy labels;multi-scale aggregation;noise immunity","Image segmentation;Visualization;Technological innovation;Image color analysis;Roads;Predictive models;Noise measurement","","","","23","IEEE","20 Oct 2023","","","IEEE","IEEE Conferences"
"ConvNeXt-Mask2Former: A Semantic Segmentation Model for Land Classification in Remote Sensing Images","J. Zheng","Big Data and Software Engineering Department, Zhejiang Wanli University, ZJWL, Ningbo, China","2024 5th International Conference on Computer Vision, Image and Deep Learning (CVIDL)","26 Jul 2024","2024","","","676","682","With the rapid development of remote sensing technology and deep learning, significant progress has been made in land classification of remote sensing images using semantic segmentation models. However, it still faces some deficiencies and challenges in terms of multi-scale features of images, the computational burden of high resolution images, and long-range dependencies. Therefore, in this paper, I design a semantic segmentation network based on encoder and decoder (Encoder-Decoder) architecture, using ConvNeXt as the encoder, i.e., the backbone network, and Mask2Former as the decoder, with the addition of an auxiliary segmentation module (ASM), and at the same time, an image enhancement pipeline is used to assist the training in the image preprocessing stage. Mask2Former is an architecture with good flexibility and generalization ability, which can be easily used in different visual tasks. At the same time, as a convolutional network without the addition of an attention mechanism, ConvNeXt has excellent feature extraction capabilities. I evaluate my model on the widely used high resolution image datasets Potsdam and Vaihinge and compare the results with the current mainstream semantic segmentation models, and the experimental results show that my model can be optimized for the accurate segmentation of land categories. Specifically, my model has an optimal mPA of $\mathbf{9 3 . 7 9 \%}$ and mIoU of $86.80 \%$ on the Potsdam dataset, and an optimal mPA of $82.32 \%$ and mIoU of $\mathbf{9 0 . 0 5 \%}$ on the Vaihinge dataset, which is both significantly higher than the existing models, and provide important references for land classification tasks and land use.","","979-8-3503-7382-0","10.1109/CVIDL62147.2024.10603728","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10603728","ConvNeXt;Mask2Former;semantic segmentation;remote sensing images;land classification","Training;Image resolution;Semantic segmentation;Computational modeling;Land surface;Computer architecture;Decoding","","1","","26","IEEE","26 Jul 2024","","","IEEE","IEEE Conferences"
"Scribble-Supervised Cell Segmentation Using Multiscale Contrastive Regularization","H. -J. Oh; K. Lee; W. -K. Jeong","Department of Computer Science and Engineering, Korea University, Seoul, Korea; School of Electrical and Computer Engineering, UNIST, Ulsan, Korea; Department of Computer Science and Engineering, Korea University, Seoul, Korea",2022 IEEE 19th International Symposium on Biomedical Imaging (ISBI),"26 Apr 2022","2022","","","1","5","Current state-of-the-art supervised deep learning-based segmentation approaches have demonstrated superior performance in medical image segmentation tasks. However, such supervised approaches require fully annotated pixel-level ground-truth labels, which are labor-intensive and time-consuming to acquire. Recently, Scribble2Label (S2L) demonstrated that using only a handful of scribbles with self-supervised learning can generate accurate segmentation results without full annotation. However, owing to the relatively small size of scribbles, the model is prone to overfit and the results may be biased to the selection of scribbles. In this work, we address this issue by employing a novel multiscale contrastive regularization term for S2L. The main idea is to extract features from intermediate layers of the neural network for contrastive loss so that structures at various scales can be effectively separated. To verify the efficacy of our method, we conducted ablation studies on well-known datasets, such as Data Science Bowl 2018 and MoNuSeg. The results show that the proposed multiscale contrastive loss is effective in improving the performance of S2L, which is comparable to that of the supervised learning segmentation method.","1945-8452","978-1-6654-2923-8","10.1109/ISBI52829.2022.9761608","National Research Foundation; Health; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9761608","Contrastive Learning;Weakly Supervised Learning;Cell Segmentation","Image segmentation;Image resolution;Annotations;Supervised learning;Neural networks;Data science;Feature extraction","","6","","18","IEEE","26 Apr 2022","","","IEEE","IEEE Conferences"
"Mobile-Seed: Joint Semantic Segmentation and Boundary Detection for Mobile Robots","Y. Liao; S. Kang; J. Li; Y. Liu; Y. Liu; Z. Dong; B. Yang; X. Chen","Wuhan University, Wuhan, China; Technical University of Munich, München, Germany; Wuhan University, Wuhan, China; King's College London, London, U.K.; Institute of Infocomm Research (I2R), A*STAR, Singapore; Wuhan University, Wuhan, China; Wuhan University, Wuhan, China; National University of Defense Technology, Changsha, China",IEEE Robotics and Automation Letters,"14 Mar 2024","2024","9","4","3902","3909","Precise and rapid delineation of sharp boundaries and robust semantics is essential for numerous downstream robotic tasks, such as robot grasping and manipulation, real-time semantic mapping, and online sensor calibration performed on edge computing units. Although boundary detection and semantic segmentation are complementary tasks, most studies focus on lightweight models for semantic segmentation but overlook the critical role of boundary detection. In this work, we introduce Mobile-Seed, a lightweight, dual-task framework tailored for simultaneous semantic segmentation and boundary detection. Our framework features a two-stream encoder, an active fusion decoder (AFD) and a dual-task regularization approach. The encoder is divided into two pathways: one captures category-aware semantic information, while the other discerns boundaries from multi-scale features. The AFD module dynamically adapts the fusion of semantic and boundary information by learning channel-wise relationships, allowing for precise weight assignment of each channel. Furthermore, we introduce a regularization loss to mitigate the conflicts in dual-task learning and deep diversity supervision. Compared to existing methods, the proposed Mobile-Seed offers a lightweight framework to simultaneously improve semantic segmentation performance and accurately locate object boundaries. Experiments on the Cityscapes dataset have shown that Mobile-Seed achieves notable improvement over the state-of-the-art (SOTA) baseline by 2.2 percentage points (pp) in mIoU and 4.2 pp in mF-score, while maintaining an online inference speed of 23.9 frames-per-second (FPS) with 1024 $\boldsymbol{\times }$ 2048 resolution input on an RTX 2080 Ti GPU. Additional experiments on CamVid and PASCAL Context datasets confirm our method's generalizability.","2377-3766","","10.1109/LRA.2024.3373235","National Natural Science Foundation of China(grant numbers:42201477,42130105); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10461032","Deep learning for visual perception;visual learning;deep learning methods","Semantics;Semantic segmentation;Task analysis;Feature extraction;Decoding;Streaming media;Head","","1","","39","IEEE","8 Mar 2024","","","IEEE","IEEE Journals"
"An Efficient Deep Representation Based Framework for Large-Scale Terrain Classification","Y. Yan; A. Rangarajan; S. Ranka","University of Florida, Gainesville, Florida, USA; University of Florida, Gainesville, Florida, USA; University of Florida, Gainesville, FL, US",2018 24th International Conference on Pattern Recognition (ICPR),"29 Nov 2018","2018","","","940","945","In this paper, we present a novel terrain classification framework for large-scale remote sensing images. A well-performing multi-scale superpixel tessellation based segmentation approach is employed to generate homogeneous and irregularly shaped regions, and a transfer learning technique is sequentially deployed to derive representative deep features by utilizing successful pre-trained convolutional neural network (CNN) models. This design is aimed to overcome the big problem of lacking available ground-truth data and to increase the generalization power of the multi-pixel descriptor. In the subsequent classification step, we train a fast and robust support vector machine (SVM) to assign the pixel-level labels. Its maximum-margin property can be easily combined with a graph Laplacian propagation approach. Moreover, we analyze the advantages of applying a feature selection technique to the deep CNN features which are extracted by transfer learning. In the experiments, we evaluate the whole framework based on different geographical types. Compared with other region-based classification methods, the results show that our framework can obtain state-of-the-art performance w.r.t. both classification accuracy and computational efficiency.","1051-4651","978-1-5386-3788-3","10.1109/ICPR.2018.8545021","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8545021","remote sensing;superpixel segmentation;convolutional neural network;transfer learning;feature selection;semi-supervised learning","Feature extraction;Image segmentation;Training;Labeling;Image resolution;Remote sensing","","2","","25","IEEE","29 Nov 2018","","","IEEE","IEEE Conferences"
"MLSL: Multi-Level Self-Supervised Learning for Domain Adaptation with Spatially Independent and Semantically Consistent Labeling","J. Iqbal; M. Ali","Information Technology University, Pakistan; Information Technology University, Pakistan",2020 IEEE Winter Conference on Applications of Computer Vision (WACV),"14 May 2020","2020","","","1853","1862","Most of the recent Deep Semantic Segmentation algorithms suffer from large generalization errors, even when powerful hierarchical representation models, based on convolutional neural networks, have been employed. This could be attributed to limited training data and large distribution gap in train and test domain datasets. In this paper, we propose a multi-level self-supervised learning model for domain adaptation of semantic segmentation. Exploiting the idea that an object (and most of the stuff given context) should be labeled consistently regardless of its location, we generate spatially independent and semantically consistent (SISC) pseudo-labels by segmenting multiple sub-images using base model and designing an aggregation strategy. Image level pseudo weak-labels, PWL, are computed to guide domain adaptation by capturing global context similarity in source and target domain at latent space level. Thus helping latent space learn the representation even when there are very few pixels belonging to the domain category (small object for example) compared to rest of the image. Our multi-level Self-supervised learning (MLSL) outperforms existing state-of-art (self or adversarial learning) algorithms. Specifically, keeping all setting similar and employing MLSL we obtain an mIoUgain of 5.1% on GTA-V to Cityscapes adaptation and 4.3% on SYNTHIA to Cityscapes adaptation compared to existing state-of-art method.","2642-9381","978-1-7281-6553-0","10.1109/WACV45572.2020.9093626","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9093626","","Semantics;Image segmentation;Adaptation models;Training;Computational modeling;Task analysis;Roads","","24","","35","IEEE","14 May 2020","","","IEEE","IEEE Conferences"
"Multiscale Attention Feature Aggregation Network for Cloud and Cloud Shadow Segmentation","K. Chen; M. Xia; H. Lin; M. Qian","Collaborative Innovation Center on Atmospheric Environment and Equipment Technology, Jiangsu Key Laboratory of Big Data Analysis Technology (B-DAT), Nanjing University of Information Science and Technology, Nanjing, China; Collaborative Innovation Center on Atmospheric Environment and Equipment Technology, Jiangsu Key Laboratory of Big Data Analysis Technology (B-DAT), Nanjing University of Information Science and Technology, Nanjing, China; College of Information Science and Technology, Nanjing Forestry University, Nanjing, China; State Key Laboratory of Information Engineering in Surveying, Mapping and Remote Sensing, Wuhan University (LIESMARS), Wuhan University, Wuhan, China",IEEE Transactions on Geoscience and Remote Sensing,"20 Jun 2023","2023","61","","1","16","Cloud and cloud shadow segmentation is one of the most important problems in remote sensing image processing. Due to the vulnerability to ground object interference, noise interference and other factors, and the lack of generalization ability, traditional deep learning networks would inevitably lose details and spatial information, resulting in imprecise segmentation of cloud and cloud shadow boundaries, missing detection, and false detection. In order to solve these problems, a multiscale attention feature aggregation network is proposed, which extracts semantic information at different levels based on the residual network. A multiscale strip pooling attention module is designed to extract multiscale context information and deep spatial channel information. In order to improve the feature extraction of the model, the deep multihead feedforward transfer attention module is introduced to enable the adjacent two layers of the backbone network to guide each other for feature mining. Then, a bilateral feature fusion module is used to guide the fusion of low-level semantic information and high-level detail information. Finally, in order to enhance the repair of boundary details of cloud and cloud shadow, a boundary refinement boosting model is designed. The experimental results show that our model can handle complex cloud cover scenes and has excellent performance on the cloud, the cloud shadow dataset, and the cloud and snow dataset based on WorldView2 (CSWV) dataset. Its segmentation accuracy is superior to the existing methods, which is of great significance to the related work of cloud detection.","1558-0644","","10.1109/TGRS.2023.3283435","National Natural Science Foundation of China(grant numbers:42075130); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10144804","Attention;cloud shadow;deep learning;image segmentation","Feature extraction;Cloud computing;Image segmentation;Remote sensing;Semantics;Deep learning;Data mining","","12","","57","IEEE","6 Jun 2023","","","IEEE","IEEE Journals"
"A Deep Model for Joint Object Detection and Semantic Segmentation in Traffic Scenes","J. Peng; Z. Nan; L. Xu; J. Xin; N. Zheng","Institute of Artificial Intelligence and Robotics, Xi’an Jiaotong University, Xi’an, P.R. China; Institute of Artificial Intelligence and Robotics, Xi’an Jiaotong University, Xi’an, P.R. China; Institute of Artificial Intelligence and Robotics, Xi’an Jiaotong University, Xi’an, P.R. China; Institute of Artificial Intelligence and Robotics, Xi’an Jiaotong University, Xi’an, P.R. China; Institute of Artificial Intelligence and Robotics, Xi’an Jiaotong University, Xi’an, P.R. China",2020 International Joint Conference on Neural Networks (IJCNN),"28 Sep 2020","2020","","","1","8","Object detection and semantic segmentation are two fundamental techniques of various applications in the fields of Intelligent Vehicles (IV) and Advanced Driving Assistance System (ADAS). Early studies separately handle these two problems. In this paper, inspired by some recent works, we propose a deep neural network model for joint object detection and semantic segmentation. Given an image, an encoder-decoder convolution network extracts a set of feature maps, these feature maps are shared by the detection branch and the segmentation branch to jointly carry out the object detection and semantic segmentation. In the detection branch, we design a PriorBox initialization mechanism to propose more object candidates. In the segmentation branch, we use the multi-scale atrous convolution to explore the global and local semantic information in traffic scenes. Benefiting from the PriorBox Initialization Mechanism (PBIM) and Multi-Scale Atrous Convolution (MSAC), our model presents the competitive performance. In the experiments, we widely compare with several recently-proposed methods on the public Cityscapes dataset, achieving the highest accuracy. In addition, to verify the robustness and generalization of our model, the extension experiments are also conducted on the well-known VOC2012 dataset.","2161-4407","978-1-7281-6926-2","10.1109/IJCNN48605.2020.9206883","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9206883","traffic scenes;object detection;semantic segmentation","Semantics;Object detection;Decoding;Feature extraction;Image segmentation;Convolution;Task analysis","","10","","35","IEEE","28 Sep 2020","","","IEEE","IEEE Conferences"
"Vector Mapping Method for Buildings in Remote Sensing Images Based on Joint Semantic-Geometric Learning","J. Yin; F. Wu; Y. Qi","Institute of Geospatial Information, PLA Strategic Support Force Information Engineering University, Zhengzhou, China; Institute of Geospatial Information, PLA Strategic Support Force Information Engineering University, Zhengzhou, China; Institute of Geospatial Information, PLA Strategic Support Force Information Engineering University, Zhengzhou, China",IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing,"9 Oct 2023","2023","16","","9067","9076","An important high-precision building vector mapping method automatically delineates building polygons from high-resolution remote sensing images. Deep learning methods have greatly improved the accuracy of automatic building segmentation in remote sensing images. However, building polygons in vector forms have a compact and regular structured expression effect, which corresponds more with the application requirements of cartography and geographic information systems (GIS). We propose a vector mapping method for buildings in remote sensing images with joint semantic-geometric learning to generate building polygon vectors in remote sensing images automatically. The method, aiming to provide cartography and GIS data sources, consists of three modules: multi-task segmentation, contour regularization, and polygon optimization. To reduce missing extractions and mis-extractions and obtain a complete building segmentation mask, the multitask segmentation module performs joint semantic-geometric learning on three related tasks: building instance detection, pixel-wise contour segmentation, and edge extraction. The regularization module normalizes the segmentation mask expression using geometric constraints and image information, whereas the polygon optimization module combines geometric constraints with deep learning methods to ensure vectorization quality. The experimental results show that the proposed method adapts well to building vector extraction tasks under different scenarios and can generate building vector polygons that match the ground truth labels. This method offers significant advantages in solving problems, such as building polygon irregularity and vertex offset.","2151-1535","","10.1109/JSTARS.2023.3319605","National Natural Science Foundation of China(grant numbers:42201491); Natural Science Foundation for Distinguished Young Scholars of Henan Province(grant numbers:212300410014); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10265008","Contour regularization;joint semantic-geometric learning;multitask segmentation;polygon optimization;remote sensing images","Buildings;Image segmentation;Remote sensing;Feature extraction;Multitasking;Optimization;Task analysis;Deep learning","","2","","45","CCBYNCND","27 Sep 2023","","","IEEE","IEEE Journals"
"SAPNet: Segmentation-Aware Progressive Network for Perceptual Contrastive Deraining","S. Zheng; C. Lu; Y. Wu; G. Gupta","Wenzhou-Kean University, Wenzhou, China; Wenzhou-Kean University, Wenzhou, China; Wenzhou-Kean University, Wenzhou, China; Wenzhou-Kean University, Wenzhou, China",2022 IEEE/CVF Winter Conference on Applications of Computer Vision Workshops (WACVW),"15 Feb 2022","2022","","","52","62","Deep learning algorithms have recently achieved promising deraining performance–s on both the natural and synthetic rainy datasets. As an essential low-level preprocessing stage, a deraining network should clear the rain streaks and preserve the fine semantic details. However, most existing methods only consider low-level image restoration. That limits their performances at high-level tasks requiring precise semantic information. To address this issue, in this paper, we present a segmentation aware progressive network (SAPNet) based upon contrastive learning for single image deraining. We start our method with a lightweight derain network formed with progressive dilated units (PDU). The PDU can significantly expand the receptive field and characterize multiscale rain streaks without the heavy computation on multiscale images. A fundamental aspect of this work is an unsupervised background segmentation (UBS) network initialized with ImageNet and Gaussian weights. The UBS can faithfully preserve an image s semantic information and improve the generalization ability to unseen photos. Furthermore, we introduce a perceptual contrastive loss (PCL) and a learned perceptual image similarity loss (LPISL) to regulate model learning. By ex-ploiting the rainy image and ground-truth as the negative and the positive sample in the VGG-16 latent space, we bridge the fine semantic details between the derained image and the ground-truth in a fully constrained manner. Comprehensive experiments on synthetic and real-world rainy images show our model surpasses top-performing methods and aids object detection and semantic segmentation with considerable efficacy. A Pytorch Implementation is available at https://github.com/ShenZheng2000/SAPNetfor-image-deraining.","2690-621X","978-1-6654-5824-5","10.1109/WACVW54805.2022.00011","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9707514","","Bridges;Deep learning;Image segmentation;Rain;Handheld computers;Semantics;Lighting","","34","","59","IEEE","15 Feb 2022","","","IEEE","IEEE Conferences"
"River Extraction Method of Remote Sensing Image Based on Edge Feature Fusion","B. Guo; J. Zhang; X. Li","Nanchang Key Laboratory of Welding Robot and Intelligent Technology, Nanchang Institute of Technology, Nanchang, China; Nanchang Key Laboratory of Welding Robot and Intelligent Technology, Nanchang Institute of Technology, Nanchang, China; Nanchang Key Laboratory of Welding Robot and Intelligent Technology, Nanchang Institute of Technology, Nanchang, China",IEEE Access,"24 Jul 2023","2023","11","","73340","73351","The extraction of rivers from remote sensing images is crucial for urban planning and water resource utilization. To address the low accuracy of traditional methods for extracting rivers from remote sensing images in complex scenes, a novel method based on edge feature fusion is proposed. As the input item of Learning Vector Quantization (LVQ), the preprocessed the remote sensing image is fed into four traditional edge detection algorithms for processing, and a finer edge image is obtained. Furthermore, VGG16 and ResNet50 models are utilized as feature extractors for transfer learning. The scale feature output layer and the semantic feature output layer are introduced to construct a Two-Branch Fusion (TBF) model. The scale feature output layer of the model makes full use of the rich convolution features of VGG16, and optimizes the training steps via collaborative loss and deep supervision. The semantic feature output layer extracts the residual maps matched by the ResNet50’s different scale convolution layers, and employs edge features to refine the segmentation boundary, thereby sharpening predictions at the edges. Moreover, the TBF model effectively integrates multi-scale edge information, thus improving the prediction accuracy of the model. The proposed model is tested with SVM, FCN, and UNet on the remote sensing river image dataset. The results show that the proposed model has greater PA (Pixel Accuracy), Pre (Precision), and mIoU (mean Intersection over Union) than other models. The test results indicate that the proposed model’s river segmentation effect is more refined. When applying this method to the Inria Aerial Image Labeling Dataset and Pascal VOC2012 Dataset, the model’s generalization ability is validated, and its evaluation index and segmentation effect are superior to those of other models.","2169-3536","","10.1109/ACCESS.2023.3296641","Jiangxi Provincial Natural Science Foundation(grant numbers:20224BAB202033); Jiangxi Provincial Key Research and Development Program(grant numbers:20192BBE50015); Science and Technology Project of Jiangxi Provincial Education Department(grant numbers:GJJ201912); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10185557","Edge feature fusion;river extraction;remote sensing image","Image edge detection;Feature extraction;Rivers;Remote sensing;Image segmentation;Training;Semantics;Urban planning","","4","","33","CCBYNCND","18 Jul 2023","","","IEEE","IEEE Journals"
"Methodology of Data Fusion Using Deep Learning for Semantic Segmentation of Land Types in the Amazon","J. P. De Oliveira; M. G. F. Costa; C. Filho","Operations and Management Center of the Amazon Protection System (CENSIPAM), Manaus, Brazil; Federal University of Amazonas, Manaus, Brazil; Federal University of Amazonas, Manaus, Brazil",IEEE Access,"21 Oct 2020","2020","8","","187864","187875","This study proposes a methodology using deep learning and a multi-resolution segmentation algorithm to perform the semantic segmentation of remote sensing images. Initially the image is segmented using a CNN, and then an image with homogeneous regions is generated using a multi-resolution segmentation algorithm. Finally, a data fusion process is performed with these two images, generating the final classified image. The field of study was the Brazilian Amazon region. The proposed methodology classifies images in the following classes: forest, pasture and agriculture. The input data used were LANDSAT-8/OLI images. The reference data were extracted from the results of the TerraClass project in 2014. Two datasets were evaluated: the first with six bands and the second with three bands. Three CNN architectures were evaluated together with three optimization methods: SGDM, ADAM, and RMSProp and the dropout and L2 regularization methods as methods for generalization improvement. The best model, CNN + optimization method + technique for generalization improvement, evaluated in the validation set, was submitted to a 5-fold cross validation methodology, and the results were compared with pre-trained networks using the learning transfer methodology; in this case the networks used for comparison were ResNet50, InceptionResnetv2, MobileNetv2 and Xception. The proposed methodology was evaluated through image segmentation of some regions of the Amazon. Finally, the proposed methodology was evaluated in regions used by other authors. The accuracy values obtained for the images evaluated were over 99%.","2169-3536","","10.1109/ACCESS.2020.3031533","Samsung Electronics of Amazonia Ltd., through the terms of Federal Law no. 8.387/1991, by the agreement no. 004, assigned by the Center for Research and Development in Electronics and Information from the Federal University of Amazonas—CETELI/UFAM; Coordenação de Aperfeiçoamento de Pessoal de Nível Superior—Brasil (CAPES)—Funding Code 001; Academic English Solutions; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9224624","Deep learning;convolutional neural networks;remote sensing;image segmentation","Remote sensing;Artificial satellites;Earth;Image segmentation;Deep learning;Forestry;Agriculture","","6","","30","CCBY","15 Oct 2020","","","IEEE","IEEE Journals"
"ROXSI: Robust Cross-Sequence Semantic Interaction for Brain Tumor Segmentation on Multi-Sequence MR Images","Z. Kuang; Z. Yan; A. Abayazeed; F. Wagner; L. Yu; M. Reyes","School of Electronic Information and Communications, Huazhong University of Science and Technology, Wuhan, Hubei, China; School of Electronic Information and Communications, Huazhong University of Science and Technology, Wuhan, Hubei, China; Neosoma Inc, USA; Department of Neuro-radiology, University Hospital Bern, Switzerland; School of Electronic Information and Communications, Huazhong University of Science and Technology, Wuhan, Hubei, China; ARTORG Center for Biomedical Engineering Research, University of Bern, Bern, Switzerland",IEEE Journal of Biomedical and Health Informatics,"","2024","PP","99","1","14","Deep learning-based brain tumor segmentation on multi-sequence magnetic resonance imaging (MRI) has gained widespread attention due to its great potential in supporting brain disease diagnosis. Although, compared to single-sequence images, more information is available from multi-sequence MR images, noise and artifacts on any given MR sequence can result in significant performance degradations. As in clinical routine, it is not always possible to maintain high imaging quality across all MR sequences (e.g., foreign bodies, ventricular drainage, shunts, involuntary patient motion, etc.), ensuring robustness of brain tumor segmentation from multi-sequence MR images is of great importance in clinical practice, but rarely explored. Accordingly, in this paper, we propose a robust brain tumor segmentation framework to mitigate the performance degradation caused by noise and artifacts on multi-sequence MR images. Specifically, based on semantic affinity, we propose a unique cross-sequence semantic interaction module (CSSI) to exploit inter-sequence correlations and extract noise-resilient features. In addition, we incorporate a batch-level covariance mechanism to suppress the redundant background information and improve the semantic enhancement effect of the CSSI module. In order to further improve segmentation performance, we also incorporate a sequence-level variance regularization mechanism to exploit sequence-specific features. To validate the robustness of ROXSI, brain tumor segmentation performance was evaluated under the existence of four common artifacts, at five different perturbation levels. We further performed a blinded qualitative clinical evaluation with two experienced neuro-radiologists, evaluating results from ROXSI and other popular CNN and Transformer-based segmentation models. Experimental results on two benchmark datasets demonstrate the superior robustness of ROXSI over other state-of-the-art segmentation methods. The source code is publicly available at https://github.com/HustAlexander/ROXSI.","2168-2208","","10.1109/JBHI.2024.3513479","National Natural Science Foundation of China(grant numbers:62271220,62202179); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10786192","medical image segmentation;brain tumor segmentation;multi-sequence feature fusion;cross-sequence semantic affinity","Semantics;Image segmentation;Feature extraction;Robustness;Magnetic resonance imaging;Noise;Degradation;Transformers;Training;Fuses","","","","","IEEE","9 Dec 2024","","","IEEE","IEEE Early Access Articles"
"Side-Scan Sonar Image Segmentation Based on Multi-Channel Fusion Convolution Neural Networks","Z. Wang; J. Guo; W. Huang; S. Zhang","School of Information and Navigation, Air Force Engineering University, Xi’an, China; School of Information Engineering, Xijing University, Xi’an, China; School of Information Engineering, Xijing University, Xi’an, China; School of Information Engineering, Xijing University, Xi’an, China",IEEE Sensors Journal,"14 Mar 2022","2022","22","6","5911","5928","Side-scan sonar is an important application in the field of ocean exploration. Accurate segmentation of target regions in side-scan sonar images is a challenging issue due to the low-resolution and strong noise interference. To accurately and faster segment the different categories target in sonar image, a novel convolutional neural networks (CNNs) model is proposed in this study. Firstly, the deep separable residual module is used for target regions multi-scale feature extraction and suppression noise feature information interference, and the multi-channel feature fusion method is used to enhance feature information transfer of convolution layers. Secondly, the adaptive supervised function is used for pixel-wise classification of different categories targets. Finally, to improve model generalization ability and robustness, the adaptive transfer learning method is introduced in the model training process. We have performed extensive experiments on side-scan sonar image with different targets and scales. The experimental results show that the detection accuracy of the proposed method reaches 95.73%, which is outperforms other state-of-the-art methods on the side-scan sonar image segmentation tasks. Moreover, the method has fewer computational parameters, facilitating future deployment it to underwater mobile detection devices.","1558-1748","","10.1109/JSEN.2022.3149841","National Neural Science Foundation of China(grant numbers:61671465); Neural Science Foundation of Shanxi Province(grant numbers:2021JQ-879); China National Natural Science Foundation(grant numbers:61473237); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9706494","Sonar image segmentation;convolution neural network;multi-channel feature fusion;adaptive supervised function;adaptive transfer learning","Sonar;Image segmentation;Feature extraction;Convolution;Adaptation models;Training;Sensors","","24","","40","IEEE","7 Feb 2022","","","IEEE","IEEE Journals"
"Image Reflection Removal via Contextual Feature Fusion Pyramid and Task-Driven Regularization","Y. Li; Q. Yan; K. Zhang; H. Xu","School of Electrical and Information Engineering, Tianjin University, Tianjin, China; School of Electrical and Information Engineering, Tianjin University, Tianjin, China; School of Electrical and Information Engineering, Tianjin University, Tianjin, China; School of Electrical and Information Engineering, Tianjin University, Tianjin, China",IEEE Transactions on Circuits and Systems for Video Technology,"3 Feb 2022","2022","32","2","553","565","In this paper, we propose a deep neural network for single image reflection removal. More specifically, we design a convolutional-grid module and take it as the building block of a feature fusion pyramid. The module leverages the combination effect of the grid topology to create a rich ensemble of receptive fields. Embedding the modules into a pyramidal architecture further expands the coverage of receptive fields. Another benefit of the pyramid is to fuse the multi-scale features learned by the modules locating at the ascending and descending pathways. The rich diversity of features helps the neural network analyze the contexts around overlapping objects at various spatial ranges and harvest the cues for layer separation. The proposed work also exploits useful semantic cues from the hyper-column descriptors generated by a pre-trained VGG-19 model to reduce the ambiguity of layer separation. In light of the low correlation between background and reflection layers, we design a channel-correlation based conditional discriminator to penalize residual reflection. The discriminator uses channel-wise attention to screen the features that can distinguish real background images from estimated ones. This paper also presents a task-driven regularization strategy. The high sensitivity of semantic segmentation to reflection is exploited for assessing the completeness of reflection removal. Training with this regularization strategy can boost the performance of both reflection removal and high-level task. The comparison against state-of-the-art algorithms on four public benchmark datasets demonstrates that this work exhibits superior performance in handling the complex reflections in wild scenarios. The proposed network architecture is also applicable to haze removal, which is another ill-posed layer separation problem, and has shown encouraging performance.","1558-2205","","10.1109/TCSVT.2021.3067502","National Natural Science Foundation of China(grant numbers:61972281,61572352); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9381898","Reflection removal;multi-scale feature extraction;feature fusion;semantic segmentation;task-driven regularization","Task analysis;Feature extraction;Training;Semantics;Neural networks;Image restoration;Inference algorithms","","12","","53","IEEE","19 Mar 2021","","","IEEE","IEEE Journals"
"AOSVSSNet: Attention-Guided Optical Satellite Video Smoke Segmentation Network","T. Wang; J. Hong; Y. Han; G. Zhang; S. Chen; T. Dong; Y. Yang; H. Ruan","School of Remote Sensing and Information Engineering, Wuhan University, Wuhan, China; School of Remote Sensing and Information Engineering, Wuhan University, Wuhan, China; Department of Computer Science and Technology, Tsinghua University, Beijing, China; State Key Laboratory of Information Engineering in Surveying, Mapping and Remote Sensing, Wuhan University, Wuhan, China; School of Remote Sensing and Information Engineering, Wuhan University, Wuhan, China; State Key Laboratory of Information Engineering in Surveying, Mapping and Remote Sensing, Wuhan University, Wuhan, China; Navy Research Institute, People's Liberation Army, Beijing, China; Beijing Institute of Tracking and Telecommunications Technology, Beijing, China",IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing,"12 Oct 2022","2022","15","","8552","8566","Smoke is more observable than open fires. Optical satellite video has the advantages of a wide monitoring range, fast response speed, and good economy in large-scale surface smoke monitoring tasks. It can be used in wide-area forest wildfire monitoring, battlefield dynamic monitoring, disaster relief decision-making. The smoke segmentation method based on traditional handcrafted features is easily limited by the scene and data. This article introduces the deep learning method to the optical satellite video smoke target segmentation. However, due to the lack of real smoke images and the blurred edges of smoke, there are currently few labeled datasets for smoke segmentation in high-resolution optical satellite imagery scenes, which cannot provide sufficient training data for deep learning models. The smoke image from the satellite perspective also has the characteristics of multiscale features and ground object background interference. To solve the abovementioned problems, we construct a set of high-resolution optical satellite imagery smoke synthesis datasets based on the optical imaging process of smoke targets, which saves the cost of manual labeling. In addition, we design an attention-guided optical satellite video smoke segmentation network model, which can effectively suppress the ground object background's false alarm and extract the smoke's multiscale features. Synthetic data faces the transferability problem in real-world applications, so the physical constraints of the smoke imaging process are introduced into the loss function to improve the generalization of the model in real smoke data. The comprehensive evaluation results show that the method outperforms representative semantic segmentation networks.","2151-1535","","10.1109/JSTARS.2022.3209541","Key Research and Development Program of the Ministry of Science and Technology(grant numbers:2018YFB0504905); High Resolution Earth Observation Systems National Science and Technology Major Projects(grant numbers:GFZX0404120305); Civil Aerospace Advance Research Project(grant numbers:D040107); Postdoctoral Innovation Talent Support Program(grant numbers:BX2021222); LIESMARS Special Research Funding; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9903300","Convolutional neural network;moving object segmentation;satellite video;smoke segmentation","Image segmentation;Satellites;Optical imaging;Feature extraction;Optical sensors;Convolutional neural networks;Deep learning","","6","","77","CCBY","26 Sep 2022","","","IEEE","IEEE Journals"
"Probability distribution guided optic disc and cup segmentation from fundus images","P. Cheng; J. Lyu; Y. Huang; X. Tang","Department of Electrical and Electronic Engineering, Southern University of Science and Technology, Shenzhen, China; Department of Electrical and Electronic Engineering, Southern University of Science and Technology, Shenzhen, China; Department of Electrical and Electronic Engineering, Southern University of Science and Technology, Shenzhen, China; Department of Electrical and Electronic Engineering, Southern University of Science and Technology, Shenzhen, China",2020 42nd Annual International Conference of the IEEE Engineering in Medicine & Biology Society (EMBC),"27 Aug 2020","2020","","","1976","1979","In this paper, we proposed and validated a probability distribution guided network for segmenting optic disc (OD) and optic cup (OC) from fundus images. Uncertainty is inevitable in deep learning, as induced by different sensors, insufficient samples, and inaccurate labeling. Since the input data and the corresponding ground truth label may be inaccurate, they may actually follow some potential distribution. In this study, a variational autoencoder (VAE) based network was proposed to estimate the joint distribution of the input image and the corresponding segmentation (both the ground truth segmentation and the predicted segmentation), making the segmentation network learn not only pixel-wise information but also semantic probability distribution. Moreover, we designed a building block, namely the Dilated Inception Block (DIB), for a better generalization of the model and a more effective extraction of multi-scale features. The proposed method was compared to several existing state-of-the-art methods. Superior segmentation performance has been observed over two datasets (ORIGA and REFUGE), with the mean Dice overlap coefficients being 96.57% and 95.81% for OD and 88.46% and 88.91% for OC.","2694-0604","978-1-7281-1990-8","10.1109/EMBC44109.2020.9176394","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9176394","","Image segmentation;Uncertainty;Semantics;Image reconstruction;Task analysis;Training;Biomedical optical imaging","Animals;Fundus Oculi;Gastric Fundus;Glaucoma;Optic Disk;Probability","3","","14","IEEE","27 Aug 2020","","","IEEE","IEEE Conferences"
"YOLO v4 Based Human Detection System Using Aerial Thermal Imaging for UAV Based Surveillance Applications","P. Kannadaguli","Dhaarini Academy of Technical Education, Bengaluru, India",2020 International Conference on Decision Aid Sciences and Application (DASA),"15 Jan 2021","2020","","","1213","1219","This work is related to building a Human Detection system based on You Only Look Once (YOLO) v4. It is one of the most recent Deep Learning approaches primitively built using single shot detection proposal. Unlike the double stage region-based object detection schemes this technique do not follow semantic segmentation, it does not undergo loss of the object information such as disappearance of the gradients and it does not require pre-defined anchors. This technique comprises strong feature extractors and reinforce multi scale object detection and it is very quick in the multi-threaded GPU environments. Since our fundamental research is concentrated on object classification related to Unmanned Aerial Vehicle (UAV) applications, as a first step we choose to detect the humans from thermal dataset. Therefore, we used thermal images and videos possessed from thermal cameras of UAV 1m to 50m above ground level as our dataset in building the model and testing. The YOLO v4 uses ground truth bounding boxes to extract the features like Weighted Residual Connections (WRC), Cross Stage Partial Connections (CSP), Cross mini Batch Normalization (CmBN), Self-Adversarial Training (SAT), Mish Activation (MA), Mosaic Data Augmentation (MDA) and Drop Block Regularization (DBR). Finally, the performance analysis of these model in terms of mean Average Precision (mAP) indicates that the modelling using YOLO v4 performs in a promising way and it can be used in automatic human detection systems.","","978-1-7281-9677-0","10.1109/DASA51403.2020.9317198","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9317198","YOLO v4;Deep Learning;Thermal Imaging;UAV;Object Detection;Smart City;Surveillance","Feature extraction;Unmanned aerial vehicles;Rats;Object detection;Training;Testing;Surveillance","","25","","56","IEEE","15 Jan 2021","","","IEEE","IEEE Conferences"
"SKD-Net: Spectral-based Knowledge Distillation in Low-Light Thermal Imagery for robotic perception","A. Sikdar; J. Teotia; S. Sundaram","The Robert Bosch Centre for Cyber-Physical Systems, Indian Institute of Science, Bangalore; Robert Bosch Centre for CyberPhysical Systems, Indian Institute of Science, Bangalore; The Department of Aerospace Engineering, Indian Institute of Science, Bangalore",2024 IEEE International Conference on Robotics and Automation (ICRA),"8 Aug 2024","2024","","","9041","9047","Enhancing the generalization capacity for semantic segmentation of aerial perception systems for safety-critical applications is vital, especially for environments with low-light and adverse conditions. Multi-spectral fusion techniques aim to maintain the merits of electro-optical (EO) and infrared (IR) images, e.g., retaining low-level features and capturing detailed textures from both modalities. However, these techniques encounter limitations when faced with scenarios involving missing modalities, especially during inference when only IR images are available. In this paper, we propose a novel spectral-based knowledge distillation architecture known as SKD-Net to improve the performance of deep learning models for missing modality scenarios for semantic segmentation tasks. In this architecture, we make use of Gated Spectral Unit to combine information from both modalities. SKD-Net aims to extract valuable semantic information from EO images while preserving spectral knowledge from the IR images within the feature space. The model retains the style information in the shallow layers while simultaneously fusing the high-level semantic context obtained from EO and IR images to improve the feature generation capacity when dealing with only IR images during inference. SKD-Net outperforms state-of-the-art multi-modal fusion and distillation models by 2.8% on average in scenarios with missing modalities when using only IR data during inference in two public benchmarking datasets. This performance increase is achieved without additional computational costs compared to the baseline segmentation models.","","979-8-3503-8457-4","10.1109/ICRA57147.2024.10611323","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10611323","","Training;Semantic segmentation;Computational modeling;Semantics;Computer architecture;Logic gates;Benchmark testing","","1","","38","IEEE","8 Aug 2024","","","IEEE","IEEE Conferences"
"GLSNet++: Global and Local-Stream Feature Fusion for LiDAR Point Cloud Semantic Segmentation Using GNN Demixing Block","R. Bao; K. Palaniappan; Y. Zhao; G. Seetharaman","Department of Electrical Engineering and Computer Science, University of Missouri, Columbia, MO, USA; Department of Electrical Engineering and Computer Science, University of Missouri, Columbia, MO, USA; Department of Electrical Engineering and Computer Science, University of Missouri, Columbia, MO, USA; Naval Research Laboratory, Washington, DC, USA",IEEE Sensors Journal,"1 Apr 2024","2024","24","7","11610","11624","Semantic point cloud segmentation is a critical task in 3-D computer vision, offering valuable contextual information for navigation, cartography, landmarks, object recognition, and building modeling. We developed global and local stream deep network (GLSNet++), an innovative deep learning architecture for robust context-dependent 3-D point cloud segmentation. GLSNet++ uniquely combines dual streams of global and local feature manifolds to capture multiscale contextual and structural information, addressing challenges due to highly varying object sizes in urban scenes. To effectively and efficiently refine mixed class labels from cross-scale global and local streams, GLSNet++ incorporates a novel graph neural network (GNN)-based demixing block (GDB) for accurately resolving class membership near voxel boundaries with spatial context-dependent feature fusion. We validate GLSNet++ on the IEEE DFT4 LiDAR dataset, achieving competitive city-scale semantic segmentation that can be extended to more classes, higher-resolution point clouds, and larger geographic regions. GLSNet++ exhibits strong generalization when tested on an independent LiDAR dataset from Columbia, Missouri evaluated using OpenStreetMap (OSM).","1558-1748","","10.1109/JSEN.2023.3345747","U.S. Army Research Laboratory(grant numbers:W911NF-1820285); Engineering Research and Development Center–Information Technology Laboratory (ERDC-ITL)(grant numbers:W912HZ23C0041); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10430108","3-D semantic segmentation;building information modeling (BIM);ensemble stacking;feature fusion;geographical information system (GIS);hyperspectral unmixing;point clouds","Point cloud compression;Three-dimensional displays;Laser radar;Streams;Semantics;Semantic segmentation;Graph neural networks","","3","","58","CCBYNCND","8 Feb 2024","","","IEEE","IEEE Journals"
"A Siamese Network Combining Multiscale Joint Supervision and Improved Consistency Regularization for Weakly Supervised Building Change Detection","Y. Dai; K. Zhao; L. Shen; S. Liu; X. Yan; Z. Li","State-Province Joint Engineering Laboratory of Spatial Information Technology for High-Speed Railway Safety, Faculty of Geosciences and Environmental Engineering, Southwest Jiaotong University, Chengdu, China; State-Province Joint Engineering Laboratory of Spatial Information Technology for High-Speed Railway Safety, Faculty of Geosciences and Environmental Engineering, Southwest Jiaotong University, Chengdu, China; State-Province Joint Engineering Laboratory of Spatial Information Technology for High-Speed Railway Safety, Faculty of Geosciences and Environmental Engineering, Southwest Jiaotong University, Chengdu, China; State-Province Joint Engineering Laboratory of Spatial Information Technology for High-Speed Railway Safety, Faculty of Geosciences and Environmental Engineering, Southwest Jiaotong University, Chengdu, China; State-Province Joint Engineering Laboratory of Spatial Information Technology for High-Speed Railway Safety, Faculty of Geosciences and Environmental Engineering, Southwest Jiaotong University, Chengdu, China; State-Province Joint Engineering Laboratory of Spatial Information Technology for High-Speed Railway Safety, Faculty of Geosciences and Environmental Engineering, Southwest Jiaotong University, Chengdu, China",IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing,"8 Jun 2023","2023","16","","4963","4982","Building change detection (BCD) from remote sensing images is essential in various practical applications. Recently, inspired by the achievement of deep learning in semantic segmentation (SS), methods that treat the BCD problem as a binary SS task using deep siamese networks have attracted increasing attention. However, similar to their counterparts, these approaches still face the challenge of collecting massive pixel-level annotations. To address this issue, this article presents a novel weakly supervised method for BCD from remote sensing images using image-level labels. The proposed method elaborately designs a siamese network to integrate a multiscale joint supervision (MJS) module and an improved consistency regularization (ICR) module into a unified framework to improve the so-called class activation maps (CAMs), which is vital for producing high-quality pseudomasks using image-level annotations to support pixel-level BCD. To be specific, the MSJ is used for generating refined multiscale CAMs to well capture changes at different scales corresponding to various buildings of varying sizes. The ICR contributes to improving the consistency of CAMs to highlight the boundaries of changed buildings. Extensive experiments on two public BCD datasets have demonstrated that the proposed method outperforms the current state-of-the-art approaches. Furthermore, the visual detection maps also indicate that the proposed method can achieve scale-adaptive change detection results and preserve object boundaries more effectively.","2151-1535","","10.1109/JSTARS.2023.3279863","National Natural Science Foundation of China(grant numbers:42071386,41971330); Basic Research Fund Project of Sichuan Provincial Department of Science and Technology(grant numbers:2023JDKY0017-3); Interdisciplinary Cultivation Fund; Southwest Jiaotong University; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10134553","Building change detection (BCD);class activation maps (CAM);image-level weakly supervised;improved consistency regularization (ICR);multiscale joint supervision (MJS)","Buildings;Cams;Feature extraction;Annotations;Semantic segmentation;Remote sensing;Image recognition;Deep learning;Change detection algorithms","","6","","71","CCBY","25 May 2023","","","IEEE","IEEE Journals"
"AFFD-Net: A Dual-Decoder Network Based on Attention-Enhancing and Feature Fusion for Retinal Vessel Segmentation","X. Zijian; N. Chunyu; L. Mingye; M. Kaizheng; S. Lemin; W. Wei; Y. Guanshi","School of Life Science and Technology, Changchun University of Science and Technology, Changchun, China; School of Life Science and Technology, Changchun University of Science and Technology, Changchun, China; Department of Information Systems and Business Analytics, RMIT University, Melbourne, VIC, Australia; School of Life Science and Technology, Changchun University of Science and Technology, Changchun, China; School of Life Science and Technology, Changchun University of Science and Technology, Changchun, China; School of Computing and Information Systems, The University of Melbourne, Parkville, VIC, Australia; School of Electrical and Information Engineering, Jilin Agricultural Science and Technology University, Jilin, China",IEEE Access,"15 May 2023","2023","11","","45871","45887","The morphological characteristics of retinal vessels in the fundus serve as the primary basis for diagnosing and assessing the risk of ophthalmic diseases. An effective segmentation scheme for retinal vessels can aid in the early diagnosis and treatment of these diseases, as well as help prevent their progression. However, accurate vessel segmentation is challenging due to the low contrast of fundus images and the complexity of the vessels’ morphological structure. To address the low sensitivity and poor generalization ability of the existing methods in vascular extraction, a Dual-decoder Network based on Attention-enhancing and multi-scale Feature Fusion (AFFD-Net) is proposed. AFFD-Net inherits the codec concept of U-Net. To improve the performance of our U-Net model, we made two modifications. Firstly, we reduced the number of convolution kernel filters in each layer, thereby significantly reducing the number of training parameters. This helps to avoid overfitting and improves the model’s ability to generalize. Secondly, we added a Multi-scale Feature Extraction (MFE) module and an M/A intermediate decoder to enhance the model’s sensitivity. MFE is designed as the first encoding unit of AFFD-Net to obtain rich vascular features in the complex anatomical background and adapt to the large-scale variations of vessels. The M/A intermediate decoder is composed of the Multi-scale Feature Fusion (MFF) module and the Attention-enhancing Hybrid Feature Fusion (AHFF) module. The MFF module integrates deep semantic information and shallow spatial information to ensure that the features at each scale in the middle layer are fully utilized. The AHFF module adaptively fuses the hybrid features at different scales to generate two feature descriptors with different focuses which can improve the expressiveness of the model. AFFD-Net is evaluated on three public databases including DRIVE, STARE, and CHASE_DB1, and the sensitivity values obtained are 84.19%, 84.58%, and 82.62%, respectively. It has higher sensitivity and better generalization ability than other state-of-the-art methods. Compared with classical networks including U-Net, U-Net++, and U-Net3+, AFFD-Net has fewer parameters and higher segmentation accuracy. Our proposed segmentation model exhibits superior performance across a range of metrics, indicating its promising potential for practical applications.","2169-3536","","10.1109/ACCESS.2023.3273597","Science and Technology Development Project of Jilin Province, China(grant numbers:20220101123JC,20200404219YY); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10121014","Deep learning;feature extraction;image segmentation;neural network;retinal vessel","Feature extraction;Retinal vessels;Filter banks;Semantics;Convolutional codes;Deep learning;Mathematical models;Neural  networks","","3","","70","CCBYNCND","8 May 2023","","","IEEE","IEEE Journals"
"Multi-Scale Convolutional Vision Transformer for Semantic Segmentation","R. R. Otyrba; A. A. Sirota","Department of Information Security and Processing Technologies, Faculty of Computer Sciences, Voronezh State University, Voronezh, Russia; Department of Information Security and Processing Technologies, Faculty of Computer Sciences, Voronezh State University, Voronezh, Russia","2023 5th International Conference on Control Systems, Mathematical Modeling, Automation and Energy Efficiency (SUMMA)","15 Dec 2023","2023","","","235","240","Semantic segmentation is a fundamental task in computer vision, and the development of encoder networks remains one of the primary directions in this field. Encoder networks play a crucial role since the final result of segmentation directly depends on the quality of the extracted image features. This paper presents a novel hierarchically structured encoder architecture that integrates Convolutional Neural Networks with multi-scale feature capturing and Transformers. The proposed model introduces three significant architectural solutions. First, it introduces a new Self-Attention Block that includes an effective Inception-like multi-branch convolution layer with a Multi-Head Self-Attention layer. Second, the computational complexity of Multi-Head Self-Attention is efficiently reduced by resizing the spatial dimensions of Key and Value tokens using Average Pooling. Third, it abandons positional encoding and adopts a Depth-Wise convolutional layer in the Feed-Forward Network instead. We conducted experiments on datasets of varying complexity for image classification, such as CIFAR-100 and Caltech-256, to assess the generalization ability and feature extraction efficiency of the network. It demonstrates competitive performance, achieving an accuracy of 0.775 on CIFAR-100 and 0.665 on Caltech-256, compared to state-of-the-art Transformers, while having lower model complexity. These results demonstrate the great potential of our model and underline the need for further research aimed at its refinement, the search for the best architecture characteristics, pre-training on the ImageNet-1K dataset, and its eventual integration into semantic segmentation tasks.","","979-8-3503-0912-6","10.1109/SUMMA60232.2023.10349661","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10349661","computer vision;semantic segmentation;encoder network;visual transformer;convolution neural network","Semantic segmentation;Computational modeling;Neural networks;Computer architecture;Transformers;Feature extraction;Mathematical models","","","","32","IEEE","15 Dec 2023","","","IEEE","IEEE Conferences"
"Self-Supervised Learning for Point Clouds through Multi-crop Mutual Prediction","C. Zeng; W. Wang; J. Xiao; A. Nguyen; Y. Yue","Institute of Deep Perception Technology, JITRI, Wuxi, China; School of Advanced Technology, Xi’an Jiaotong-Liverpool University, Suzhou, China; School of Advanced Technology, Xi’an Jiaotong-Liverpool University, Suzhou, China; Department of Computer Science, University of Liverpool, Liverpool, United Kingdom; Institute of Deep Perception Technology, JITRI, Wuxi, China",2023 IEEE 4th International Conference on Pattern Recognition and Machine Learning (PRML),"14 Dec 2023","2023","","","160","167","As the demand for high-level autonomous driving continues to grow, efficient processing of large amount of point cloud data from LiDAR sensors has become a severe issue. Despite its great success, it is extremely difficult, if not impossible, to apply supervised learning in many real-world applications with natural scenes lacking annotations. In addition, removing label dependency and improving algorithm generalization ability are the current key research challenges in 3D deep learning. We propose a crop-based self-supervised learning framework for point clouds called Multi-Crop Mutual Prediction (MCMP). By cropping the original point clouds into different number of multiscale patches and making patches from the same object as close as possible through dynamic updates of online prototypes, the encoder is well able to maintain consistency between both local and global features. MCMP framework is model-agnostic so it is convenient to integrate it into encoders of any architectures. We pre-train three commonly used point cloud feature extraction networks on a single object-level dataset MondelNet40 and evaluate MCMP on three downstream tasks, namely, object classification, partial segmentation and semantic segmentation. Experimental results show that MCMP outperforms the previous crop-based self-supervised approaches in the object classification and semantic segmentation tasks and produces comparable results with the supervised benchmark methods.","","979-8-3503-2430-3","10.1109/PRML59573.2023.10348385","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10348385","Self-Supervised Learning;Point Cloud;Object Classification;Part Segmentation;Semantic Segmentation","Point cloud compression;Training;Three-dimensional displays;Semantic segmentation;Supervised learning;Semantics;Self-supervised learning","","","","41","IEEE","14 Dec 2023","","","IEEE","IEEE Conferences"
"A Robust Shape-Aware Rib Fracture Detection and Segmentation Framework With Contrastive Learning","Z. Cao; L. Xu; D. Z. Chen; H. Gao; J. Wu","College of Computer Science and Technology, Zhejiang University, Hangzhou, China; College of Computer Science and Technology, Zhejiang University, Hangzhou, China; Department of Computer Science and Engineering, University of Notre Dame, Notre Dame, IN, USA; School of Computer Engineering and Science, Shanghai University, Shanghai, China; Second Affiliated Hospital School of Medicine, and School of Public Health, Zhejiang University, Hangzhou, China",IEEE Transactions on Multimedia,"8 May 2023","2023","25","","1584","1591","The rib fracture is a common type of thoracic skeletal trauma, and its inspections using computed tomography (CT) scans are critical for clinical evaluation and treatment planning. However, it is often challenging for radiologists to quickly and accurately detect rib fractures due to tiny objects and blurriness in large 3D CT images. Previous diagnoses for automatic rib fracture mostly relied on deep learning (DL)-based object detection, which highly depends on label quality and quantity. Moreover, general object detection methods did not take into consideration the typically elongated and oblique shapes of ribs in 3D volumes. To address these issues, we propose a shape-aware method based on DL called SA-FracNet for rib fracture detection and segmentation. First, we design a pixel-level pretext task founded on contrastive learning on massive unlabeled CT images. Second, we train the fine-tuned rib fracture detection model based on the pre-trained weights. Third, we develop a fracture shape-aware multi-task segmentation network to delineate the fracture based on the detection result. Experiments demonstrate that our proposed SA-FracNet achieves state-of-the-art rib fracture detection and segmentation performance on the public RibFrac dataset, with a detection sensitivity of 0.926 and segmentation Dice of 0.754. Test on a private dataset also validates the robustness and generalization of our SA-FracNet.","1941-0077","","10.1109/TMM.2023.3263074","National Key R&D Program of China(grant numbers:2019YFC0118802); National Natural Science Foundation of China(grant numbers:62176231); Zhejiang public welfare technology research Project(grant numbers:LGF20F020013); NSF(grant numbers:CCF-1617735); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10091887","Computer-aided diagnosis;rib fracture detection and segmentation;self-supervised contrastive learning;shape-aware model","Ribs;Computed tomography;Three-dimensional displays;Image segmentation;Task analysis;Training;Deep learning","","17","","51","IEEE","4 Apr 2023","","","IEEE","IEEE Journals"
"An Entity Relation Extraction Framework Based on Large Language Model and Multi-Tasks Iterative Prompt Engineering","H. Geng; C. Shi; X. Jiang; Z. Kong; S. Liu","Key Laboratory of Computing Power Network and Information Security, Ministry of Education, Shandong Computer Science Center (National Supercomputer Center in Jinan), Qilu University of Technology (Shandong Academy of Sciences), Jinan, China; Key Laboratory of Computing Power Network and Information Security, Ministry of Education, Shandong Computer Science Center (National Supercomputer Center in Jinan), Qilu University of Technology (Shandong Academy of Sciences), Jinan, China; Key Laboratory of Computing Power Network and Information Security, Ministry of Education, Shandong Computer Science Center (National Supercomputer Center in Jinan), Qilu University of Technology (Shandong Academy of Sciences), Jinan, China; Key Laboratory of Computing Power Network and Information Security, Ministry of Education, Shandong Computer Science Center (National Supercomputer Center in Jinan), Qilu University of Technology (Shandong Academy of Sciences), Jinan, China; Key Laboratory of Computing Power Network and Information Security, Ministry of Education, Shandong Computer Science Center (National Supercomputer Center in Jinan), Qilu University of Technology (Shandong Academy of Sciences), Jinan, China","2024 IEEE International Conference on Systems, Man, and Cybernetics (SMC)","20 Jan 2025","2024","","","4763","4769","Document-level entity relation extraction is an important task in the field of natural language processing, which plays an important role in semantic understanding and knowledge graph construction. However, existing deep neural networks and graph neural networks models are limited by their performance and parameters number, which can not capture global semantics and have poor generalization ability. Furthermore, existing methods employing large language model for entity relation extraction do not establish good relationships among multi-tasks of entity relation extraction task, resulting in more information can not be effectively shared and transmitted between tasks. In addition, previous approaches can not effectively eliminate false entities and relationships. To solve these problems, we propose an entity relation extraction framework based on large language model and multi-tasks iterative prompt engineering. In our model, we design an iterative prompt engineering, which can better establish the relationship among multi-tasks, and ensure every task to obtain the optimal results. Moreover, we design semantic merging, group disambiguation and self-verification modules to eliminate the false entity relations and noise nodes. Additionally, we design summary prompts to provide sufficient global semantics for better text segmentation. Finally, we evaluated our model on wikiann, wikineural, ACE2005, CoNLL2003, CoNLL2004, and SciERC datasets and compared it with other baseline models.","","978-1-6654-1020-5","10.1109/SMC54092.2024.10831494","Research and Development Program (Major Innovation Projects) in Shandong Province(grant numbers:2021CXGCO10102,2022JBZ01-01); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10831494","","Large language models;Semantics;Noise;Merging;Knowledge graphs;Multitasking;Natural language processing;Iterative methods;Prompt engineering;Data mining","","","","26","IEEE","20 Jan 2025","","","IEEE","IEEE Conferences"
"Discriminative Multi-modal Feature Fusion for RGBD Indoor Scene Recognition","H. Zhu; J. -B. Weibel; S. Lu","Agency for Science Technology and Research, Singapore, SG; Georgia Institute of Technology, Atlanta, GA, US; Agency for Science Technology and Research, Singapore, SG",2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR),"12 Dec 2016","2016","","","2969","2976","RGBD scene recognition has attracted increasingly attention due to the rapid development of depth sensors and their wide application scenarios. While many research has been conducted, most work used hand-crafted features which are difficult to capture high-level semantic structures. Recently, the feature extracted from deep convolutional neural network has produced state-of-the-art results for various computer vision tasks, which inspire researchers to explore incorporating CNN learned features for RGBD scene understanding. On the other hand, most existing work combines rgb and depth features without adequately exploiting the consistency and complementary information between them. Inspired by some recent work on RGBD object recognition using multi-modal feature fusion, we introduce a novel discriminative multi-modal fusion framework for rgbd scene recognition for the first time which simultaneously considers the inter-and intra-modality correlation for all samples and meanwhile regularizing the learned features to be discriminative and compact. The results from the multimodal layer can be back-propagated to the lower CNN layers, hence the parameters of the CNN layers and multimodal layers are updated iteratively until convergence. Experiments on the recently proposed large scale SUN RGB-D datasets show that our method achieved the state-of-the-art without any image segmentation.","1063-6919","978-1-4673-8851-1","10.1109/CVPR.2016.324","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7780693","","Correlation;Image color analysis;Image recognition;Computer vision;Sensors;Image segmentation;Object recognition","","61","2","22","IEEE","12 Dec 2016","","","IEEE","IEEE Conferences"
"Dual Encoder–Decoder Network for Land Cover Segmentation of Remote Sensing Image","Z. Wang; M. Xia; L. Weng; K. Hu; H. Lin","Collaborative Innovation Center on Atmospheric Environment and Equipment Technology, Nanjing University of Information Science and Technology, Nanjing, China; Collaborative Innovation Center on Atmospheric Environment and Equipment Technology, Nanjing University of Information Science and Technology, Nanjing, China; Collaborative Innovation Center on Atmospheric Environment and Equipment Technology, Nanjing University of Information Science and Technology, Nanjing, China; Collaborative Innovation Center on Atmospheric Environment and Equipment Technology, Nanjing University of Information Science and Technology, Nanjing, China; College of Information Science and Technology, Nanjing Forestry University, Nanjing, China",IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing,"11 Jan 2024","2024","17","","2372","2385","Although the vision transformer-based methods (ViTs) exhibit an excellent performance than convolutional neural networks (CNNs) for image recognition tasks, their pixel-level semantic segmentation ability is limited due to the lack of explicit utilization of local biases. Recently, a variety of hybrid structures of ViT and CNN have been proposed, but these methods have poor multiscale fusion ability and cannot accurately segment high-resolution and high-content complex land cover remote sensing images. Therefore, a dual encoder–decoder network named DEDNet is proposed in this work. In the encoding stage, the local and global information of the image is extracted by parallel CNN encoder and transformer encoder. In the decoding stage, the cross-stage fusion module is constructed to achieve neighborhood attention guidance to enhance the positioning of small targets, effectively avoiding intraclass inconsistency. At the same time, the multihead feature extraction module is proposed to strengthen the recognition ability of the target boundary and effectively avoid interclass ambiguity. Before outputting, the fusion spatial pyramid pooling classifier is proposed to merge the outputs of the two decoding strategies. The experiments demonstrate that the proposed model has superior generalization performance and can handle various semantic segmentation tasks of land cover.","2151-1535","","10.1109/JSTARS.2023.3347595","National Natural Science Foundation of China(grant numbers:42075130); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10375099","Dual encoder–decoder;image segmentation;land cover;vision transformer","Transformers;Feature extraction;Convolution;Task analysis;Decoding;Semantics;Semantic segmentation","","14","","54","CCBYNCND","27 Dec 2023","","","IEEE","IEEE Journals"
"An Efficient Weakly-Supervised Learning Method for Optic Disc Segmentation","Y. Wen; L. Chen; L. Qiao; C. Zhou; S. Xi; R. Guo; Y. Deng","Key Laboratory of Digital Media Technology of Sichuan Province, School of Computer Science and Engineering; Key Laboratory of Digital Media Technology of Sichuan Province, School of Computer Science and Engineering; School of Medicine, University of Electronic Science and Technology of China; Key Laboratory of Digital Media Technology of Sichuan Province, School of Computer Science and Engineering; Key Laboratory of Digital Media Technology of Sichuan Province, School of Computer Science and Engineering; Key Laboratory of Digital Media Technology of Sichuan Province, School of Computer Science and Engineering; Dept. Biomedical Engineering, King’s College London, London, UK",2020 IEEE International Conference on Bioinformatics and Biomedicine (BIBM),"13 Jan 2021","2020","","","835","842","Accurate optic disc segmentation plays an essential role in the early diagnosis of glaucoma, which has been a major cause of irreversible blindness for the past decade. Recently, U-shape Convolutional Neural Network (CNN) models have achieved favourable performance in optic disc segmentation. However, it is worth noting that these models require a large number of pixel-level annotations while these annotations are difficult to obtain in clinical practice. As a solution, weakly-supervised training methods are commonly implemented, but it will provoke U-shape CNN generating inaccurate, diluted, and grid-like segmentation results. In this paper, we propose a novel Hybrid Network (HyNet) to solve the issue above. HyNet consists of a U-shape backbone hybridized with a cross-scale connection structure, which makes better use of multi-scale visual semantics. Nevertheless, the generalization ability of HyNet is affected by the domain shift among different datasets. Therefore, we innovatively combine weakly- and fully-supervised training methods, namely Hybrid Process (HyProcess), to solve the domain shift problem. Experimental results on ONHSD, DRIONS-DB, and DRISHTI-GS datasets show that our model outperforms the state-of-the-art, reaching Dice of 82.39(%), 93.72(%), and 95.34(%) respectively. Additionally, our ablation study validates the effectiveness of HyNet along with HyProcess, and further analysis reflects their value in clinical practice.","","978-1-7281-6215-7","10.1109/BIBM49941.2020.9313558","Research and Development; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9313558","Segmentation;Optic disc;Convolutional neural network;Weakly supervision","Image segmentation;Training;Annotations;Visualization;Optical imaging;Semantics;Interpolation","","4","","39","IEEE","13 Jan 2021","","","IEEE","IEEE Conferences"
"UMiT-Net: A U-Shaped Mix-Transformer Network for Extracting Precise Roads Using Remote Sensing Images","F. Deng; W. Luo; Y. Ni; X. Wang; Y. Wang; G. Zhang","College of Computer Science and Cyber Security, Chengdu University of Technology, Chengdu, China; College of Computer Science and Cyber Security, Chengdu University of Technology, Chengdu, China; Bureau of Geophysical Prospecting Inc. (BGP), China National Petroleum Corporation (CNPC), Zhuozhou, China; College of Geophysics, Chengdu University of Technology, Chengdu, China; Bureau of Geophysical Prospecting Inc. (BGP), China National Petroleum Corporation (CNPC), Zhuozhou, China; State Key Laboratory of Oil and Gas Reservoir Geology and Exploitation, School of Geoscience and Technology, Southwest Petroleum University, Chengdu, China",IEEE Transactions on Geoscience and Remote Sensing,"13 Jun 2023","2023","61","","1","13","Automatic extraction of high-precision roads from remote sensing images is crucial for path planning and road monitoring. However, there is room to improve the accuracy and generalization of existing methods in segmentation due to the challenges posed by ground object occlusion and complex backgrounds. Most existing methods rely on convolutional neural networks (CNNs), but the limitations of convolution prevent direct semantic interaction at a distance. In contrast, mix-Transformer (MiT) obtains long-term modeling capability through the self-attention mechanism, and inspired by it, we propose a multiscale self-adaptive network [U-shaped MiT network (UMiT-Net)] based on the U-shaped structure. First, UMiT-Net extracts global features with the efficient MiT backbone. Second, the dilated attention module (DAM) is used in the bottleneck of the network to fuse semantic features further to ensure the connectivity of the road. Third, in the decoder, to improve the accuracy of road segmentation, we construct the multiscale self-adaptive module (MSAM), which summarizes rich scene understanding from dense contexts with strip windows conforming to road morphology, and embed an edge enhancement module (EEM) to correct road edges. Finally, we design patch expanding (PE), which solves the problem of heavy computation of upsampling due to high resolution. The experimental results show that our UMiT-Net is substantially ahead of other state-of-the-art methods and has a significant improvement in generalization ability.","1558-0644","","10.1109/TGRS.2023.3281132","National Natural Science Foundation of China(grant numbers:41930112); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10138345","Mix-Transformer (MiT);remote sensing image;road extraction;semantic segmentation;U-shaped","Roads;Feature extraction;Remote sensing;Semantics;Decoding;Dams;Image edge detection","","7","","51","IEEE","29 May 2023","","","IEEE","IEEE Journals"
"Liver Semantic Segmentation Algorithm Based on Improved Deep Adversarial Networks in Combination of Weighted Loss Function on Abdominal CT Images","K. Xia; H. Yin; P. Qian; Y. Jiang; S. Wang","Changshu Affiliated Hospital of Soochow University, Changshu No.1 People’s Hospital, Changshu, China; School of Information and Control Engineering, China University of Mining and Technology, Xuzhou, China; Jiangsu Key Laboratory of Media Design and Software Technology, Wuxi, China; Jiangsu Key Laboratory of Media Design and Software Technology, Wuxi, China; School of Architecture Building and Civil Engineering, Loughborough University, Loughborough, U.K.",IEEE Access,"31 Jul 2019","2019","7","","96349","96358","Due to the space inconsistency between benchmark image and segmentation result in many existing semantic segmentation algorithms for abdominal CT images, an improved model based on the basic framework of DeepLab-v3 is proposed, and Pix2pix network is introduced as the generation adversarial model. Our proposed model realizes the segmentation framework combining deep feature with multi-scale semantic feature. In order to improve the generalization ability and training accuracy of the model, this paper proposes a combination of the traditional multi-classification cross-entropy loss function with the content loss function of generator output and the adversarial loss function of discriminator output. A large number of qualitative and quantitative experimental results show that the performance of our proposed semantic segmentation algorithm is better than the existing algorithms, and can improve the segmentation efficiency while ensuring the space consistency of the semantics segmentation for abdominal CT images.","2169-3536","","10.1109/ACCESS.2019.2929270","Jiangsu Committee of Health on the Subject(grant numbers:H2018071); National Natural Science Foundation of China(grant numbers:61702225,61772241); Natural Science Foundation of Jiangsu Province(grant numbers:BK20160187); Six Talent Peaks Project in Jiangsu Province(grant numbers:XYDXX-127); Science and Technology Demonstration Project of Social Development of Wuxi(grant numbers:WX18IVJN002); Open Fund Project of Jiangsu Key Laboratory of Media Design and Software Technology (Jiangnan University)(grant numbers:19ST0205); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8764548","Semantic segmentation;generation adversarial networks;weighted loss function;multi-scale features;game adversarial;atrous space pyramid pooling","Image segmentation;Semantics;Liver;Generators;Generative adversarial networks;Cancer;Convolution","","88","","32","CCBY","16 Jul 2019","","","IEEE","IEEE Journals"
"Occlusion-Aware Unsupervised Light Field Depth Estimation Based on Multi-Scale GANs","W. Yan; X. Zhang; H. Chen","College of Electrical and Information Engineering, Hunan University, Changsha, Hunan, China; College of Electrical and Information Engineering, Hunan University, Changsha, Hunan, China; College of Computer Science and Electronic Engineering, Hunan University, Changsha, Hunan, China",IEEE Transactions on Circuits and Systems for Video Technology,"2 Jul 2024","2024","34","7","6318","6333","The estimation of depth from 4D light field images is a fundamental problem for perceiving and reconstructing environmental scenes. While learning-based methods have achieved remarkable results in this field, most of them rely on supervised learning, which faces significant challenges in real-world applications due to the lack of sufficient available ground truth depth maps. In this paper, we propose an unsupervised learning architecture based on a generative adversarial learning model for light field image depth estimation (OALFGAN). Specifically, our approach involves a multi-scale deep convolutional generative adversarial network learning system that includes a sparse-to-dense cascaded multi-scale generator and a discriminator, which decomposes the problem of generating high-quality images into more manageable sub-problems. To address the issue of violations of photometric consistency that may be caused by occlusion, we introduce a spatial-angular attention module that adaptively extracts view features with fewer occlusions and richer textures to generate more accurate disparity maps. Furthermore, we design a loss function that incorporates adaptive angular entropy consistency, symmetry loss, and edge-aware loss based on the distribution regularity and self-constraint of light field images to further optimize occlusion and disparity discontinuity issues and improve the reliability of the final depth prediction. Our proposed method demonstrates superior performance over existing methods on synthetic datasets, both quantitatively and qualitatively. Moreover, our proposed method exhibits excellent generalization performance on real-world datasets, demonstrating the effectiveness of our approach.","1558-2205","","10.1109/TCSVT.2024.3359661","National Natural Science Foundation of China(grant numbers:62171184,62273139); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10416250","Light field;depth estimation;occlusion;unsupervised learning;adversarial learning;multi-cue loss;attention modulel","Estimation;Light fields;Learning systems;Generative adversarial networks;Task analysis;Image edge detection;Circuits and systems","","3","","64","IEEE","29 Jan 2024","","","IEEE","IEEE Journals"
"SNLRUX++ for Building Extraction From High-Resolution Remote Sensing Images","Y. Lei; J. Yu; S. Chan; W. Wu; X. Liu","College of Computer Science and Technology, Zhejiang University of Technology, Hangzhou, China; College of Computer Science and Technology, Zhejiang University of Technology, Hangzhou, China; College of Computer Science and Technology, Zhejiang University of Technology, Hangzhou, China; College of Computer Science and Technology, Zhejiang University of Technology, Hangzhou, China; College of Computer Science and Technology, Zhejiang University of Technology, Hangzhou, China",IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing,"30 Dec 2021","2022","15","","409","421","Building extraction plays an important role in high-resolution remote sensing image processing, which can be used as the basis for urban planning and demographic analysis. In recent years, many powerful general semantic segmentation models have emerged, but these models often perform poorly when transferred to remote sensing images because of the characteristics of remote sensing images. To this end, we propose a new deep learning network called Selective Nonlocal ResUNeXt++ (SNLRUX++) for building extraction. First, the cascaded multiscale feature fusion is proposed to transform the high-performance image classification network ResNeXt into the segmentation network ResUNeXt++. Second, selective nonlocal operation is designed to establish long-range dependencies while avoiding introducing excessive noise and computational effort. Finally, multiscale prediction is applied as deep supervision to accelerate training and convergence, and improves prediction performance of objects at different scales. The experimental results on two different remote sensing image datasets show the effectiveness and generalization ability of the proposed method.","2151-1535","","10.1109/JSTARS.2021.3135705","National Natural Science Foundation of China(grant numbers:61906168,61902351); Natural Science Foundation of Zhejiang Province(grant numbers:LY21F020023); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9652051","Building extraction;convolution neural network;deep learning;high-resolution image;remote sensing","Remote sensing;Feature extraction;Semantics;Image segmentation;Buildings;Task analysis;Deep learning","","13","","51","CCBYNCND","15 Dec 2021","","","IEEE","IEEE Journals"
"Semantic Stereo for Incidental Satellite Images","M. Bosch; K. Foster; G. Christie; S. Wang; G. D. Hager; M. Brown",The Johns Hopkins University; The Johns Hopkins University; The Johns Hopkins University; The Johns Hopkins University; The Johns Hopkins University; The Johns Hopkins University,2019 IEEE Winter Conference on Applications of Computer Vision (WACV),"7 Mar 2019","2019","","","1524","1532","The increasingly common use of incidental satellite images for stereo reconstruction versus rigidly tasked binocular or trinocular coincident collection is helping to enable timely global-scale 3D mapping; however, reliable stereo correspondence from multi-date image pairs remains very challenging due to seasonal appearance differences and scene change. Promising recent work suggests that semantic scene segmentation can provide a robust regularizing prior for resolving ambiguities in stereo correspondence and reconstruction problems. To enable research for pairwise semantic stereo and multi-view semantic 3D reconstruction with incidental satellite images, we have established a large-scale public dataset including multi-view, multi-band satellite images and ground truth geometric and semantic labels for two large cities. To demonstrate the complementary nature of the stereo and segmentation tasks, we present lightweight public baselines adapted from recent state of the art convolutional neural network models and assess their performance.","1550-5790","978-1-7281-1975-5","10.1109/WACV.2019.00167","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8659180","","Semantics;Three-dimensional displays;Satellites;Laser radar;Image segmentation;Benchmark testing;Metadata","","109","","48","IEEE","7 Mar 2019","","","IEEE","IEEE Conferences"
"DeepSaliency: Multi-Task Deep Neural Network Model for Salient Object Detection","X. Li; L. Zhao; L. Wei; M. -H. Yang; F. Wu; Y. Zhuang; H. Ling; J. Wang","College of Computer Science, Zhejiang University, Hangzhou, China; College of Computer Science, Zhejiang University, Hangzhou, China; College of Computer Science, Zhejiang University, Hangzhou, China; Electrical Engineering and Computer Science, University of California at Merced, Merced, CA, USA; College of Computer Science, Zhejiang University, Hangzhou, China; College of Computer Science, Zhejiang University, Hangzhou, China; Department of Computer and Information Sciences, Temple University, Philadelphia, PA, USA; Visual Computing Group, Microsoft Research Asia, Beijing, China",IEEE Transactions on Image Processing,"24 Jun 2016","2016","25","8","3919","3930","A key problem in salient object detection is how to effectively model the semantic properties of salient objects in a data-driven manner. In this paper, we propose a multi-task deep saliency model based on a fully convolutional neural network with global input (whole raw images) and global output (whole saliency maps). In principle, the proposed saliency model takes a data-driven strategy for encoding the underlying saliency prior information, and then sets up a multi-task learning scheme for exploring the intrinsic correlations between saliency detection and semantic image segmentation. Through collaborative feature learning from such two correlated tasks, the shared fully convolutional layers produce effective features for object perception. Moreover, it is capable of capturing the semantic information on salient objects across different levels using the fully convolutional layers, which investigate the feature-sharing properties of salient object detection with a great reduction of feature redundancy. Finally, we present a graph Laplacian regularized nonlinear regression model for saliency refinement. Experimental results demonstrate the effectiveness of our approach in comparison with the state-of-the-art approaches.","1941-0042","","10.1109/TIP.2016.2579306","National Natural Science Foundation of China(grant numbers:61472353,U1509206); National Basic Research Program of China(grant numbers:2012CB316400,2015CB352302); Fundamental Research Funds for the Central Universities; NSF CAREER(grant numbers:1149783); NSF IIS(grant numbers:1152576); National Natural Science Foundation of China(grant numbers:61528204); NSF IIS(grant numbers:1218156,1350521); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7488288","salient object detection;CNN;multi-task;datadriven;Salient object detection;CNN;multi-task;data-driven","Semantics;Object detection;Image segmentation;Feature extraction;Neural networks;Convolution;Computational modeling;Regression analysis","","421","4","60","IEEE","9 Jun 2016","","","IEEE","IEEE Journals"
"Unsupervised Visual Representation Learning Based on Segmentation of Geometric Pseudo-Shapes for Transformer-Based Medical Tasks","T. Viriyasaranon; S. M. Woo; J. -H. Choi","Division of Mechanical and Biomedical Engineering, Graduate Program in System Health Science and Engineering, Ewha Womans University, Seoul, South Korea; Center for Liver and Pancreatobiliary Cancer, National Cancer Center, Goyang-si, South Korea; Division of Mechanical and Biomedical Engineering, Graduate Program in System Health Science and Engineering, Ewha Womans University, Seoul, South Korea",IEEE Journal of Biomedical and Health Informatics,"4 Apr 2023","2023","27","4","2003","2014","Recently, transformer-based architectures have been shown to outperform classic convolutional architectures and have rapidly been established as state-of-the-art models for many medical vision tasks. Their superior performance can be explained by their ability to capture long-range dependencies of their multi-head self-attention mechanism. However, they tend to overfit on small- or even medium-sized datasets because of their weak inductive bias. As a result, they require massive, labeled datasets, which are expensive to obtain, especially in the medical domain. This motivated us to explore unsupervised semantic feature learning without any form of annotation. In this work, we aimed to learn semantic features in a self-supervised manner by training transformer-based models to segment the numerical signals of geometric shapes inserted on original computed tomography (CT) images. Moreover, we developed a Convolutional Pyramid vision Transformer (CPT) that leverages multi-kernel convolutional patch embedding and local spatial reduction in each of its layer to generate multi-scale features, capture local information, and reduce computational cost. Using these approaches, we were able to noticeably outperformed state-of-the-art deep learning-based segmentation or classification models of liver cancer CT datasets of 5,237 patients, the pancreatic cancer CT datasets of 6,063 patients, and breast cancer MRI dataset of 127 patients.","2168-2208","","10.1109/JBHI.2023.3237596","Technology Development Program of MSS(grant numbers:S3146559); National Research Foundation of Korea(grant numbers:NRF-2022M3A9I2017587,NRF-2022R1A2C1092072); Korea Medical Device Development Fund(grant numbers:1711174276,RS-2020-KD000016); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10018448","CT images;MRI images;cancer classification;cancer segmentation;self-supervised pretraining;vision transformer;liver cancer;pancreatic cancer;breast cancer","Computed tomography;Shape;Task analysis;Transformers;Biomedical imaging;Image segmentation;Cancer","Humans;Female;Breast Neoplasms;Electric Power Supplies;Liver Neoplasms;Pancreatic Neoplasms;Semantics;Image Processing, Computer-Assisted","6","","53","IEEE","17 Jan 2023","","","IEEE","IEEE Journals"
"Research on Industrial Surface Defect Detection Based on Improved YOLOv5s","Z. Quan; X. Yan; K. Wang","Hefei University of Technology, Hefei, China; Nanjing University of Aeronautics and Astronautics, Nanjing, China; Nanjing University of Aeronautics and Astronautics, Nanjing, China","2024 IEEE 6th International Conference on Power, Intelligent Computing and Systems (ICPICS)","24 Dec 2024","2024","","","1527","1533","Metals and plastics are widely used as important basic materials in industrial manufacturing. In this paper, we propose a detection technique based on improved YOLOv5s. First, we integrate a CFPNet module into the backbone network of YOLOv5s for dense prediction. CFPNet generates multiscale features by introducing multiple cascading stages in the backbone network, where each stage consists of a sub-backbone network for feature extraction and a lightweight transformation block for feature integration. This design enables deeper and more efficient feature fusion. Second, we introduce the ODConv module into the YOLOv5s network. The ODConv module employs a novel multidimensional attention mechanism that learns complementary attention in the four dimensions of the convolutional kernel in any convolutional layer through a parallel strategy. The module improves the detection accuracy while reducing additional parameters by enhancing the feature learning capability. Experimental results show that the improved YOLOv5s algorithm achieves an overall average accuracy of 74.1%, which is a 12.3% improvement compared to the original YOLOv5s model. The amount of parameters in this algorithm is reduced by 14.7% compared to the YOLOv5-SC algorithm. This improvement achieves a better balance between computational complexity, accuracy, and generalization capability, meeting the need for lightweight defect detection in industrial production.","2834-8567","979-8-3503-7431-5","10.1109/ICPICS62053.2024.10797075","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10797075","Defect Detection;CFPNet;ODConv;YOLOv5s","YOLO;Representation learning;Accuracy;Metals;Production;Feature extraction;Surface fitting;Plastics;Computational complexity;Defect detection","","","","5","IEEE","24 Dec 2024","","","IEEE","IEEE Conferences"
"Joint Multi-person Pose Estimation and Semantic Part Segmentation","F. Xia; P. Wang; X. Chen; A. L. Yuille","University of California, Los Angeles, Los Angeles, CA; University of California, Los Angeles, Los Angeles, CA; University of California, Los Angeles, Los Angeles, CA; Johns Hopkins University, Baltimore, MD",2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR),"9 Nov 2017","2017","","","6080","6089","Human pose estimation and semantic part segmentation are two complementary tasks in computer vision. In this paper, we propose to solve the two tasks jointly for natural multi-person images, in which the estimated pose provides object-level shape prior to regularize part segments while the part-level segments constrain the variation of pose locations. Specifically, we first train two fully convolutional neural networks (FCNs), namely Pose FCN and Part FCN, to provide initial estimation of pose joint potential and semantic part potential. Then, to refine pose joint location, the two types of potentials are fused with a fully-connected conditional random field (FCRF), where a novel segment-joint smoothness term is used to encourage semantic and spatial consistency between parts and joints. To refine part segments, the refined pose and the original part potential are integrated through a Part FCN, where the skeleton feature from pose serves as additional regularization cues for part segments. Finally, to reduce the complexity of the FCRF, we induce human detection boxes and infer the graph inside each box, making the inference forty times faster. Since theres no dataset that contains both part segments and pose labels, we extend the PASCAL VOC part dataset [6] with human pose joints and perform extensive experiments to compare our method against several most recent strategies. We show that our algorithm surpasses competing methods by 10.6% in pose estimation with much faster speed and by 1.5% in semantic part segmentation.","1063-6919","978-1-5386-0457-1","10.1109/CVPR.2017.644","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8100127","","Pose estimation;Image segmentation;Semantics;Shape;Graphical models;Complexity theory","","164","6","38","IEEE","9 Nov 2017","","","IEEE","IEEE Conferences"
"Real-Time Instrument Segmentation in Robotic Surgery Using Auxiliary Supervised Deep Adversarial Learning","M. Islam; D. A. Atputharuban; R. Ramesh; H. Ren","NUS Graduate School for Integrative Sciences and Engineering and Department of Biomedical Engineering, National University of Singapore, Singapore; Department of Electronics and Telecommunications, University of Moratuwa, Moratuwa, Srilanka; Department of Instrumentation and Control Engineering, National Institute of Technology Trichy, Tiruchirappalli, India; Department of Biomedical Engineering, National University of Singapore, Singapore",IEEE Robotics and Automation Letters,"13 Mar 2019","2019","4","2","2188","2195","Robot-assisted surgery is an emerging technology that has undergone rapid growth with the development of robotics and imaging systems. Innovations in vision, haptics, and accurate movements of robot arms have enabled surgeons to perform precise minimally invasive surgeries. Real-time semantic segmentation of the robotic instruments and tissues is a crucial step in robot-assisted surgery. Accurate and efficient segmentation of the surgical scene not only aids in the identification and tracking of instruments but also provides contextual information about the different tissues and instruments being operated with. For this purpose, we have developed a light-weight cascaded convolutional neural network to segment the surgical instruments from high-resolution videos obtained from a commercial robotic system. We propose a multi-resolution feature fusion module to fuse the feature maps of different dimensions and channels from the auxiliary and main branch. We also introduce a novel way of combining auxiliary loss and adversarial loss to regularize the segmentation model. Auxiliary loss helps the model to learn low-resolution features, and adversarial loss improves the segmentation prediction by learning higher order structural information. The model also consists of a light-weight spatial pyramid pooling unit to aggregate rich contextual information in the intermediate stage. We show that our model surpasses existing algorithms for pixelwise segmentation of surgical instruments in both prediction accuracy and segmentation time of high-resolution videos.","2377-3766","","10.1109/LRA.2019.2900854","Singapore Academic Research Fund(grant numbers:R-397-000-297-114); NMRC Bedside & Bench(grant numbers:R-397-000-245-511); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8648150","Deep learning in robotics and automation;visual tracking;object detection;segmentation and categorization","Instruments;Robots;Surgery;Real-time systems;Image segmentation;Tools;Tracking","","77","","38","IEEE","21 Feb 2019","","","IEEE","IEEE Journals"
"Boundaries Matters: A Novel Multi-Branch Semi-Supervised Semantic Segmentation Method","Y. Li; C. Zhang; H. Wang","Science School, Beijing University of Civil Engineering and Architecture, Beijing, China; Science School, Beijing University of Civil Engineering and Architecture, Beijing, China; Science School, Beijing University of Civil Engineering and Architecture, Beijing, China",IEEE Intelligent Systems,"","2024","PP","99","1","9","In recent years, semi-supervised semantic segmentation (SSS) research has been progressing rapidly. Existing methods usually ignore the classification of detailed pixels such as boundaries, resulting in degraded segmentation performance. To overcome this challenge, we propose a new multi-branch SSS framework, BoundaryMatch, which combines image- and feature-level perturbation branches with boundary detail guidance branch, all utilizing a shared encoder. Specifically, this boundary module enhances segmentation by integrating the learning of spatial information into low-level layers in single-stream manner. Finally, the low-level features and deeper features are fused together to predict the final segmentation result and achieve accurate correction of boundary pixels. This multi-branch approach improves on the shortcomings of consistency regularization that focus only on maintaining the global consistency of the image. Extensive experiments conducted on the Cityscapes and PASCAL VOC 2012 datasets demonstrate that the method proposed in this paper effectively enhances the performance of SSS.","1941-1294","","10.1109/MIS.2024.3491998","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10745265","","Perturbation methods;Semantic segmentation;Training;Data models;Semantics;Predictive models;Convolution;Semisupervised learning;Kernel;Intelligent systems","","","","","IEEE","5 Nov 2024","","","IEEE","IEEE Early Access Articles"
"MRSE-Net: Multiscale Residuals and SE-Attention Network for Water Body Segmentation From Satellite Images","X. Zhang; J. Li; Z. Hua","School of Information and Electronic Engineering, Shandong Technology and Business University, Institute of Network Technology, Yantai, China; School of Computer Science and Technology, Shandong Technology and Business University, Yantai, China; School of Information and Electronic Engineering, Shandong Technology and Business University, Institute of Network Technology, Yantai, China",IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing,"1 Jul 2022","2022","15","","5049","5064","Automatic extraction of water bodies from various satellite images containing complex targets is a very important and challenging task in remote sensing and image interpretation. In recent years, convolutional neural networks (CNNs) have become an important choice in the field of semantic segmentation of remote sensing images. However, generic CNN models present many problems when performing water body segmentation, such as: 1) blurred water body boundaries; 2) difficulty in accommodating different scales of rivers, often losing information about many small-scale rivers; and 3) a large number of trainable parameters. This article proposes an end-to-end CNN structure based on multiscale residuals and squeeze-and-excitation (SE)-attention for water segmentation, called MRSE-Net. MRSE-Net consists of an encoder–decoder and a skip connection, which captures contextual information at different scales using the encoder, and then passes the encoder feature mapping through the improved skip connection, while localization is achieved by the decoder is implemented. With the multiscale residual module, the number of parameters in our model can be significantly reduced and water pixels can be extracted accurately. The SE-attention module is used to enhance the prediction results, mitigate the blurring effect, and make the segmented water boundaries more continuous. Landsat-8 images are used to train our model and validate our proposed method’s performance and effectiveness. In addition, we evaluate our method on Landsat-7 and Sentinel-2 images and obtain the best water segmentation results. Preliminary results on Sentinel-2 images show that the cross-sensor generalization capability of our model is beyond the range of the Landsat sensor family.","2151-1535","","10.1109/JSTARS.2022.3185245","National Natural Science Foundation of China(grant numbers:61772319,62002200,61972235,12001327); Shandong Natural Science Foundation of China(grant numbers:ZR2021QF134,ZR2021MF068); Yantai science and technology innovation development plan(grant numbers:2022JCYJ031); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9803863","Convolutional neural network (CNN);deep learning;multiscale residual;SE-attention;satellite image analysis;water body extraction","Remote sensing;Image segmentation;Feature extraction;Semantics;Earth;Rivers;Convolutional neural networks","","28","","77","CCBY","22 Jun 2022","","","IEEE","IEEE Journals"
"Reconstruction-Assisted Feature Encoding Network for Histologic Subtype Classification of Non-Small Cell Lung Cancer","H. Li; Q. Song; D. Gui; M. Wang; X. Min; A. Li","School of Information Science and Technology, University of Science and Technology of China, Hefei, Anhui, China; Department of Radiology, Anhui Chest Hospital, Hefei, Anhui, China; School of Information Science and Technology, University of Science and Technology of China, Hefei, Anhui, China; School of Information Science and Technology, University of Science and Technology of China, Hefei, Anhui, China; Department of Radiology, Anhui Chest Hospital, Hefei, Anhui, China; School of Information Science and Technology, University of Science and Technology of China, Hefei, Anhui, China",IEEE Journal of Biomedical and Health Informatics,"9 Sep 2022","2022","26","9","4563","4574","Accurate histological subtype classification between adenocarcinoma (ADC) and squamous cell carcinoma (SCC) using computed tomography (CT) images is of great importance to assist clinicians in determining treatment and therapy plans for non-small cell lung cancer (NSCLC) patients. Although current deep learning approaches have achieved promising progress in this field, they are often difficult to capture efficient tumor representations due to inadequate training data, and in consequence show limited performance. In this study, we propose a novel and effective reconstruction-assisted feature encoding network (RAFENet) for histological subtype classification by leveraging an auxiliary image reconstruction task to enable extra guidance and regularization for enhanced tumor feature representations. Different from existing reconstruction-assisted methods that directly use generalizable features obtained from shared encoder for primary task, a dedicated task-aware encoding module is utilized in RAFENet to perform refinement of generalizable features. Specifically, a cascade of cross-level non-local blocks are introduced to progressively refine generalizable features at different levels with the aid of lower-level task-specific information, which can successfully learn multi-level task-specific features tailored to histological subtype classification. Moreover, in addition to widely adopted pixel-wise reconstruction loss, we introduce a powerful semantic consistency loss function to explicitly supervise the training of RAFENet, which combines both feature consistency loss and prediction consistency loss to ensure semantic invariance during image reconstruction. Extensive experimental results show that RAFENet effectively addresses the difficult issues that cannot be resolved by existing reconstruction-based methods and consistently outperforms other state-of-the-art methods on both public and in-house NSCLC datasets. Supplementary material is available at https://github.com/lhch1994/Rafenet_sup_material.","2168-2208","","10.1109/JBHI.2022.3192010","National Natural Science Foundation of China(grant numbers:61971393,61871361,61571414,61471331); Central Government Transfer Payment Financial Construction(grant numbers:Z155080000004); Elite Special Scientific Research Fund of China Red Cross Foundation(grant numbers:XM_HR_YXFN_2021_05_19); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9832737","Deep learning;image reconstruction;feature encoding network;histologic subtype classification;non-small cell lung cancer","Image reconstruction;Task analysis;Feature extraction;Tumors;Semantics;Deep learning;Computed tomography","Carcinoma, Non-Small-Cell Lung;Carcinoma, Squamous Cell;Humans;Image Processing, Computer-Assisted;Lung Neoplasms;Tomography, X-Ray Computed","13","","53","IEEE","18 Jul 2022","","","IEEE","IEEE Journals"
"An Intelligent Low-Complexity Computing Interleaving Wavelet Scattering Based Mobile Shuffling Network for Acoustic Scene Classification","X. Y. Kek; C. S. Chin; Y. Li","Faculty of Science, Agriculture and Engineering, Newcastle University, Victoria St, Singapore; Faculty of Science, Agriculture and Engineering, Newcastle University, Victoria St, Singapore; Xylem Water Solutions Singapore Pte Ltd., 3A International Business Park, Singapore",IEEE Access,"11 Aug 2022","2022","10","","82185","82201","The key towards a low complexity model for convolution neural network is in controlling the number of parameters of the network and ensuring that the input representation is not extremely large. Hence, to tackle low complexity for acoustic scene classification (ASC), this paper proposed an enhanced wavelet scattering representation with a combination of mobile network modules and shuffling modules. While wavelet scattering comprises wavelet transform with multiple wavelet scales, the averaging operation to make the wavelet scattering invariant to translation limit the maximum timescale. Hence, wavelet scattering is affected by Heisenberg’s Uncertainty Principle. However, creating an input representation with multiple timescales does not meet the brief of low complexity modelling. Hence, we proposed a simple mixing of the first and second order with different timescales. The result is an input representation with nearly the same dimension as the usual wavelet scattering but with enhanced multiscale. To further leverage the ‘interleaved’ wavelet scattering, this paper presents sub-spectral shuffling inspired by shuffling modules that use stochasticity to improve the model’s generalization. Unlike channel shuffle that shuffles channel-wise and spatial shuffle that shuffles pixel-wise, sub-spectral shuffle aims at shuffling the feature maps frequency-wise with the concept of binning. Each bin is shuffled, so the high-frequency spectrum is shuffled to low-frequency spectrum position. As such, the model learns the general acoustic profile of a scene rather than memorizing what is happening at the low-frequency or high-frequency spectrum is erratic for ASC. In addition, this paper also studied temporal shuffling, which shuffles the feature maps temporal-wise, and evaluated sub-spectral shuffling, temporal shuffling, and channel shuffling individually. Our results demonstrated the superiority of sub-spectral shuffling and the modularity of shuffling modules. We then evaluate various combinations of the three shuffling modules on three acoustic scene classification datasets. Our best model combines the three shuffling modules and achieves 70.6% classification accuracy on DCASE 2021 Task 1a dataset, 82.15% on ESC-50 dataset, 81% on Urbansound8K, with ~65K parameters and a size of 126.6KB. In addition, the inclusion of shuffling modules has increased the model performance. Sub-spectral shuffling is especially useful in improving logloss, a metric used to determine the confidence level of the model.","2169-3536","","10.1109/ACCESS.2022.3196338","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9849650","Acoustic scene classification;convolutional neural network;mobile network;wavelet scattering","Convolution;Convolutional neural networks;Scattering;Complexity theory;Adaptation models;Acoustics;Neural networks","","2","","55","CCBY","4 Aug 2022","","","IEEE","IEEE Journals"
"Compositional Semantic Mix for Domain Adaptation in Point Cloud Segmentation","C. Saltori; F. Galasso; G. Fiameni; N. Sebe; F. Poiesi; E. Ricci","Department of Information Engineering and Computer Science, University of Trento, Trento, Italy; Department of Computer Science, Sapienza University of Rome, Roma, Italy; NVIDIA AI Technology Center, 2788 San Tomas Expy, Santa Clara, CA, USA; Department of Information Engineering and Computer Science, University of Trento, Trento, Italy; Fondazione Bruno Kessler (FBK), Trento, Italy; Department of Information Engineering and Computer Science, University of Trento, Trento, Italy",IEEE Transactions on Pattern Analysis and Machine Intelligence,"3 Nov 2023","2023","45","12","14234","14247","Deep-learning models for 3D point cloud semantic segmentation exhibit limited generalization capabilities when trained and tested on data captured with different sensors or in varying environments due to domain shift. Domain adaptation methods can be employed to mitigate this domain shift, for instance, by simulating sensor noise, developing domain-agnostic generators, or training point cloud completion networks. Often, these methods are tailored for range view maps or necessitate multi-modal input. In contrast, domain adaptation in the image domain can be executed through sample mixing, which emphasizes input data manipulation rather than employing distinct adaptation modules. In this study, we introduce compositional semantic mixing for point cloud domain adaptation, representing the first unsupervised domain adaptation technique for point cloud segmentation based on semantic and geometric sample mixing. We present a two-branch symmetric network architecture capable of concurrently processing point clouds from a source domain (e.g. synthetic) and point clouds from a target domain (e.g. real-world). Each branch operates within one domain by integrating selected data fragments from the other domain and utilizing semantic information derived from source labels and target (pseudo) labels. Additionally, our method can leverage a limited number of human point-level annotations (semi-supervised) to further enhance performance. We assess our approach in both synthetic-to-real and real-to-real scenarios using LiDAR datasets and demonstrate that it significantly outperforms state-of-the-art methods in both unsupervised and semi-supervised settings.","1939-3539","","10.1109/TPAMI.2023.3310261","OSRAM GmbH; MUR PNRR; FAIR(grant numbers:PE00000013); NextGenerationEU; EU project FEROX(grant numbers:101070440); PRIN PREVUE(grant numbers:2017N2RK7K); EU ISFP PRECRISIS(grant numbers:101100539); EU H2020 MARVEL(grant numbers:957337); Vision and Learning joint laboratory of FBK and UNITN; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10234713","Domain adaptation;unsupervised learning;semi-supervised learning;semantic segmentation;point cloud","Point cloud compression;Three-dimensional displays;Semantics;Semantic segmentation;Task analysis;Data models;Computer architecture","","4","","73","CCBY","30 Aug 2023","","","IEEE","IEEE Journals"
"A Method of Crop Seedling Plant Segmentation on Edge Information Fusion Model","X. Zuo; H. Lin; D. Wang; Z. Cui","School of Mathematics and Big Data, Guizhou Education University, Guiyang, China; Big Data Science and Intelligent Engineering Research Institute, Guizhou Education University, Guiyang, China; School of Mathematics and Big Data, Guizhou Education University, Guiyang, China; School of Mathematics and Big Data, Guizhou Education University, Guiyang, China",IEEE Access,"16 Sep 2022","2022","10","","95281","95293","Automatic segmentation of plant images is a hot issue in plant phenotyping research. It is also one of the core technologies for applications such as crop growth process monitoring and pest identification. Due to the different scales and sizes of fruits, branches and leaves of fruit and vegetable plants in the natural environment, and irregular edges, it is difficult to accurately segment. In order to accurately segment crop seedlings in natural environment and realize automatic measurement of seedling location and phenotype, this paper proposes a crop seedling plant segmentation network model that fuses the semantic and edge information of target regions. The backbone network is composed of the UNET network, which guides the backbone network to perceive the plant edge information when extracting features; uses the spatial hole feature pyramid to build a feature fusion module, which fuses the features extracted by the UNET backbone network and the edge perception module. Combining edge-aware loss and feature fusion loss, a joint loss function is constructed for overall network optimization. The encoder-decoder network is referenced in the study. The encoding network uses densenet to reuse and fuse multi-layer features to improve the way of information transmission; the decoding network uses transposed convolution for upsampling, combined with layer jump connections to fuse shallow detail information and deep semantic information; add a hole between encoding and decoding Atrous spatial pyramid pooling (ASPP) to extract feature maps of different receptive fields to integrate multi-scale features and aggregate contextual information. The experimental results show that under the same network training parameters, the average cross-merging rate and average recall rate obtained by testing the method in this paper are 58.13% and 64.72%, respectively, which are better than the segmentation results corresponding to the manually labeled samples; in addition, adding in the training samples After 10% of the outdoor seedling images, the average pixel accuracy of the proposed method on the outdoor test set can reach 90.54%, with good generalization ability.","2169-3536","","10.1109/ACCESS.2022.3187825","Young Science and Technology Talents Development Project through the Guizhou Provincial Education Department(grant numbers:QianJiaoHeKY[2022]293); Earth Thesis Project through Guizhou Education University; Research Projects of Innovation Group of Guizhou Provincial Department of Education(grant numbers:QianJiaoHeKY[2021]022); Key Disciplines of Guizhou Province—Computer Science and Technology(grant numbers:ZDXK[2018]007); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9812623","Plant segmentation;regional semantic information;edge sensing module","Image segmentation;Feature extraction;Crops;Image edge detection;Training;Semantics;Image color analysis","","5","","39","CCBY","1 Jul 2022","","","IEEE","IEEE Journals"
"Rethinking Transformers for Semantic Segmentation of Remote Sensing Images","Y. Liu; Y. Zhang; Y. Wang; S. Mei","School of Electronics and Information, Northwestern Polytechnical University, Xi’an, China; School of Electronics and Information, Northwestern Polytechnical University, Xi’an, China; School of Electronics and Information, Northwestern Polytechnical University, Xi’an, China; School of Electronics and Information, Northwestern Polytechnical University, Xi’an, China",IEEE Transactions on Geoscience and Remote Sensing,"16 Aug 2023","2023","61","","1","15","Transformer has been widely applied in image processing tasks as a substitute for convolutional neural networks (CNNs) for feature extraction due to its superiority in global context modeling and flexibility in model generalization. However, the existing transformer-based methods for semantic segmentation of remote sensing (RS) images are still with several limitations, which can be summarized into two main aspects: 1) the transformer encoder is generally combined with CNN-based decoder, leading to inconsistency in feature representations; and 2) the strategies for global and local context information utilization are not sufficiently effective. Therefore, in this article, a global-local transformer segmentor (GLOTS) framework is proposed for the semantic segmentation of RS images to acquire consistent feature representations by adopting transformers for both encoding and decoding, in which a masked image modeling (MIM) pretrained transformer encoder is adopted to learn semantic-rich representations of input images and a multiscale global-local transformer decoder is designed to fully exploit the global and local features. Specifically, the transformer decoder uses a feature separation-aggregation module (FSAM) to utilize the feature adequately at different scales and adopts a global-local attention module (GLAM) containing global attention block (GAB) and local attention block (LAB) to capture the global and local context information, respectively. Furthermore, a learnable progressive upsampling strategy (LPUS) is proposed to restore the resolution progressively, which can flexibly recover the fine-grained details in the upsampling process. The experiment results on the three benchmark RS datasets demonstrate that the proposed GLOTS is capable of achieving better performance with some state-of-the-art methods, and the superiority of the proposed framework is also verified by ablation studies. The code will be available at https://github.com/lyhnsn/GLOTS.","1558-0644","","10.1109/TGRS.2023.3302024","National Natural Science Foundation of China(grant numbers:62171381); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10209224","Encoder–decoder structure;global-local transformer;remote sensing (RS);semantic segmentation","Decoding;Semantic segmentation;Task analysis;Current transformers;Semantics;Feature extraction;Visualization","","42","","69","IEEE","4 Aug 2023","","","IEEE","IEEE Journals"
"Semisupervised Edge-Aware Road Extraction via Cross Teaching Between CNN and Transformer","Z. -X. Yang; Z. -H. You; S. -B. Chen; J. Tang; B. Luo","IMIS Lab of Anhui Province, Anhui Provincial Key Lab of Multimodal Cognitive Computation, MOE Key Lab of IC&SP, School of Computer Science and Technology, Anhui University, Hefei, China; IMIS Lab of Anhui Province, Anhui Provincial Key Lab of Multimodal Cognitive Computation, MOE Key Lab of IC&SP, School of Computer Science and Technology, Anhui University, Hefei, China; IMIS Lab of Anhui Province, Anhui Provincial Key Lab of Multimodal Cognitive Computation, MOE Key Lab of IC&SP, School of Computer Science and Technology, Anhui University, Hefei, China; IMIS Lab of Anhui Province, Anhui Provincial Key Lab of Multimodal Cognitive Computation, MOE Key Lab of IC&SP, School of Computer Science and Technology, Anhui University, Hefei, China; IMIS Lab of Anhui Province, Anhui Provincial Key Lab of Multimodal Cognitive Computation, MOE Key Lab of IC&SP, School of Computer Science and Technology, Anhui University, Hefei, China",IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing,"19 Sep 2023","2023","16","","8353","8362","Semisupervised semantic segmentation of remote sensing images has been proven to be an effective approach to reduce manual annotation costs and leverage available unlabeled data to improve segmentation performance. However, some existing methods that focus on self-training and consistent regularization fail to consider large-scale characteristics of remote sensing images and the importance of incorporating road edge information. In this article, we propose a novel semisupervised edge-aware network (SSEANet) for remote sensing image semantic segmentation by jointly training convolutional neural network and transformer. SSEANet focuses on the consistency loss of multiscale features and uses an attention mechanism to fuse road edge information. Extensive experiments on DeepGlobe, Massachusetts, and AerialKITTI–Bavaria datasets show that the proposed method outperforms state-of-the-art, demonstrating its effectiveness.","2151-1535","","10.1109/JSTARS.2023.3310612","NSFC Key Project of International(grant numbers:61860206004); NSFC Key Project of Joint Fund for Enterprise Innovation and Development(grant numbers:U20B2068); National Natural Science Foundation of China(grant numbers:61976004); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10236589","Cross teaching (CT);edge-aware;road extraction;semisupervised learning;transformer","Roads;Remote sensing;Transformers;Image edge detection;Task analysis;Training;Data models","","4","","39","CCBYNCND","31 Aug 2023","","","IEEE","IEEE Journals"
"PVF-NET: Point & Voxel Fusion 3D Object Detection Framework for Point Cloud","Z. Cui; Z. Zhang","Faculty of Engineering and IT, University of Technology of Sydney, Sydney, Australia; Faculty of Engineering and IT, University of Technology of Sydney, Sydney, Australia",2020 17th Conference on Computer and Robot Vision (CRV),"5 Jun 2020","2020","","","125","133","In this paper, we present a novel 3D object detection framework for locating 3D bounding boxes of the target in autonomous driving scenes. Our proposed framework consists of two novel modules, which are twofold proposal fusion module and the RoI deep fusion module. In the former module, we utilized the 3D voxel Sparse Convolution Neural Network (CNN) and PointNet-like network to coarse generate the voxel-based and point-based proposals, where these proposals contain voxel-dense and point-wise features under the raw point cloud. Twofold proposal fusion module integrated those proposals and extended the proposal generalization, thereby dramatically improve the proposals’ recall rate in the first stage for further utilizing in the proposals refinement stage. Given the coarse integrated 3D proposals produced by the twofold proposal module, the RoI deep fusion module is proposed to abstract and aggregate the multi-scale voxel-based feature and the point-wise feature through voxel-aware pooling layer and point-aware pooling layer, respectively. Follow by that, the specific features on the different proposals are integrated via the proposals-aware fusion layer to further enrich the feature dimensionality and utilize the high-quality proposals for the proposals refinement stage to reinforce the prediction of the target bounding boxes. We conduct the experiments on KITTI dataset and evaluate our method on 3D object detection task. Our method achieved 76.79 mAP in moderate difficulty and outperformed many influential object detection models on the KITTI benchmark leaderboard.","","978-1-7281-9891-0","10.1109/CRV50864.2020.00025","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9108695","3D object detection;robotics vision;lidar point cloud","Point cloud compression;Solid modeling;Three-dimensional displays;Neural networks;Object detection;Feature extraction;Proposals","","4","","24","IEEE","5 Jun 2020","","","IEEE","IEEE Conferences"
"An Interactive Prompt Based Network for Urban Floods Area Segmentation Using UAV Images","L. Shi; K. Yang; Y. Chen; G. Chen","Faculty of Geography, Yunnan Normal University, Kunming, China; Faculty of Geography, Yunnan Normal University, Kunming, China; Faculty of Geography, Yunnan Normal University, Kunming, China; Faculty of Geography, Yunnan Normal University, Kunming, China",IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing,"5 Dec 2024","2025","18","","935","948","As climate change intensifies, extreme weather events like floods are occurring with increasing frequency. While data-driven deep learning methods are effective for extracting flood disaster information, their efficiency is constrained by the scarcity of postdisaster samples, the high cost of annotations, and the models’ strong dependence on both the quantity and quality of data. This study introduces an interactive semantic segmentation model based on multisource UAV flood images, incorporating four types of prompts. By embedding expert knowledge into the prompt design, the model reduces annotation costs and enhances generalization capabilities. First, a prompt encoder is developed to map different types of prompt information into a three-channel space using convolutional techniques, thereby reducing sample labeling costs. Moreover, an image encoder that integrates Mamba and convolution is developed to effectively extract global spatial and channel features from flood images while minimizing computational load. Finally, a spatial and channel attention module with residual connections is introduced to enable multiscale fusion and filtering of prompt information and image features across both spatial and channel dimensions, improving the utilization of prompt information. To validate the model's performance, we conduct experiments using UAV flood imagery collected from diverse regions, backgrounds, and angles. The results demonstrate that, under consistent prompt conditions, our model extracts flood areas more efficiently, reducing misclassification and omission errors. Compared with the next best benchmark model, the intersection over union for the flood category improves by at least 3.75%.","2151-1535","","10.1109/JSTARS.2024.3498865","National Natural Science Foundation of China(grant numbers:42071381); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10753337","Flood detection;prompt learning;semantic segmentation;UAV","Climate change;Floods;Detection algorithms;Prompt engineering;Learning systems;Semantic segmentation;Autonomous aerial vehicles;Interactive systems;Meteorology;Load modeling;Feature extraction;Weather forecasting;Deep learning;Satellite images;Urban areas","","","","54","CCBYNCND","14 Nov 2024","","","IEEE","IEEE Journals"
"Generative ConvNet Foundation Model With Sparse Modeling and Low-Frequency Reconstruction for Remote Sensing Image Interpretation","Z. Dong; Y. Gu; T. Liu","School of Electronics and Information Engineering, Harbin Institute of Technology, Harbin, China; School of Electronics and Information Engineering, Harbin Institute of Technology, Harbin, China; School of Electronics and Information Engineering, Harbin Institute of Technology, Harbin, China",IEEE Transactions on Geoscience and Remote Sensing,"11 Jan 2024","2024","62","","1","16","Foundation models offer a highly versatile and precise solution for intelligent interpretation of remote sensing images, thus greatly facilitating various remote sensing applications. Nevertheless, conventional remote sensing foundational models based on generative transformers neglect the consideration of multiscale features and frequency information, limiting their potential for dense prediction tasks in remote sensing scenarios. In this article, we make the first attempt to propose a generative convolutional neural network (ConvNet) foundation model tailored for remote sensing scenarios, which comprises two key components: First, a large dataset named GeoSense, containing approximately nine million diverse remote sensing images, is constructed to enhance the robustness and generalization of the foundation model during the pretraining phase. Second, a sparse modeling and low-frequency reconstruction (SMLFR) framework is designed for self-supervised representation learning of the ConvNet foundation model. Specifically, a sparse modeling strategy is proposed in masked image modeling (MIM), which allows ConvNet to process variable-length sequences by treating unmasked patches as voxels and sparsifying the encoder. In addition, a low-frequency reconstruction target is designed to guide the model’s attention toward essential ground object features in remote sensing images, while mitigating unnecessary detail interference. To evaluate the general performance of our proposed foundation model, comprehensive experiments have been carried out on five datasets across three downstream tasks. Experimental results demonstrate that our method consistently achieves state-of-the-art performance across all the benchmark datasets and downstream tasks. The code and pretrained models will be available at https://github.com/HIT-SIRS/SMLFR.","1558-0644","","10.1109/TGRS.2023.3348479","National Key Research and Development Program of China(grant numbers:2022ZD0118401); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10378718","Convolutional neural network (ConvNet);foundation model;remote sensing;self-supervised pretraining","Remote sensing;Data models;Computational modeling;Task analysis;Image reconstruction;Sensors;Transformers","","6","","109","IEEE","1 Jan 2024","","","IEEE","IEEE Journals"
"Prior Information Guided Coarse-to-fine Dual-branch Encoding Network for Fovea Localization and Optic Disc/Cup Segmentation","H. Lei; J. Zhao; L. Huang; H. Xie; D. Zhao; B. Lei","College of Computer Science & Software Engineering, Shenzhen University, Shenzhen, China; College of Computer Science & Software Engineering, Shenzhen University, Shenzhen, China; College of Computer Science & Software Engineering, Shenzhen University, Shenzhen, China; School of Biomedical Engineering, Shenzhen University, Shenzhen, China; School of Computer Information & Science, Southwest University, Shenzhen, China; School of Biomedical Engineering, Shenzhen University, Shenzhen, China",2023 IEEE International Conference on Bioinformatics and Biomedicine (BIBM),"18 Jan 2024","2023","","","1218","1223","Fundus images are commonly used to document the presence and severity of various retinal degenerative diseases, where the fovea, optic disc (OD), and optic cup (OC) serve as important anatomical landmarks. Locating and segmenting these landmarks are crucial for clinical diagnosis and treatment. Many existing methods treat the recognition of the fovea, OD, and OC as separate tasks without incorporating any clinical prior knowledge related to various anatomical structures. In this paper, we propose a prior information guided coarse-to-fine dual-branch encoding network, which enables fovea localization and OD/OC segmentation. In coarse stage, we employ a dual-branch network consisting of convolutional neural network (CNN) and Transformer to encode local and global features, and then utilize multi-scale feature fusion techniques to merge the extracted semantic features, aiming to enhance the localization accuracy. In addition, we effectively use the distance information from each pixel to the landmark of interest, and output the results of distance map and heat map regression as prior information to further guide the network to learn the positional relationship between fovea and OD. In fine stage, we refine the region of interest (ROI) of the OD, balance the distribution of the OD and OC using polar coordinate transformation (PCT), extract critical boundary features using the boundary attention module (BAM), and improve the generalization performance of our method through model ensemble strategy. Extensive experimental results demonstrate that our proposed method outperforms existing state-of-the-art (SOTA) methods on the publicly available GAMMA and REFUGE datasets.","2156-1133","979-8-3503-3748-8","10.1109/BIBM58861.2023.10385988","National Natural Science Foundation of China; Natural Science Foundation of Guangdong Province; National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10385988","Prior information;coarse-to-fine;dual-branch;fovea localization;optic disc and cup segmentation","Location awareness;Heating systems;Image segmentation;Feature extraction;Optical imaging;Retina;Transformers","","","","19","IEEE","18 Jan 2024","","","IEEE","IEEE Conferences"
"Regularized Densely-Connected Pyramid Network for Salient Instance Segmentation","Y. -H. Wu; Y. Liu; L. Zhang; W. Gao; M. -M. Cheng","College of Computer Science, Nankai University, Tianjin, TKLNDST, China; ETH Zurich, Zürich, Switzerland; School of Information and Communication Engineering, University of Electronic Science and Technology of China, Chengdu, China; Science and Technology on Complex System Control and Intelligent Agent Cooperation Laboratory, Beijing, China; College of Computer Science, Nankai University, Tianjin, TKLNDST, China",IEEE Transactions on Image Processing,"26 Mar 2021","2021","30","","3897","3907","Much of the recent efforts on salient object detection (SOD) have been devoted to producing accurate saliency maps without being aware of their instance labels. To this end, we propose a new pipeline for end-to-end salient instance segmentation (SIS) that predicts a class-agnostic mask for each detected salient instance. To better use the rich feature hierarchies in deep networks and enhance the side predictions, we propose the regularized dense connections, which attentively promote informative features and suppress non-informative ones from all feature pyramids. A novel multi-level RoIAlign based decoder is introduced to adaptively aggregate multi-level features for better mask predictions. Such strategies can be well-encapsulated into the Mask R-CNN pipeline. Extensive experiments on popular benchmarks demonstrate that our design significantly outperforms existing state-of-the-art competitors by 6.3% (58.6% vs. 52.3%) in terms of the AP metric. The code is available at https://github.com/yuhuan-wu/RDPNet.","1941-0042","","10.1109/TIP.2021.3065822","Major Project for New Generation of AI(grant numbers:2018AAA0100400); Science and Technology Innovation Project from the Chinese Ministry of Education; NSFC(grant numbers:61922046); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9382868","Salient instance segmentation;feature pyramid;RoIAlign","Feature extraction;Semantics;Image segmentation;Visualization;Task analysis;Object detection;Convolution","","25","","72","IEEE","22 Mar 2021","","","IEEE","IEEE Journals"
"StereoSpike: Depth Learning With a Spiking Neural Network","U. Rançon; J. Cuadrado-Anibarro; B. R. Cottereau; T. Masquelier","CerCo, CNRS UM 5549, Université Toulouse III, Toulouse, France; CerCo, CNRS UM 5549, Université Toulouse III, Toulouse, France; CerCo, CNRS UM 5549, Université Toulouse III, Toulouse, France; CerCo, CNRS UM 5549, Université Toulouse III, Toulouse, France",IEEE Access,"12 Dec 2022","2022","10","","127428","127439","Depth estimation is an important computer vision task, useful in particular for navigation in autonomous vehicles, or for object manipulation in robotics. Here, we propose to solve it using StereoSpike, an end-to-end neuromorphic approach, combining two event-based cameras and a Spiking Neural Network (SNN) with a modified U-Net-like encoder-decoder architecture. More specifically, we used the Multi Vehicle Stereo Event Camera Dataset (MVSEC). It provides a depth ground-truth, which was used to train StereoSpike in a supervised manner, using surrogate gradient descent. We propose a novel readout paradigm to obtain a dense analog prediction–the depth of each pixel– from the spikes of the decoder. We demonstrate that this architecture generalizes very well, even better than its non-spiking counterparts, leading to near state-of-the-art test accuracy. To the best of our knowledge, it is the first time that such a large-scale regression problem is solved by a fully spiking neural network. Finally, we show that very low firing rates (< 5%) can be obtained via regularization, with a minimal cost in accuracy. This means that StereoSpike could be efficiently implemented on neuromorphic chips, opening the door for low power and real time embedded systems.","2169-3536","","10.1109/ACCESS.2022.3226484","Program DesCartes; National Research Foundation, Prime Minister’s Office, Singapore under its Campus for Research Excellence and Technological Enterprise (CREATE) Program; Agence Nationale de la Recherche(grant numbers:ANR-16-CE37-0002-01 “3D3M.”); Agence Nationale de la Recherche(grant numbers:ANR-20-CE23-0004-04 “DeepSee.”); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9969606","Computer vision;bio-inspired learning;deep neural architectures;neuromorphic computing;spiking neural networks;stereo depth regression","Cameras;Task analysis;Neuromorphics;Computer vision;Computer architecture;Estimation;Biological neural networks;Deep learning;Neural networks","","22","","48","CCBY","2 Dec 2022","","","IEEE","IEEE Journals"
"Lightweight Multiscale Neural Architecture Search With Spectral–Spatial Attention for Hyperspectral Image Classification","C. Cao; H. Xiang; W. Song; H. Yi; F. Xiao; X. Gao","MOE Key Laboratory of Intelligent Computing and Information Processing, Xiangtan University, Xiangtan, China; MOE Key Laboratory of Intelligent Computing and Information Processing, Xiangtan University, Xiangtan, China; MOE Key Laboratory of Intelligent Computing and Information Processing, Xiangtan University, Xiangtan, China; MOE Key Laboratory of Intelligent Computing and Information Processing, Xiangtan University, Xiangtan, China; MOE Key Laboratory of Intelligent Computing and Information Processing, Xiangtan University, Xiangtan, China; Hunan Provincial Key Laboratory of Intelligent Computing and Language Information Processing, Hunan Normal University, Changsha, China",IEEE Transactions on Geoscience and Remote Sensing,"17 Mar 2023","2023","61","","1","15","Hyperspectral image (HSI) classification based on neural architecture search (NAS) is a currently attractive frontier as it not only automatically searches complex neural network architecture but also avoids professional knowledge and experience design and alleviates the lacking of generalization ability as well when dealing with a new classification task. However, the existing HSI classification based on NAS has some drawbacks: 1) a huge number of training parameters and high calculations are inductive to overfitting and high complexity and 2) efficient operators are lacking in the search space, which can distinguish spatial locations and spectral features in different bands. Furthermore, as the category samples in HSI data show a serious long-tail distribution phenomenon, HSI classification remains challenging. To address these issues, we propose a lightweight multiscale NAS with spatial–spectral attention (LMSS-NAS) for HSI classification. The main work includes threefold: 1) in order to reduce the number of model parameters and promote spectral–spatial feature fusion, a new lightweight efficient search space is designed, which consists of three equivalent lightweight convolution operators with multiple receptive fields; 2) to fully use the spectral–spatial correlation of HSI, a cube-to-pixel classification framework is designed to mine the local spatial and spectral context; and 3) focal loss and label smoothing loss in computer vision tasks are jointly migrated to LMSS-NAS to improve the unbalanced samples’ classification and model robustness. Experimental results on four public hyperspectral datasets show that the proposed method can achieve competitive classification performance as well as low computational cost. The code is available at https://github.com/xh-captain/LMSS-NAS.","1558-0644","","10.1109/TGRS.2023.3253247","Natural Science Foundation of Hunan Province of China(grant numbers:2022JJ30552,2022JJ30571); Research Foundation of Education Department of Hunan Province of China(grant numbers:21A0109,21B0172); National Natural Science Foundation of China (CN)(grant numbers:61972333,62272404); National Key Research and Development Program of China(grant numbers:2020YFA071350); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10061276","Hyperspectral image (HSI) classification;lightweight;neural architecture search (NAS);spatial separable convolution (SSC);spectral-spatial attention","Convolution;Computer architecture;Feature extraction;Computational modeling;Convolutional neural networks;Three-dimensional displays;Task analysis","","11","","46","IEEE","6 Mar 2023","","","IEEE","IEEE Journals"
"Transformer-CNN Cohort: Semi-supervised Semantic Segmentation by the Best of Both Students","X. Zheng; Y. Luo; C. Fu; K. Liu; L. Wang","AI Thrust, HKUST(GZ), Guangzhou, China; Department of Computer Science, Brown University, USA; Department of Computer Science and Engineering, Northeastern University, Shenyang, China; SMMG/ROAS Thrust, HKUST(GZ); Dept. of CSE, AI/CMA Thrust, HKUST(GZ), HKUST, China",2024 IEEE International Conference on Robotics and Automation (ICRA),"8 Aug 2024","2024","","","11147","11154","The popular methods for semi-supervised semantic segmentation mostly adopt a unitary network model using convolutional neural networks (CNNs) and enforce consistency of the model’s predictions over perturbations applied to the inputs or model. However, such a learning paradigm suffers from two critical limitations: a) learning the discriminative features for the unlabeled data; b) learning both global and local information from the whole image. In this paper, we propose a novel Semi-supervised Learning (SSL) approach, called Transformer-CNN Cohort (TCC), that consists of two students with one based on the vision transformer (ViT) and the other based on the CNN. Our method subtly incorporates the multi-level consistency regularization on the predictions and the heterogeneous feature spaces via pseudo-labeling for the unlabeled data. First, as the inputs of the ViT student are image patches, the feature maps extracted encode crucial class-wise statistics. To this end, we propose class-aware feature consistency distillation (CFCD) that first leverages the outputs of each student as the pseudo labels and generates class-aware feature (CF) maps for knowledge transfer between the two students. Second, as the ViT student has more uniform representations for all layers, we propose consistency-aware cross distillation (CCD) to transfer knowledge between the pixel-wise predictions from the cohort. We validate the TCC framework on Cityscapes and Pascal VOC 2012 datasets, which outperforms existing SSL methods by a large margin. Project page: https://vlislab22.github.io/TCC/.","","979-8-3503-8457-4","10.1109/ICRA57147.2024.10611036","National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10611036","","Semantic segmentation;Perturbation methods;Predictive models;Semisupervised learning;Transformers;Feature extraction;Convolutional neural networks","","3","","61","IEEE","8 Aug 2024","","","IEEE","IEEE Conferences"
