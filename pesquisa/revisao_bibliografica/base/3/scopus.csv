"Authors","Author full names","Author(s) ID","Title","Year","Source title","Volume","Issue","Art. No.","Page start","Page end","Page count","Cited by","DOI","Link","Abstract","Author Keywords","Document Type","Publication Stage","Open Access","Source","EID"
"","","","24th International Conference on Medical Image Computing and Computer Assisted Intervention, MICCAI 2021","2021","Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","12903 LNCS","","","","","5669","0","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85116503927&partnerID=40&md5=5657c1cb4213a2ba6a2c8507f33b2f61","The proceedings contain 531 papers. The special focus in this conference is on Medical Image Computing and Computer Assisted Intervention. The topics include: TransBTS: Multimodal Brain Tumor Segmentation Using Transformer; automatic Polyp Segmentation via Multi-scale Subtraction Network; patch-Free 3D Medical Image Segmentation Driven by Super-Resolution Technique and Self-Supervised Guidance; progressively Normalized Self-Attention Network for Video Polyp Segmentation; SGNet: Structure-Aware Graph-Based Network for Airway Semantic Segmentation; NucMM Dataset: 3D Neuronal Nuclei Instance Segmentation at Sub-Cubic Millimeter Scale; AxonEM Dataset: 3D Axon Instance Segmentation of Brain Cortical Regions; improved Brain Lesion Segmentation with Anatomical Priors from Healthy Subjects; carveMix: A Simple Data Augmentation Method for Brain Lesion Segmentation; TransFuse: Fusing Transformers and CNNs for Medical Image Segmentation; boundary-Aware Transformers for Skin Lesion Segmentation; A Topological-Attention ConvLSTM Network and Its Application to EM Images; BiX-NAS: Searching Efficient Bi-directional Architecture for Medical Image Segmentation; multi-task, Multi-domain Deep Segmentation with Shared Representations and Contrastive Regularization for Sparse Pediatric Datasets; TEDS-Net: Enforcing Diffeomorphisms in Spatial Transformers to Guarantee Topology Preservation in Segmentations; learning Consistency- and Discrepancy-Context for 2D Organ Segmentation; partially-Supervised Learning for Vessel Segmentation in Ocular Images; unsupervised Network Learning for Cell Segmentation; MT-UDA: Towards Unsupervised Cross-modality Medical Image Segmentation with Limited Source Labels; context-Aware Virtual Adversarial Training for Anatomically-Plausible Segmentation; Pancreas CT Segmentation by Predictive Phenotyping; interactive Segmentation via Deep Learning and B-Spline Explicit Active Surfaces; multi-compound Transformer for Accurate Biomedical Image Segmentation; kCBAC-Net: Deeply Supervised Complete Bipartite Networks with Asymmetric Convolutions for Medical Image Segmentation; Refined Local-imbalance-based Weight for Airway Segmentation in CT.","","Conference review","Final","","Scopus","2-s2.0-85116503927"
"Chen Y.; Qin Z.; Zhang M.; Zhang J.","Chen, Yuyan (59075921500); Qin, Zhenyu (59205886900); Zhang, Mengyao (59205887000); Zhang, Jun (57276938600)","59075921500; 59205886900; 59205887000; 57276938600","Remote Sensing Extraction Method of Terraced Fields Based on Improved DeepLab v3+; [改进DeepLab v3+模型下的梯田遥感提取研究]","2024","Smart Agriculture","6","3","","46","57","11","1","10.12133/j.smartag.SA202312028","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85197637442&doi=10.12133%2fj.smartag.SA202312028&partnerID=40&md5=c37ed0f5b9d90d08f6d35d5096079080","[Objective] The accurate estimation of terraced field areas is crucial for addressing issues such as slope erosion control, water retention, soil conservation, and increasing food production. The use of high-resolution remote sensing imagery for terraced field information extraction holds significant importance in these aspects. However, as imaging sensor technologies continue to advance, traditional methods focusing on shallow features may no longer be sufficient for precise and efficient extraction in complex terrains and environments. Deep learning techniques offer a promising solution for accurately extracting terraced field areas from high-resolution remote sensing imagery. By utilizing these advanced algorithms, detailed terraced field characteristics with higher levels of automation can be better identified and analyzed. The aim of this research is to explore a proper deep learning algorithm for accurate terraced field area extraction in high-resolution remote sensing imagery. [Methods] Firstly, a terraced dataset was created using high-resolution remote sensing images captured by the Gaofen-6 satellite during fallow periods. The dataset construction process involved data preprocessing, sample annotation, sample cropping, and dataset partitioning with training set augmentation. To ensure a comprehensive representation of terraced field morphologies, 14 typical regions were selected as training areas based on the topographical distribution characteristics of Yuanyang county. To address misclassifica-tions near image edges caused by limited contextual information, a sliding window approach with a size of 256 pixels and a stride of 192 pixels in each direction was utilized to vary the positions of terraced fields in the images. Additionally, geometric augmentation techniques were applied to both images and labels to enhance data diversity, resulting in a high-resolution terraced remote sensing dataset. Secondly, an improved DeepLab v3+ model was proposed. In the encoder section, a lightweight MobileNet v2 was utilized instead of Xception as the backbone network for the semantic segmentation model. Two shallow features from the 4th and 7th layers of the MobileNet v2 network were extracted to capture relevant information. To address the need for local details and global context simultaneously, the multi-scale feature fusion (MSFF) module was employed to replace the atrous spatial pyramid pooling (ASPP) module. The MSFF module utilized a series of dilated convolutions with increasing dilation rates to handle information loss. Furthermore, a coordinate attention mechanism was applied to both shallow and deep features to enhance the network's understanding of targets. This design aimed to lightweight the DeepLab v3+ model while maintaining segmentation accuracy, thus improving its efficiency for practical applications. [Results and Discussions] The research findings reveal the following key points: (1) The model trained using a combination of near-infrared, red, and green (NirRG) bands demonstrated the optimal overall performance, achieving precision, recall, F1-Score, and intersection over union (IoU) values of 90.11%, 90.22%, 90.17% and 82.10%, respectively. The classification results indicated higher accuracy and fewer discrepancies, with an error in reference area of only 12 hm2. (2) Spatial distribution patterns of terraced fields in Yuanyang county were identified through the deep learning model. The majority of terraced fields were found within the slope range of 8ºto 25º, covering 84.97% of the total terraced area. Additionally, there was a noticeable concentration of terraced fields within the altitude range of 1 000 m to 2 000 m, accounting for 95.02% of the total terraced area. (3) A comparison with the original DeepLab v3+ network showed that the improved DeepLab v3+ model exhibited enhancements in terms of precision, recall, F1-Score, and IoU by 4.62%, 2.61%, 3.81% and 2.81%, respectively. Furthermore, the improved DeepLab v3+ outperformed UNet and the original DeepLab v3+ in terms of parameter count and floating-point operations. Its parameter count was only 28.6% of UNet and 19.5% of the original DeepLab v3+, while the floating-point operations were only 1/5 of UNet and DeepLab v3+. This not only improved computational efficiency but also made the enhanced model more suitable for resource-limited or computationally less powerful environments. The lightweighting of the DeepLab v3+ network led to improvements in accuracy and speed. However, the slection of the NirGB band combination during fallow periods significantly impacted the model's generalization ability. [Conclusions] The research findings highlights the significant contribution of the near-infrared (NIR) band in enhancing the model's ability to learn terraced field features. Comparing different band combinations, it was evident that the NirRG combination resulted in the highest overall recognition performance and precision metrics for terraced fields. In contrast to PSPNet, UNet, and the original DeepLab v3+, the proposed model showcased superior accuracy and performance on the terraced field dataset. Noteworthy improvements were observed in the total parameter count, floating-point operations, and the Epoch that led to optimal model performance, outperforming UNet and DeepLab v3+. This study underscores the heightened accuracy of deep learning in identifying terraced fields from high-resolution remote sensing imagery, providing valuable insights for enhanced monitoring and management of terraced landscapes. copyright©2024 by the authors.","convolutional neural network; DeepLab v3+; GF-6 satellite; remote sensing; terrace extraction","Article","Final","","Scopus","2-s2.0-85197637442"
"Cherukuri V.; Vijay Kumar B.G.; Bala R.; Monga V.","Cherukuri, Venkateswararao (57195523324); Vijay Kumar, B.G. (34870934900); Bala, Raja (7004915617); Monga, Vishal (6602625251)","57195523324; 34870934900; 7004915617; 6602625251","Multi-Scale Regularized Deep Network for Retinal Vessel Segmentation","2019","Proceedings - International Conference on Image Processing, ICIP","2019-September","","8803762","824","828","4","5","10.1109/ICIP.2019.8803762","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85076819118&doi=10.1109%2fICIP.2019.8803762&partnerID=40&md5=416e9092f64e6cef8cd3dbe503c3547c","Vessel segmentation of retinal images is a key diagnostic capability in ophthalmology. Early approaches addressing this problem employed hand-crafted filters to capture vessel structures, accompanied by morphological processing. More recently, deep learning techniques have been employed to significantly enhance segmentation accuracy. We propose a novel domain enriched deep network that consists of two components: 1) a representation network which learns geometric (specifically curvilinear) features that are tailored to retinal images, followed by 2) a task network that utilizes the features obtained from the representation layer to perform pixel-level segmentation. The representation and task networks are learned jointly for any given training set. To obtain effective representation filters, we develop a new orientation constraint that enables geometric diversity of curvilinear features. A multi-scale extension is further developed to enhance segmentation of thin vessels. Experiments performed on two challenging benchmark databases reveal that the proposed regularized deep network can outperform state of the art alternatives as measured by common evaluation metrics. Further, the proposed method exhibits a more graceful decay in performance as training data is reduced. © 2019 IEEE.","deep learning; priors.; segmentation","Conference paper","Final","","Scopus","2-s2.0-85076819118"
"","","","9th International Conference on Image and Graphics, ICIG 2017","2017","Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","10668 LNCS","","","","","1955","0","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85040218912&partnerID=40&md5=62810551460a2d88d38e34e9db5219a3","The proceedings contain 172 papers. The special focus in this conference is on Image and Graphics. The topics include: New tikhonov regularization for blind image restoration; real-time multi-camera video stitching based on improved optimal stitch line and multi-resolution fusion; image quality assessment of enriched tonal levels images; a variational model to extract texture from noisy image data with local variance constraints; Joint visualization of UKF tractography data; semantic segmentation based automatic two-tone portrait synthesis; Parameters sharing multi-items non-parametric factor microfacet model for isotropic and anisotropic BRDFs; SRG and RMSE-based automated segmentation for volume data; shape recovery of endoscopic videos by shape from shading using mesh regularization; RGB-D saliency detection with multi-feature-fused optimization; lazy recoloring; Similar trademark image retrieval integrating LBP and convolutional neural network; adaptive learning compressive tracking based on Kalman filter; Online high-accurate calibration of RGB+3D-LiDAR for autonomous driving; run-based connected components labeling using double-row scan; a 3D tube-object centerline extraction algorithm based on steady fluid dynamics; Moving objects detection in video sequences captured by a PTZ camera; Fast grid-based fluid dynamics simulation with conservation of momentum and kinetic energy on GPU; adaptive density optimization of lattice structures sustaining the external multi-load; hyperspectral image classification based on deep forest and spectral-spatial cooperative feature; research on color image segmentation; hyperspectral image classification using multi vote strategy on convolutional neural network and sparse representation joint feature; efficient deep belief network based hyperspectral image classification; classification of hyperspectral imagery based on dictionary learning and extended multi-attribute profiles; sparse acquisition integral imaging system.","","Conference review","Final","","Scopus","2-s2.0-85040218912"
"Xu G.; Li G.","Xu, Gang (55726324100); Li, Guo (57226172200)","55726324100; 57226172200","Research on lightweight neural network of aerial powerline image segmentation; [轻量化航拍图像电力线语义分割]","2021","Journal of Image and Graphics","26","11","","2605","2618","13","8","10.11834/jig.200690","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85119292127&doi=10.11834%2fjig.200690&partnerID=40&md5=6748002fd1eaaa7cd6c261b0bd5e777b","Objective: Powerline semantic segmentation of aerial images, as an important content of powerline intelligent inspection research, has received widespread attention. Recently, several deep learning-based methods have been proposed in this field and achieved high accuracy. However, two major problems still need to be solved before deep learning models can be applied in practice. First, the sample size of publicly available datasets is small. Unlike target objects in other semantic segmentation tasks (e.g., cars and buildings), powerlines have few textures and structural features, which make powerlines easy to be misidentified, especially in scenes that are not covered by the training set. Therefore, constructing a training set that contains many different background samples is crucial to improve the generalization capability of the model. The second problem is the conflict between the amount of model computation and the limited terminal computing resources. Previous work has demonstrated that an improved U-Net model can segment powerlines from aerial images with satisfactory accuracy. However, the model is computationally expensive for many resource-constrained inference terminals (e.g., unmanned aerial vehicles(UAVs)). Method: In this study, the background images in the training set were learned using a generative adversarial network (GAN) to generate a series of pseudo-backgrounds, and curved powerlines were drawn on the generated images by utilizing conic curves. In detail, a multi-scale-based automatic growth model called progressive growing of GANs (PGGAN) was adopted to learn the mapping of a random noise vector to the background images in the training set. Then, its generator was used to generate serials of the background images. These background images and the curved powerlines generated by the conic curves were fused in the alpha channel. We created three training sets. The first one consisted of only 2 000 real background pictures, and the second was a mixture of 10 000 real and generated background images. The third training dataset was composed of 200 generated backgrounds and used to evaluate the similarity between the generated and original images. At the input of the segmentation network, random hue perturbation was applied to the images to enhance the generalization of the model across seasons. Then, the convergence accuracy of U-Net networks with three different loss functions was compared in RGB and grayscale color spaces to determine the best combination. Specifically, we trained U-Net with focal, soft-IoU, and Dice loss functions in RGB and gray spaces and compared the convergence accuracy, convergence speed, and overfitting of the six obtained models. Afterward, sparse regularization was applied to the pre-trained full model, and structured network pruning was performed to reduce the computation load in network inference. A saliency metric that combines first-order Taylor expansion and 2-norm metric was proposed to guide the regularization and pruning process. It provided a higher compression rate compared with the 2-norm that was used in the previous pruning algorithm. Conventional saliency metrics based on first-order expansion can change by orders of magnitude during the regularization process, thus making threshold selection during the iterative process difficult. Compared with these conventional metrics, the proposed metric has a more stable range of values, which enables the use of iteration-based regularization methods. We adopted a 0-norm-based regularization method to widen the saliency gap between important and unimportant neurons. To select the decision threshold, we used an adaptive approach, which was more robust to changes in luminance compared with the fixed-threshold method used in previous work. Result: Experimental results showed that the convergence accuracy of the curved powerline dataset was higher than that of the straight powerline dataset. In RGB space, the hybrid dataset using GAN had higher convergence accuracy than the dataset using only real images, but no significant improvement in gray space was observed due to the possibility of model collapse. We confirmed that hue disturbance can effectively improve the performance of the model across seasons. The experimental results of the different loss functions revealed that the convergence intersection-over-union(IoU) of RGB and gray spaces under their respective optimal loss functions was 0.578 and 0.586, respectively. Dice and soft-IoU had a negligible difference in convergence speed and achieved the best accuracy in gray and RGB spaces, respectively. The convergence of focal loss was the slowest in both spaces, and neither achieved the optimal accuracy. At the pruning stage, by using the conventional 2-norm saliency metric, the proposed gray space lightweight model (IoU of 0.459) reduced the number of floating-point operations per second (FLOPs) and parameters to 3.05% and 0.03% of the full model in RGB space, respectively (IoU of 0.573). When the proposed joint saliency metric was used, the numbers of FLOPs and parameters further decreased to 0.947% and 0.015% of the complete model, respectively, while maintaining an IoU of 0.42. The experiment also showed that the Otsu threshold method worked stably within the appropriate range of illumination changes, and a negligible difference from the optimal threshold was observed. Conclusion: Improvements in the dataset and loss function independently enhanced the performance of the baseline model. Sparse regularization and network pruning reduced the network parameters and calculation load, which facilitated the deployment of the model on resource-constrained inferring terminals, such as UAVs. The proposed saliency measure exhibited better compression capabilities than the conventional 2-norm metric, and the adaptive threshold method helped improve the robustness of the model when the luminance changed. © 2021, Editorial Office of Journal of Image and Graphics. All right reserved.","Generated adversarial network(GAN); Image semantic segmentation; Network pruning; Smart inspection; Sparse regularization","Article","Final","","Scopus","2-s2.0-85119292127"
"","","","24th International Conference on Medical Image Computing and Computer Assisted Intervention, MICCAI 2021","2021","Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","12908 LNCS","","","","","5669","0","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85116449682&partnerID=40&md5=a936522a42c5fb2669e4c78d144da38a","The proceedings contain 531 papers. The special focus in this conference is on Medical Image Computing and Computer Assisted Intervention. The topics include: TransBTS: Multimodal Brain Tumor Segmentation Using Transformer; automatic Polyp Segmentation via Multi-scale Subtraction Network; patch-Free 3D Medical Image Segmentation Driven by Super-Resolution Technique and Self-Supervised Guidance; progressively Normalized Self-Attention Network for Video Polyp Segmentation; SGNet: Structure-Aware Graph-Based Network for Airway Semantic Segmentation; NucMM Dataset: 3D Neuronal Nuclei Instance Segmentation at Sub-Cubic Millimeter Scale; AxonEM Dataset: 3D Axon Instance Segmentation of Brain Cortical Regions; improved Brain Lesion Segmentation with Anatomical Priors from Healthy Subjects; carveMix: A Simple Data Augmentation Method for Brain Lesion Segmentation; TransFuse: Fusing Transformers and CNNs for Medical Image Segmentation; boundary-Aware Transformers for Skin Lesion Segmentation; A Topological-Attention ConvLSTM Network and Its Application to EM Images; BiX-NAS: Searching Efficient Bi-directional Architecture for Medical Image Segmentation; multi-task, Multi-domain Deep Segmentation with Shared Representations and Contrastive Regularization for Sparse Pediatric Datasets; TEDS-Net: Enforcing Diffeomorphisms in Spatial Transformers to Guarantee Topology Preservation in Segmentations; learning Consistency- and Discrepancy-Context for 2D Organ Segmentation; partially-Supervised Learning for Vessel Segmentation in Ocular Images; unsupervised Network Learning for Cell Segmentation; MT-UDA: Towards Unsupervised Cross-modality Medical Image Segmentation with Limited Source Labels; context-Aware Virtual Adversarial Training for Anatomically-Plausible Segmentation; Pancreas CT Segmentation by Predictive Phenotyping; interactive Segmentation via Deep Learning and B-Spline Explicit Active Surfaces; multi-compound Transformer for Accurate Biomedical Image Segmentation; kCBAC-Net: Deeply Supervised Complete Bipartite Networks with Asymmetric Convolutions for Medical Image Segmentation; Refined Local-imbalance-based Weight for Airway Segmentation in CT.","","Conference review","Final","","Scopus","2-s2.0-85116449682"
"","","","Proceedings - 21st IEEE International Conference on Machine Learning and Applications, ICMLA 2022","2022","Proceedings - 21st IEEE International Conference on Machine Learning and Applications, ICMLA 2022","","","","","","1805","0","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85152284147&partnerID=40&md5=5f193ef8ea2866c93de32e33e8f65499","The proceedings contain 268 papers. The topics discussed include: addressing sample efficiency and model-bias in model-based reinforcement learning; reinforcement learning based architectures for dynamic generation of smart home services; attention-based partial decoupling of policy and value for generalization in reinforcement learning; an empirical evaluation of multivariate time series classification with input transformation across different dimensions; topological regularization for dense prediction; real-time cattle interaction recognition via triple-stream network; deeper bidirectional neural networks with generalized non-vanishing hidden neurons; recycling material classification using convolutional neural networks; automatic key information extraction from visually rich documents; multi-stream deep residual network for cloud imputation using multi-resolution remote sensing imagery; structural health and intelligent monitoring of wind turbine blades with a motorized telescope; and deep learning based re-identification of wooden euro-pallets.","","Conference review","Final","","Scopus","2-s2.0-85152284147"
"","","","21st International Conference on Image Analysis and Processing, ICIAP 2022","2022","Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","13233 LNCS","","","","","2040","0","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85131156934&partnerID=40&md5=da694c2bf2b4a4cf0ff415d80c89905e","The proceedings contain 168 papers. The special focus in this conference is on Image Analysis and Processing. The topics include: Leveraging Road Area Semantic Segmentation with Auxiliary Steering Task; Pulmonary-Restricted COVID-19 Informative Visual Screening Using Chest X-ray Images from Portable Devices; unsupervised Multi-camera Domain Adaptation for Object Detection in Cultural Sites; automatic Classification of Fresco Fragments: A Machine and Deep Learning Study; The AIRES-CH Project: Artificial Intelligence for Digital REStoration of Cultural Heritages Using Nuclear Imaging and Multidimensional Adversarial Neural Networks; Grad 2 VAE: An Explainable Variational Autoencoder Model Based on Online Attentions Preserving Curvatures of Representations; pruning in the Face of Adversaries; Towards Latent Space Optimization of GANs Using Meta-Learning; DMSANet: Dual Multi Scale Attention Network; enhanced Data-Recalibration: Utilizing Validation Data to Mitigate Instance-Dependent Noise in Classification; a Comparison of Deep Learning Methods for Inebriation Recognition in Humans; contrastive Supervised Distillation for Continual Representation Learning; multiple Input Branches Shift Graph Convolutional Network with DropEdge for Skeleton-Based Action Recognition; Robust Object Detection with Multi-input Multi-output Faster R-CNN; don’t Wait Until the Accident Happens: Few-Shot Classification Framework for Car Accident Inspection in a Real World; CVGAN: Image Generation with Capsule Vector-VAE; Avalanche RL: A Continual Reinforcement Learning Library; towards an Efficient Facial Image Compression with Neural Networks; consistency Regularization for Unsupervised Domain Adaptation in Semantic Segmentation; medicinal Boxes Recognition on a Deep Transfer Learning Augmented Reality Mobile Application; towards Efficient and Data Agnostic Image Classification Training Pipeline for Embedded Systems; an Intelligent Scanning Vehicle for Waste Collection Monitoring; avoiding Shortcuts in Unpaired Image-to-Image Translation.","","Conference review","Final","","Scopus","2-s2.0-85131156934"
"Liu Y.; Zhang Y.; Wang Y.; Mei S.","Liu, Yuheng (57937822000); Zhang, Yifan (58747629100); Wang, Ye (59295669000); Mei, Shaohui (25822578400)","57937822000; 58747629100; 59295669000; 25822578400","Rethinking Transformers for Semantic Segmentation of Remote Sensing Images","2023","IEEE Transactions on Geoscience and Remote Sensing","61","","5617515","","","","48","10.1109/TGRS.2023.3302024","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85166767925&doi=10.1109%2fTGRS.2023.3302024&partnerID=40&md5=781fefb001e5d235d7452a09b510f153","Transformer has been widely applied in image processing tasks as a substitute for convolutional neural networks (CNNs) for feature extraction due to its superiority in global context modeling and flexibility in model generalization. However, the existing transformer-based methods for semantic segmentation of remote sensing (RS) images are still with several limitations, which can be summarized into two main aspects: 1) the transformer encoder is generally combined with CNN-based decoder, leading to inconsistency in feature representations; and 2) the strategies for global and local context information utilization are not sufficiently effective. Therefore, in this article, a global-local transformer segmentor (GLOTS) framework is proposed for the semantic segmentation of RS images to acquire consistent feature representations by adopting transformers for both encoding and decoding, in which a masked image modeling (MIM) pretrained transformer encoder is adopted to learn semantic-rich representations of input images and a multiscale global-local transformer decoder is designed to fully exploit the global and local features. Specifically, the transformer decoder uses a feature separation-aggregation module (FSAM) to utilize the feature adequately at different scales and adopts a global-local attention module (GLAM) containing global attention block (GAB) and local attention block (LAB) to capture the global and local context information, respectively. Furthermore, a learnable progressive upsampling strategy (LPUS) is proposed to restore the resolution progressively, which can flexibly recover the fine-grained details in the upsampling process. The experiment results on the three benchmark RS datasets demonstrate that the proposed GLOTS is capable of achieving better performance with some state-of-the-art methods, and the superiority of the proposed framework is also verified by ablation studies. The code will be available at https://github.com/lyhnsn/GLOTS.  © 1980-2012 IEEE.","Encoder-decoder structure; global-local transformer; remote sensing (RS); semantic segmentation","Article","Final","","Scopus","2-s2.0-85166767925"
"Tao C.; Meng Y.; Li J.; Yang B.; Hu F.; Li Y.; Cui C.; Zhang W.","Tao, Chongxin (57224219625); Meng, Yizhuo (57206468834); Li, Junjie (57224222967); Yang, Beibei (57216704141); Hu, Fengmin (57206468334); Li, Yuanxi (57429376600); Cui, Changlu (57196024192); Zhang, Wen (56224552600)","57224219625; 57206468834; 57224222967; 57216704141; 57206468334; 57429376600; 57196024192; 56224552600","MSNet: multispectral semantic segmentation network for remote sensing images","2022","GIScience and Remote Sensing","59","1","","1177","1198","21","22","10.1080/15481603.2022.2101728","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85135455948&doi=10.1080%2f15481603.2022.2101728&partnerID=40&md5=84cbc21d0e3d34aa1c13b752822c5071","In the research of automatic interpretation of remote sensing images, semantic segmentation based on deep convolutional neural networks has been rapidly developed and applied, and the feature segmentation accuracy and network model generalization ability have been gradually improved. However, most of the network designs are mainly oriented to the three visible RGB bands of remote sensing images, aiming to be able to directly borrow the mature natural image semantic segmentation networks and pre-trained models, but simultaneously causing the waste and loss of spectral information in the invisible light bands such as near-infrared (NIR) of remote sensing images. Combining the advantages of multispectral data in distinguishing typical features such as water and vegetation, we propose a novel deep neural network structure called the multispectral semantic segmentation network (MSNet) for semantic segmentation of multi-classified feature scenes. The multispectral remote sensing image bands are split into two groups, visible and invisible, and ResNet-50 is used for feature extraction in both coding stages, and cascaded upsampling is used to recover feature map resolution in the decoding stage, and the multi-scale image features and spectral features from the upsampling process are fused layer by layer using the feature pyramid structure to finally obtain semantic segmentation results. The training and validation results on two publicly available datasets show that MSNet has competitive performance. The code is available: https://github.com/taochx/MSNet. © 2022 The Author(s). Published by Informa UK Limited, trading as Taylor & Francis Group.","feature fusion; Multispectral remote sensing images; semantic segmentation; spectral feature","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85135455948"
"Yang Z.-X.; You Z.-H.; Chen S.-B.; Tang J.; Luo B.","Yang, Zi-Xiong (58571587400); You, Zhi-Hui (57804166100); Chen, Si-Bao (9734673000); Tang, Jin (24286986300); Luo, Bin (57203411639)","58571587400; 57804166100; 9734673000; 24286986300; 57203411639","Semisupervised Edge-Aware Road Extraction via Cross Teaching between CNN and Transformer","2023","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","16","","","8353","8362","9","5","10.1109/JSTARS.2023.3310612","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85170558408&doi=10.1109%2fJSTARS.2023.3310612&partnerID=40&md5=4c1c9a8c6770f4a5119212758f1200de","Semisupervised semantic segmentation of remote sensing images has been proven to be an effective approach to reduce manual annotation costs and leverage available unlabeled data to improve segmentation performance. However, some existing methods that focus on self-training and consistent regularization fail to consider large-scale characteristics of remote sensing images and the importance of incorporating road edge information. In this article, we propose a novel semisupervised edge-aware network (SSEANet) for remote sensing image semantic segmentation by jointly training convolutional neural network and transformer. SSEANet focuses on the consistency loss of multiscale features and uses an attention mechanism to fuse road edge information. Extensive experiments on DeepGlobe, Massachusetts, and AerialKITTI-Bavaria datasets show that the proposed method outperforms state-of-the-art, demonstrating its effectiveness. © 2008-2012 IEEE.","Cross teaching (CT); edge-aware; road extraction; semisupervised learning; transformer","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85170558408"
"Li B.; Sun K.; Lang Y.; Guan L.; Lu F.; Zhu Y.","Li, Benliang (56129844300); Sun, Keyang (57225952075); Lang, Yao (57750918100); Guan, Lan (57215054765); Lu, Fei (58587543400); Zhu, Yi (58440060200)","56129844300; 57225952075; 57750918100; 57215054765; 58587543400; 58440060200","Application of CSGE-PSPnet Remote Sensing Image Semantic Segmentation Technology in Transmission and Transformation Engineering Design","2022","Proceedings - 2022 Asian Conference on Frontiers of Power and Energy, ACFPE 2022","","","","85","89","4","0","10.1109/ACFPE56003.2022.9952233","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85143796067&doi=10.1109%2fACFPE56003.2022.9952233&partnerID=40&md5=968a2f17bf930da6d061c12cb8a789f2","With the breakthrough development of remote sensing technology and deep learning, remote sensing technology also has a broad application prospect in the field of transmission and transformation engineering design. Semantic segmentation, as a key link in remote sensing image processing, is the basis for improving the intelligence of transmission and substation engineering, for siting, construction, maintenance, and monitoring of transmission and substation engineering by accurately identifying important facilities, sensitive locations and environmental changes. Therefore, this paper proposes a remote sensing image semantic segmentation model(CSGE-PSPnet), which is based on PSPNet(Pyramid Scene Parsing Network) and SGE(Spatial Group-Wise Enhance Attention). CSGE-PSPnet could enhance local significance features while extracting global features, realizes the refined extraction of key information, and designs CSGE Block(Spatial-Channel Group-Wise Enhance Attention) to extract channel and spatial local information at the same time to enhance local significance features. Multi-level features are fused through dense connection structures to retain feature information of small-scale targets. Subsequently, the deep global features are mined by PSPNet on the feature map after fusion, and the contextual information of different regions is fused to obtain the segmentation results. In addition, one auxiliary loss layer and one major loss layer further improve the model's generalization ability. The experimental results show that CSGE-PSPnet is superior to other methods and can accurately identify small targets, and CSGE Block significantly enhances the feature representation.  © 2022 IEEE.","mix domain attention; power transmission and transformation engineering; PSPNet; semantic segmentation of remote sensing images; site selection","Conference paper","Final","","Scopus","2-s2.0-85143796067"
"Kannadaguli P.","Kannadaguli, Prashanth (56656540700)","56656540700","YOLO v4 Based Human Detection System Using Aerial Thermal Imaging for UAV Based Surveillance Applications","2020","2020 International Conference on Decision Aid Sciences and Application, DASA 2020","","","9317198","1213","1219","6","34","10.1109/DASA51403.2020.9317198","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85100549065&doi=10.1109%2fDASA51403.2020.9317198&partnerID=40&md5=157773091b6ac0e839ac4b8d871b2d9d","This work is related to building a Human Detection system based on You Only Look Once (YOLO) v4. It is one of the most recent Deep Learning approaches primitively built using single shot detection proposal. Unlike the double stage region-based object detection schemes this technique do not follow semantic segmentation, it does not undergo loss of the object information such as disappearance of the gradients and it does not require pre-defined anchors. This technique comprises strong feature extractors and reinforce multi scale object detection and it is very quick in the multi-threaded GPU environments. Since our fundamental research is concentrated on object classification related to Unmanned Aerial Vehicle (UAV) applications, as a first step we choose to detect the humans from thermal dataset. Therefore, we used thermal images and videos possessed from thermal cameras of UAV 1m to 50m above ground level as our dataset in building the model and testing. The YOLO v4 uses ground truth bounding boxes to extract the features like Weighted Residual Connections (WRC), Cross Stage Partial Connections (CSP), Cross mini Batch Normalization (CmBN), Self-Adversarial Training (SAT), Mish Activation (MA), Mosaic Data Augmentation (MDA) and Drop Block Regularization (DBR). Finally, the performance analysis of these model in terms of mean Average Precision (mAP) indicates that the modelling using YOLO v4 performs in a promising way and it can be used in automatic human detection systems. © 2020 IEEE.","Deep Learning; Object Detection; Smart City; Surveillance; Thermal Imaging; UAV; YOLO v4","Conference paper","Final","","Scopus","2-s2.0-85100549065"
"","","","21st International Conference on Image Analysis and Processing, ICIAP 2022","2022","Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","13232 LNCS","","","","","2040","0","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85130960916&partnerID=40&md5=dd1d0cdc6839a35ffe0f7e5c3c6b4fdd","The proceedings contain 168 papers. The special focus in this conference is on Image Analysis and Processing. The topics include: Leveraging Road Area Semantic Segmentation with Auxiliary Steering Task; Pulmonary-Restricted COVID-19 Informative Visual Screening Using Chest X-ray Images from Portable Devices; unsupervised Multi-camera Domain Adaptation for Object Detection in Cultural Sites; automatic Classification of Fresco Fragments: A Machine and Deep Learning Study; The AIRES-CH Project: Artificial Intelligence for Digital REStoration of Cultural Heritages Using Nuclear Imaging and Multidimensional Adversarial Neural Networks; Grad 2 VAE: An Explainable Variational Autoencoder Model Based on Online Attentions Preserving Curvatures of Representations; pruning in the Face of Adversaries; Towards Latent Space Optimization of GANs Using Meta-Learning; DMSANet: Dual Multi Scale Attention Network; enhanced Data-Recalibration: Utilizing Validation Data to Mitigate Instance-Dependent Noise in Classification; a Comparison of Deep Learning Methods for Inebriation Recognition in Humans; contrastive Supervised Distillation for Continual Representation Learning; multiple Input Branches Shift Graph Convolutional Network with DropEdge for Skeleton-Based Action Recognition; Robust Object Detection with Multi-input Multi-output Faster R-CNN; don’t Wait Until the Accident Happens: Few-Shot Classification Framework for Car Accident Inspection in a Real World; CVGAN: Image Generation with Capsule Vector-VAE; Avalanche RL: A Continual Reinforcement Learning Library; towards an Efficient Facial Image Compression with Neural Networks; consistency Regularization for Unsupervised Domain Adaptation in Semantic Segmentation; medicinal Boxes Recognition on a Deep Transfer Learning Augmented Reality Mobile Application; towards Efficient and Data Agnostic Image Classification Training Pipeline for Embedded Systems; an Intelligent Scanning Vehicle for Waste Collection Monitoring; avoiding Shortcuts in Unpaired Image-to-Image Translation.","","Conference review","Final","","Scopus","2-s2.0-85130960916"
"Chen Y.; He G.; Yin R.; Zheng K.; Wang G.","Chen, Yanlin (57984018900); He, Guojin (14028364400); Yin, Ranyu (57203751769); Zheng, Kaiyuan (57983778200); Wang, Guizhou (36519783300)","57984018900; 14028364400; 57203751769; 57983778200; 36519783300","Comparative Study of Marine Ranching Recognition in Multi-Temporal High-Resolution Remote Sensing Images Based on DeepLab-v3+ and U-Net","2022","Remote Sensing","14","22","5654","","","","15","10.3390/rs14225654","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85142689637&doi=10.3390%2frs14225654&partnerID=40&md5=e4dac3773f3cd232cb8b7fb92e1688f3","The recognition and monitoring of marine ranching help to determine the type, spatial distribution and dynamic change of marine life, as well as promote the rational utilization of marine resources and marine ecological environment protection, which has important research significance and application value. In the study of marine ranching recognition based on high-resolution remote sensing images, the convolutional neural network (CNN)-based deep learning method, which can adapt to different water environments, can be used to identify multi-scale targets and restore the boundaries of extraction results with high accuracy. However, research based on deep learning still has problems such as rarely considering the feature complexity of marine ranching and inadequate discussion of model generalization for multi-temporal images. In this paper, we construct a multi-temporal dataset that describes multiple features of marine ranching to explore the recognition and generalization ability of the semantic segmentation models DeepLab-v3+ and U-Net (used in large-area marine ranching) based on GF-1 remote sensing images with a 2 m spatial resolution. Through experiments, we find that the F-score of the U-Net extraction results from multi-temporal test images is basically stable at more than 90%, while the F-score of DeepLab-v3+ fluctuates around 80%. The results show that, compared with DeepLab-v3+, U-Net has a stronger recognition and generalization ability for marine ranching. U-Net can identify floating raft aquaculture areas at different growing stages and distinguish between cage aquaculture areas with different colors but similar morphological characteristics. © 2022 by the authors.","deep learning; DeepLab-v3+; marine ranching; multi-temporal high-resolution remote sensing images; semantic segmentation; U-Net","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85142689637"
"Wang Z.; Guo J.; Huang W.; Zhang S.","Wang, Zhen (57226052226); Guo, Jianxin (57196261924); Huang, Wenzhun (55570643700); Zhang, Shanwen (55713521800)","57226052226; 57196261924; 55570643700; 55713521800","Side-Scan Sonar Image Segmentation Based on Multi-Channel Fusion Convolution Neural Networks","2022","IEEE Sensors Journal","22","6","","5911","5928","17","29","10.1109/JSEN.2022.3149841","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124737080&doi=10.1109%2fJSEN.2022.3149841&partnerID=40&md5=5bc00e557ce13ee64e3aba97a2ce589d","Side-scan sonar is an important application in the field of ocean exploration. Accurate segmentation of target regions in side-scan sonar images is a challenging issue due to the low-resolution and strong noise interference. To accurately and faster segment the different categories target in sonar image, a novel convolutional neural networks (CNNs) model is proposed in this study. Firstly, the deep separable residual module is used for target regions multi-scale feature extraction and suppression noise feature information interference, and the multi-channel feature fusion method is used to enhance feature information transfer of convolution layers. Secondly, the adaptive supervised function is used for pixel-wise classification of different categories targets. Finally, to improve model generalization ability and robustness, the adaptive transfer learning method is introduced in the model training process. We have performed extensive experiments on side-scan sonar image with different targets and scales. The experimental results show that the detection accuracy of the proposed method reaches 95.73%, which is outperforms other state-of-the-art methods on the side-scan sonar image segmentation tasks. Moreover, the method has fewer computational parameters, facilitating future deployment it to underwater mobile detection devices.  © 2001-2012 IEEE.","adaptive supervised function; adaptive transfer learning; convolution neural network; multi-channel feature fusion; Sonar image segmentation","Article","Final","","Scopus","2-s2.0-85124737080"
"Li Q.; Zorzi S.; Shi Y.; Fraundorfer F.; Zhu X.X.","Li, Qingyu (57208661005); Zorzi, Stefano (57214136113); Shi, Yilei (55495784300); Fraundorfer, Friedrich (8977349800); Zhu, Xiao Xiang (55696622200)","57208661005; 57214136113; 55495784300; 8977349800; 55696622200","RegGAN: An End-to-End Network for Building Footprint Generation with Boundary Regularization","2022","Remote Sensing","14","8","1835","","","","12","10.3390/rs14081835","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85128864677&doi=10.3390%2frs14081835&partnerID=40&md5=f45c09e07a84e47879c94d458f593cb7","Accurate and reliable building footprint maps are of great interest in many applications, e.g., urban monitoring, 3D building modeling, and geographical database updating. When compared to traditional methods, the deep-learning-based semantic segmentation networks have largely boosted the performance of building footprint generation. However, they still are not capable of delineating structured building footprints. Most existing studies dealing with this issue are based on two steps, which regularize building boundaries after the semantic segmentation networks are implemented, making the whole pipeline inefficient. To address this, we propose an end-to-end network for the building footprint generation with boundary regularization, which is termed RegGAN. Our method is based on a generative adversarial network (GAN). Specifically, a multiscale discriminator is proposed to distinguish the input between false and true, and a generator is utilized to learn from the discriminator’s response to generate more realistic building footprints. We propose to incorporate regularized loss in the objective function of RegGAN, in order to further enhance sharp building boundaries. The proposed method is evaluated on two datasets with varying spatial resolutions: the INRIA dataset (30 cm/pixel) and the ISPRS dataset (5 cm/pixel). Experimental results show that RegGAN is able to well preserve regular shapes and sharp building boundaries, which outperforms other competitors. © 2022 by the authors. Licensee MDPI, Basel, Switzerland.","building footprint; generative adversarial network; regularization; semantic segmentation","Article","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85128864677"
"","","","24th International Conference on Medical Image Computing and Computer Assisted Intervention, MICCAI 2021","2021","Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","12906 LNCS","","","","","5669","0","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85116414926&partnerID=40&md5=24c8314016f0c64bead46cfc2160b79f","The proceedings contain 531 papers. The special focus in this conference is on Medical Image Computing and Computer Assisted Intervention. The topics include: TransBTS: Multimodal Brain Tumor Segmentation Using Transformer; automatic Polyp Segmentation via Multi-scale Subtraction Network; patch-Free 3D Medical Image Segmentation Driven by Super-Resolution Technique and Self-Supervised Guidance; progressively Normalized Self-Attention Network for Video Polyp Segmentation; SGNet: Structure-Aware Graph-Based Network for Airway Semantic Segmentation; NucMM Dataset: 3D Neuronal Nuclei Instance Segmentation at Sub-Cubic Millimeter Scale; AxonEM Dataset: 3D Axon Instance Segmentation of Brain Cortical Regions; improved Brain Lesion Segmentation with Anatomical Priors from Healthy Subjects; carveMix: A Simple Data Augmentation Method for Brain Lesion Segmentation; TransFuse: Fusing Transformers and CNNs for Medical Image Segmentation; boundary-Aware Transformers for Skin Lesion Segmentation; A Topological-Attention ConvLSTM Network and Its Application to EM Images; BiX-NAS: Searching Efficient Bi-directional Architecture for Medical Image Segmentation; multi-task, Multi-domain Deep Segmentation with Shared Representations and Contrastive Regularization for Sparse Pediatric Datasets; TEDS-Net: Enforcing Diffeomorphisms in Spatial Transformers to Guarantee Topology Preservation in Segmentations; learning Consistency- and Discrepancy-Context for 2D Organ Segmentation; partially-Supervised Learning for Vessel Segmentation in Ocular Images; unsupervised Network Learning for Cell Segmentation; MT-UDA: Towards Unsupervised Cross-modality Medical Image Segmentation with Limited Source Labels; context-Aware Virtual Adversarial Training for Anatomically-Plausible Segmentation; Pancreas CT Segmentation by Predictive Phenotyping; interactive Segmentation via Deep Learning and B-Spline Explicit Active Surfaces; multi-compound Transformer for Accurate Biomedical Image Segmentation; kCBAC-Net: Deeply Supervised Complete Bipartite Networks with Asymmetric Convolutions for Medical Image Segmentation; Refined Local-imbalance-based Weight for Airway Segmentation in CT.","","Conference review","Final","","Scopus","2-s2.0-85116414926"
"Zhang Y.; Gu P.; Zhang Y.; Wang C.; Chen D.Z.","Zhang, Yizhe (55760519200); Gu, Pengfei (57219052995); Zhang, Yejia (57462160900); Wang, Chaoli (57203797020); Chen, Danny Z. (7405453271)","55760519200; 57219052995; 57462160900; 57203797020; 7405453271","GrNT: Gate-Regularized Network Training for Improving Multi-Scale Fusion in Medical Image Segmentation","2023","Proceedings - International Symposium on Biomedical Imaging","2023-April","","","","","","2","10.1109/ISBI53787.2023.10230431","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85172151003&doi=10.1109%2fISBI53787.2023.10230431&partnerID=40&md5=ce7a031ab3baeeea2b39370165402fdb","Multi-scale fusion is a key for semantic segmentation of medical images. Recent deep learning methods added network complexity to achieve better multi-scale fusion results. However, given the already very expressive architectures (e.g., U-Net), an interesting question is whether additional complexity is necessary for achieving more robust multi-scale fusion performance. In this paper, we proposed a new method for improving the multi-scale fusion performance of a medical image segmentation model. We create a set of binary gates at fusing locations that allow us to control forward and backward flows in training. A gate on-off schedule is imposed so that high-scale (level) features could drive the generation of segmentation, while low-scale features serve as complimentary for reconstructing shape details. Our method is effective, easy to implement, and occurs no extra cost when deploying the trained model. Experiments show that our gate-regularized network training (GrNT) improves widely adopted models (e.g., U-Net, Attention U-Net, and DenseVoxNet) on three segmentation datasets (2017 ISIC Skin Lesion segmentation (2D), MM-WHS CT (3D), and 2016 HVSMR (3D)). © 2023 IEEE.","","Conference paper","Final","","Scopus","2-s2.0-85172151003"
"","","","9th International Conference on Image and Graphics, ICIG 2017","2017","Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","10667 LNCS","","","","","1955","0","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85041815466&partnerID=40&md5=692a1507e6e926e556ff1d0ff7523e13","The proceedings contain 172 papers. The special focus in this conference is on Image and Graphics. The topics include: New tikhonov regularization for blind image restoration; real-time multi-camera video stitching based on improved optimal stitch line and multi-resolution fusion; image quality assessment of enriched tonal levels images; a variational model to extract texture from noisy image data with local variance constraints; Joint visualization of UKF tractography data; semantic segmentation based automatic two-tone portrait synthesis; Parameters sharing multi-items non-parametric factor microfacet model for isotropic and anisotropic BRDFs; SRG and RMSE-based automated segmentation for volume data; shape recovery of endoscopic videos by shape from shading using mesh regularization; RGB-D saliency detection with multi-feature-fused optimization; lazy recoloring; Similar trademark image retrieval integrating LBP and convolutional neural network; adaptive learning compressive tracking based on Kalman filter; Online high-accurate calibration of RGB+3D-LiDAR for autonomous driving; run-based connected components labeling using double-row scan; a 3D tube-object centerline extraction algorithm based on steady fluid dynamics; Moving objects detection in video sequences captured by a PTZ camera; Fast grid-based fluid dynamics simulation with conservation of momentum and kinetic energy on GPU; adaptive density optimization of lattice structures sustaining the external multi-load; hyperspectral image classification based on deep forest and spectral-spatial cooperative feature; research on color image segmentation; hyperspectral image classification using multi vote strategy on convolutional neural network and sparse representation joint feature; efficient deep belief network based hyperspectral image classification; classification of hyperspectral imagery based on dictionary learning and extended multi-attribute profiles; sparse acquisition integral imaging system.","","Conference review","Final","","Scopus","2-s2.0-85041815466"
"Zhang S.; Wang Z.; Wang Z.","Zhang, Shanwen (55713521800); Wang, Zhen (55719695100); Wang, Zuliang (57193567874)","55713521800; 55719695100; 57193567874","Method for image segmentation of cucumber disease leaves based on multi-scale fusion convolutional neural networks; [多尺度融合卷积神经网络的黄瓜病害叶片图像分割方法]","2020","Nongye Gongcheng Xuebao/Transactions of the Chinese Society of Agricultural Engineering","36","16","","149","157","8","20","10.11975/j.issn.1002-6819.2020.16.019","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85093862515&doi=10.11975%2fj.issn.1002-6819.2020.16.019&partnerID=40&md5=06b599f52d65ec96339aed7d21532f90","Cucumber disease leaf image segmentation is an important step in disease detection and disease type recognition. To overcome the shortcomings of the classical disease leaf segmentation methods, image semantic segmentation algorithm based on the Fully Convolution Networks (FCNs) had been widely used in the automatic segmentation of disease leaf images in the complex background. FCNs replaced the last three fully-connected layers with three convolutional layers so that the input image with any size could be accepted. FCNs classified images at the pixel level, resolving the problem of semantic segmentation at the semantic level. FCNs utilized the de-convolutional layer to upsample the feature map of the last convolutional layer and restored it to the same size of the input image so that each pixel could be generated. At the same time, the spatial information of the original input image was retained. Then, the pixel-by-pixel classification was carried out on the above feature maps. The disadvantages of FCNs were that 1) the segmented images by FCNs were still not precise enough. Although the result of 8 times sampling was much better than 32 times sampling, the result of upsampling was still blurred and smooth, and was insensitive to the details of the image; 2) Classification of each pixel did not fully consider the relationship between the pixels. The spatial regularization steps used in the usual segmentation methods based on pixel classification were neglected and lack of spatial consistency. Aiming at the low recognition accuracy problem of the traditional disease leaf image segmentation methods, the Multi-Scale Fusion Convolutional Neural Networks (MSF-CNNS) were proposed for cucumber disease leaf image segmentation. MSF-CNNs consisted of Encoder Networks (ENs) and Decoder Networks (DNs). ENs were composed of a multi-scale Convolutional Neural Networks to extract multi-scale information of images of disease leaves. DNs were a nine-point bilinear interpolation algorithm to restore the size and resolution of the input image. In the process of the model training, a transfer learning method with the gradual adjustment was used to accelerate the training speed and segmentation accuracy of the network model. The architecture of MSF-CNNs is similar to U-Net and SegNet, mainly including encoder networks and decoder networks. However, to extract the multi-scale information of the input image, a multilevel parallel structure was introduced into the encoding network, while a multi-scale connection was introduced into the decoding network. In the specific coding network, the multi-column parallel CNNs could be used to extract the multi-scale features of the image of crop disease leaves. In the decoding network, the size and resolution of the image were restored by introducing the nine-point bisector linear interpolation algorithm as the deconvolution interpolation method. In the structure of the overall network model, skip join was used to pass the characteristic information extracted from different convolutional layers, and batch normalization operation was introduced to alleviate the gradient dispersion phenomenon of the model. Segmentation experiments were carried out on the image database of cucumber disease leaves under the complex background and compared with the existing deep learning models, such as FCNs, SegNet, U-Net, and DenseNet. The results on the cucumber disease leaf image dataset validated that the proposed method met the needs of the cucumber disease leaf image segmentation in the complex environment, with pixel-classification accuracy of 92.38%, the average accuracy of 93.12%, mean intersection over the union of 91.36 and frequency weighted intersection over the union of 89.76%. Compared with FCNs, SegNet, U-NET, and DenseNet, the average accuracy of the proposed method is improved by 13.00%, 10.74%, 10.40%, 10.08%, and 6.40%, respectively. After using the progressive learning training method, the training time was reduced by 0.9 h. The results showed that the proposed method was effective for the image segmentation of the cucumber disease leaves in a complex environment, and could provide technical support for further research on cucumber disease detection and identification. © 2020, Editorial Department of the Transactions of the Chinese Society of Agricultural Engineering. All right reserved.","Convolutional neural networks; Disease; Image segmentation; Multi-scale fusion CNNs; Transfer learning; Vegetable","Article","Final","","Scopus","2-s2.0-85093862515"
"","","","9th International Conference on Scale Space and Variational Methods in Computer Vision, SSVM 2023","2023","Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","14009 LNCS","","","","","756","0","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85161109423&partnerID=40&md5=0819246117a12533c04ab19a13f836f9","The proceedings contain 57 papers. The special focus in this conference is on Scale Space and Variational Methods in Computer Vision. The topics include: EmNeF: Neural Fields for Embedded Variational Problems in Imaging; genHarris-ResNet: A Rotation Invariant Neural Network Based on Elementary Symmetric Polynomials; compressive Learning of Deep Regularization for Denoising; graph Laplacian and Neural Networks for Inverse Problems in Imaging: GraphLaNet; learning Posterior Distributions in Underdetermined Inverse Problems; proximal Residual Flows for Bayesian Inverse Problems; a Model is Worth Tens of Thousands of Examples; resolution-Invariant Image Classification Based on Fourier Neural Operators; graph Laplacian for Semi-supervised Learning; efficient Neural Generation of 4K Masks for Homogeneous Diffusion Inpainting; a Geometrically Aware Auto-Encoder for Multi-texture Synthesis; Fast Marching Energy CNN; deep Accurate Solver for the Geodesic Problem; deep Image Prior Regularized by Coupled Total Variation for Image Colorization; hybrid Training of Denoising Networks to Improve the Texture Acutance of Digital Cameras; latent-Space Disentanglement with Untrained Generator Networks for the Isolation of Different Motion Types in Video Data; natural Numerical Networks on Directed Graphs in Satellite Image Classification; piece-wise Constant Image Segmentation with a Deep Image Prior Approach; On the Inclusion of Topological Requirements in CNNs for Semantic Segmentation Applied to Radiotherapy; a Relaxed Proximal Gradient Descent Algorithm for Convergent Plug-and-Play with Proximal Denoiser; theoretical Foundations for Pseudo-Inversion of Nonlinear Operators; off-the-Grid Charge Algorithm for Curve Reconstruction in Inverse Problems; convergence Guarantees of Overparametrized Wide Deep Inverse Prior; On the Remarkable Efficiency of SMART; wasserstein Gradient Flows of the Discrepancy with Distance Kernel on the Line; a Quasi-Newton Primal-Dual Algorithm with Line Search; stochastic Gradient Descent for Linear Inverse Problems in Variable Exponent Lebesgue Spaces; an Efficient Line Search for Sparse Reconstruction; learned Discretization Schemes for the Second-Order Total Generalized Variation; fluctuation-Based Deconvolution in Fluorescence Microscopy Using Plug-and-Play Denoisers; Segmenting MR Images Through Texture Extraction and Multiplicative Components Optimization; Geometric Adaptations of PDE-G-CNNs.","","Conference review","Final","","Scopus","2-s2.0-85161109423"
"","","","24th International Conference on Medical Image Computing and Computer Assisted Intervention, MICCAI 2021","2021","Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","12904 LNCS","","","","","5669","0","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85116502894&partnerID=40&md5=d5b5b03333c7bb31b1901e5f119ea792","The proceedings contain 531 papers. The special focus in this conference is on Medical Image Computing and Computer Assisted Intervention. The topics include: TransBTS: Multimodal Brain Tumor Segmentation Using Transformer; automatic Polyp Segmentation via Multi-scale Subtraction Network; patch-Free 3D Medical Image Segmentation Driven by Super-Resolution Technique and Self-Supervised Guidance; progressively Normalized Self-Attention Network for Video Polyp Segmentation; SGNet: Structure-Aware Graph-Based Network for Airway Semantic Segmentation; NucMM Dataset: 3D Neuronal Nuclei Instance Segmentation at Sub-Cubic Millimeter Scale; AxonEM Dataset: 3D Axon Instance Segmentation of Brain Cortical Regions; improved Brain Lesion Segmentation with Anatomical Priors from Healthy Subjects; carveMix: A Simple Data Augmentation Method for Brain Lesion Segmentation; TransFuse: Fusing Transformers and CNNs for Medical Image Segmentation; boundary-Aware Transformers for Skin Lesion Segmentation; A Topological-Attention ConvLSTM Network and Its Application to EM Images; BiX-NAS: Searching Efficient Bi-directional Architecture for Medical Image Segmentation; multi-task, Multi-domain Deep Segmentation with Shared Representations and Contrastive Regularization for Sparse Pediatric Datasets; TEDS-Net: Enforcing Diffeomorphisms in Spatial Transformers to Guarantee Topology Preservation in Segmentations; learning Consistency- and Discrepancy-Context for 2D Organ Segmentation; partially-Supervised Learning for Vessel Segmentation in Ocular Images; unsupervised Network Learning for Cell Segmentation; MT-UDA: Towards Unsupervised Cross-modality Medical Image Segmentation with Limited Source Labels; context-Aware Virtual Adversarial Training for Anatomically-Plausible Segmentation; Pancreas CT Segmentation by Predictive Phenotyping; interactive Segmentation via Deep Learning and B-Spline Explicit Active Surfaces; multi-compound Transformer for Accurate Biomedical Image Segmentation; kCBAC-Net: Deeply Supervised Complete Bipartite Networks with Asymmetric Convolutions for Medical Image Segmentation; Refined Local-imbalance-based Weight for Airway Segmentation in CT.","","Conference review","Final","","Scopus","2-s2.0-85116502894"
"He J.; Li X.; Liu N.; Zhan S.","He, Jun (57071980500); Li, Xinke (35176053700); Liu, Ninghui (57218762964); Zhan, Shu (24765737400)","57071980500; 35176053700; 57218762964; 24765737400","Conditional Generative Adversarial Networks with Multi-scale Discriminators for Prostate MRI Segmentation","2020","Neural Processing Letters","52","2","","1251","1261","10","5","10.1007/s11063-020-10303-x","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85090230344&doi=10.1007%2fs11063-020-10303-x&partnerID=40&md5=6343626f142c805ac16e42913086f32c","Accurate prostate MR image segmentation is a necessary preprocessing stage for computer-assisted diagnostic algorithms. Convolutional neural network, as a research focus in recent years, has been proven to be powerful in computer vision field. Recently, the most effective prostate MRI segmentation technology mainly relies on full convolutional network which has been widely used in semantic segmentation task. However, it’s independent and identically distributed assumption neglect the structural regularity present in MR images and miss information between pixels. In this paper, we propose an MRI-conditional generative adversarial networks for prostate segmentation. Our adversarial training make it context aware and the use of adversarial loss functions learn high-level structural information. The network consist of a generator and a discriminator. The generator consists of a contraction channel and an expansion channel like U-Net. The method we proposed uses a multi-scale discriminator which consist of two discriminators with the same structure but different input sizes. The objective function has two parts: one is the adversarial loss, the other is feature matching loss which stabilizes the training and get better convergence. The experiment show that our network can accurately segment the prostate MRI and outperforms most existing methods. © 2020, Springer Science+Business Media, LLC, part of Springer Nature.","Feature matching loss; Generative adversarial networks; Generator; Magnetic resonance images (MRI)","Article","Final","","Scopus","2-s2.0-85090230344"
"Hu H.; Yang L.; Chen J.; Luo S.","Hu, Haiyang (58031787900); Yang, Linnan (36976235700); Chen, Jiaojiao (57956754700); Luo, Shuang (57201583550)","58031787900; 36976235700; 57956754700; 57201583550","The remote sensing image segmentation of land cover based on multi-scale attention features","2023","Proceedings - International Conference on Tools with Artificial Intelligence, ICTAI","","","","429","436","7","0","10.1109/ICTAI59109.2023.00069","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85182395394&doi=10.1109%2fICTAI59109.2023.00069&partnerID=40&md5=660e756997749f89ebbd880bccd3215c","Segmentation of land cover in remote sensing images is a task that involves interpreting remote sensing data using machine vision. Satisfying segmentation results in agriculture and forestry regions can guide land resource management, natural environment protection, urban construction, and the distribution of agricultural products. However, the performance of the widely used deep learning segmentation model on high-resolution remote sensing segmentation datasets in agriculture and forestry regions needs to be improved. To solve the problems of poor accuracy and loss of context information in remote sensing image semantic segmentation, this paper proposes an improved semantic segmentation network architecture. The model utilizes multi-scale feature extraction, deploys a multi-layer attention feature fusion module and an up-sampling fusion module to capture high-quality multi-scale context information, correctly handle scale changes, and help narrow the semantic gap between different levels. Finally, the proposed MLP decoder refers to the dynamic up-sampling operator to aggregate the information at different levels to achieve pixel segmentation. To verify the effectiveness of our proposed model, the researchers conducted experiments on two land cover segmentation datasets. The training process specifically designs data augmentation strategies for remote sensing segmentation tasks to enhance the model's generalization ability. The final model achieved an mIoU (mean Intersection over Union) of 65.05% on the self-built rural land cover datasets, surpassing the benchmark network UPerNet by 5.92%. On the LoveDA dataset, our model achieved state-of-the-art performance with an mIoU of 53.39%, demonstrating its versatility. © 2023 IEEE.","attention mechanism; land cover; multi-scale feature; neural network; remote sensing image","Conference paper","Final","","Scopus","2-s2.0-85182395394"
"Zheng X.; Luo Y.; Fu C.; Liu K.; Wang L.","Zheng, Xu (57357294400); Luo, Yunhao (57767363400); Fu, Chong (59302747000); Liu, Kangcheng (57214756731); Wang, Lin (57203128728)","57357294400; 57767363400; 59302747000; 57214756731; 57203128728","Transformer-CNN Cohort: Semi-supervised Semantic Segmentation by the Best of Both Students","2024","Proceedings - IEEE International Conference on Robotics and Automation","","","","11147","11154","7","0","10.1109/ICRA57147.2024.10611036","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85202441229&doi=10.1109%2fICRA57147.2024.10611036&partnerID=40&md5=204788fde679ef8ea6207cf8e7185965","The popular methods for semi-supervised semantic segmentation mostly adopt a unitary network model using convolutional neural networks (CNNs) and enforce consistency of the model's predictions over perturbations applied to the inputs or model. However, such a learning paradigm suffers from two critical limitations: a) learning the discriminative features for the unlabeled data; b) learning both global and local information from the whole image. In this paper, we propose a novel Semi-supervised Learning (SSL) approach, called Transformer-CNN Cohort (TCC), that consists of two students with one based on the vision transformer (ViT) and the other based on the CNN. Our method subtly incorporates the multi-level consistency regularization on the predictions and the heterogeneous feature spaces via pseudo-labeling for the unlabeled data. First, as the inputs of the ViT student are image patches, the feature maps extracted encode crucial class-wise statistics. To this end, we propose class-aware feature consistency distillation (CFCD) that first leverages the outputs of each student as the pseudo labels and generates class-aware feature (CF) maps for knowledge transfer between the two students. Second, as the ViT student has more uniform representations for all layers, we propose consistency-aware cross distillation (CCD) to transfer knowledge between the pixel-wise predictions from the cohort. We validate the TCC framework on Cityscapes and Pascal VOC 2012 datasets, which outperforms existing SSL methods by a large margin. Project page: https://vlislab22.github.io/TCC/. © 2024 IEEE.","","Conference paper","Final","","Scopus","2-s2.0-85202441229"
"Ding Z.; Zeng F.; Li H.; Zheng J.; Chen J.; Chen B.; Zhong W.; Li X.; Wang Z.; Huang L.; Yue X.","Ding, Ziyu (58636353300); Zeng, Fanguo (57201426735); Li, Haifeng (58738399900); Zheng, Jianyu (58092953200); Chen, Junzhi (59423051800); Chen, Biao (59386255200); Zhong, Wenshan (59423154200); Li, Xuantian (59423464400); Wang, Zhangying (15046243500); Huang, Lifei (55492448500); Yue, Xuejun (15030252400)","58636353300; 57201426735; 58738399900; 58092953200; 59423051800; 59386255200; 59423154200; 59423464400; 15046243500; 55492448500; 15030252400","Identification of sweetpotato virus disease-infected leaves from field images using deep learning","2024","Frontiers in Plant Science","15","","1456713","","","","0","10.3389/fpls.2024.1456713","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85210070666&doi=10.3389%2ffpls.2024.1456713&partnerID=40&md5=0a5a124b73c7a9d47576a75f2c1ac72b","Introduction: Sweetpotato virus disease (SPVD) is widespread and causes significant economic losses. Current diagnostic methods are either costly or labor-intensive, limiting both efficiency and scalability. Methods: The segmentation algorithm proposed in this study can rapidly and accurately identify SPVD lesions from field-captured photos of sweetpotato leaves. Two custom datasets, DS-1 and DS-2, are utilized, containing meticulously annotated images of sweetpotato leaves affected by SPVD. DS-1 is used for training, validation, and testing the model, while DS-2 is exclusively employed to validate the model’s reliability. This study employs a deep learning-based semantic segmentation network, DeepLabV3+, integrated with an Attention Pyramid Fusion (APF) module. The APF module combines a channel attention mechanism with multi-scale feature fusion to enhance the model’s performance in disease pixel segmentation. Additionally, a novel data augmentation technique is utilized to improve recognition accuracy in the edge background areas of real large images, addressing issues of poor segmentation precision in these regions. Transfer learning is applied to enhance the model’s generalization capabilities. Results: The experimental results indicate that the model, with 62.57M parameters and 253.92 Giga Floating Point Operations Per Second (GFLOPs), achieves a mean Intersection over Union (mIoU) of 94.63% and a mean accuracy (mAcc) of 96.99% on the DS-1 test set, and an mIoU of 78.59% and an mAcc of 79.47% on the DS-2 dataset. Discussion: Ablation studies confirm the effectiveness of the proposed data augmentation and APF methods, while comparative experiments demonstrate the model’s superiority across various metrics. The proposed method also exhibits excellent detection results in simulated scenarios. In summary, this study successfully deploys a deep learning framework to segment SPVD lesions from field images of sweetpotato foliage, which will contribute to the rapid and intelligent detection of sweetpotato diseases. Copyright © 2024 Ding, Zeng, Li, Zheng, Chen, Chen, Zhong, Li, Wang, Huang and Yue.","deep learning; RGB image; semantic segmentation; sweetpotato; virus disease","Article","Final","","Scopus","2-s2.0-85210070666"
"Meng P.; Jia S.; Li Q.","Meng, Pengfei (57354556600); Jia, Shuangcheng (59112843200); Li, Qian (57284818900)","57354556600; 59112843200; 57284818900","DMBR-Net: deep multiple-resolution bilateral network for real-time and accurate semantic segmentation","2023","Complex and Intelligent Systems","9","6","","6427","6436","9","1","10.1007/s40747-023-01046-y","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85159346054&doi=10.1007%2fs40747-023-01046-y&partnerID=40&md5=6f225dfa96ee1894da2ca51496c77ec7","It has been proved that the two-branch network architecture for real-time semantic segmentation is effectiveness. However, existing methods still can not obtain sufficient context information and sufficient detailed information, which limits the improvement of the accuracy of existed two-branch methods. In this paper, we proposed a real-time high-precision semantic segmentation network based on a novel multi-resolution feature fusion module, an auxiliary feature extracting module, an upsampling module and multi-ASPP(atrous spatial pyramid pooling) module. We designed a feature fusion module, which is integrated with sufficient features of different resolutions to help the network get both sufficient semantic information and sufficient detailed information. We also studied the effect of the side-branch architecture on the network, and made new discoveries that the role of the side-branch is more than regularization, it may either slow down the convergence or accelerate the convergence by influencing the gradient of different layers of the network, which is dependent on the parameters of the network and the input data. Based on the new discoveries about the side-branch architecture, we used a side-branch auxiliary feature extraction layer in the network to improve the performance of the network. We also designed an upsampling module, which can get better detailed information than the original upsampling module. In addition, we also re-considered the locations and number of atrous spatial pyramid pooling (ASPP) modules, and modified the network architecture according to the experimental results to further improve the performance of the network. We proposed a network based on the above study. We named this network Deep Multiple-resolution Bilateral Network for Real-time, referred to as DMBR-Net. The network proposed in the paper achieved 81.3% mIoU(Mean Intersection over Union) at 110FPS on the Cityscapes validation dataset, 80.7% mIoU at 104FPS on the CamVid test dataset, 32.2% mIoU at 78FPS on the COCO-Stuff test dataset. © 2023, The Author(s).","ASPP; Auxiliary feature extraction; Multi-resolution feature; Upsample","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85159346054"
"","","","7th International Brain Lesion Workshop, BrainLes 2021, held in conjunction with the Medical Image Computing and Computer Assisted Intervention, MICCAI 2021","2022","Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","12962 LNCS","","","","","1079","0","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85135017028&partnerID=40&md5=e4977a2170e5dea7ad9e8d6de1e410ae","The proceedings contain 93 papers. The special focus in this conference is on Brain Lesion. The topics include: Predicting Isocitrate Dehydrogenase Mutation Status in Glioma Using Structural Brain Networks and Graph Neural Networks; Optimization of Deep Learning Based Brain Extraction in MRI for Low Resource Environments; Reciprocal Adversarial Learning for Brain Tumor Segmentation: A Solution to BraTS Challenge 2021 Segmentation Task; unet3D with Multiple Atrous Convolutions Attention Block for Brain Tumor Segmentation; BRATS2021: Exploring Each Sequence in Multi-modal Input for Baseline U-net Performance; combining Global Information with Topological Prior for Brain Tumor Segmentation; automatic Brain Tumor Segmentation Using Multi-scale Features and Attention Mechanism; Simple and Fast Convolutional Neural Network Applied to Median Cross Sections for Predicting the Presence of MGMT Promoter Methylation in FLAIR MRI Scans; Brain Tumor Segmentation Using Non-local Mask R-CNN and Single Model Ensemble; opportunities and Challenges for Deep Learning in Brain Lesions; efficientNet for Brain-Lesion Classification; HarDNet-BTS: A Harmonic Shortcut Network for Brain Tumor Segmentation; Swin UNETR: Swin Transformers for Semantic Segmentation of Brain Tumors in MRI Images; Multi-plane UNet++ Ensemble for Glioblastoma Segmentation; Multimodal Brain Tumor Segmentation Using Modified UNet Architecture; A Video Data Based Transfer Learning Approach for Classification of MGMT Status in Brain Tumor MR Images; Multimodal Brain Tumor Segmentation Using a 3D ResUNet in BraTS 2021; 3D MRI Brain Tumour Segmentation with Autoencoder Regularization and Hausdorff Distance Loss Function; 3D CMM-Net with Deeper Encoder for Semantic Segmentation of Brain Tumors in BraTS2021 Challenge; multi Modal Fusion for Radiogenomics Classification of Brain Tumor; EMSViT: Efficient Multi Scale Vision Transformer for Biomedical Image Segmentation; a Joint Graph and Image Convolution Network for Automatic Brain Tumor Segmentation; brain Tumor Segmentation Using Neural Network Topology Search; residual 3D U-Net with Localization for Brain Tumor Segmentation.","","Conference review","Final","","Scopus","2-s2.0-85135017028"
"","","","9th International Conference on Intelligence Science and Big Data Engineering, IScIDE 2019","2019","Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","11935 LNCS","","","","","1025","0","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85077115656&partnerID=40&md5=e0fdccc68927a610529a2f015a94af1a","The proceedings contain 84 papers. The special focus in this conference is on Intelligence Science and Big Data Engineering. The topics include: Deep Blind Image Inpainting; robust Object Tracking Based on Multi-granularity Sparse Representation; a Bypass-Based U-Net for Medical Image Segmentation; real-Time Visual Object Tracking Based on Reinforcement Learning with Twin Delayed Deep Deterministic Algorithm; efficiently Handling Scale Variation for Pedestrian Detection; leukocyte Segmentation via End-to-End Learning of Deep Convolutional Neural Networks; Coupled Squeeze-and-Excitation Blocks Based CNN for Image Compression; soft Transferring and Progressive Learning for Human Action Recognition; face Sketch Synthesis Based on Adaptive Similarity Regularization; adaptive Online Learning for Video Object Segmentation; Three-Dimensional Coronary Artery Centerline Extraction and Cross Sectional Lumen Quantification from CT Angiography Images; a Robust Facial Landmark Detector with Mixed Loss; Object Guided Beam Steering Algorithm for Optical Phased Array (OPA) LIDAR; channel Max Pooling for Image Classification; A Multi-resolution Coarse-to-Fine Segmentation Framework with Active Learning in 3D Brain MRI; deep 3D Facial Landmark Detection on Position Maps; joint Object Detection and Depth Estimation in Multiplexed Image; weakly-Supervised Semantic Segmentation with Mean Teacher Learning; APAC-Net: Unsupervised Learning of Depth and Ego-Motion from Monocular Video; robust Image Recovery via Mask Matrix; proposal-Aware Visual Saliency Detection with Semantic Attention; multiple Objects Tracking Based Vehicle Speed Analysis with Gaussian Filter from Drone Video; A Novel Small Vehicle Detection Method Based on UAV Using Scale Adaptive Gradient Adjustment; a Level Set Method for Natural Image Segmentation by Texture and High Order Edge-Detector; multi-scale Residual Dense Block for Video Super-Resolution.","","Conference review","Final","","Scopus","2-s2.0-85077115656"
"","","","1st International Symposium on Geometry and Vision, ISGV 2021","2021","Communications in Computer and Information Science","1386 CCIS","","","","","392","0","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85104869502&partnerID=40&md5=b4389a1043f289f768d0eacb94ba0719","The proceedings contain 29 papers. The special focus in this conference is on Geometry and Vision. The topics include: Coverless Video Steganography Based on Inter Frame Combination; character Photo Selection for Mobile Platform; close Euclidean Shortest Path Crossing an Ordered 3D Skew Segment Sequence; a Lane Line Detection Algorithm Based on Convolutional Neural Network; segment- and Arc-Based Vectorizations by Multi-scale/Irregular Tangential Covering; algorithms for Computing Topological Invariants in Digital Spaces; discrete Linear Geometry on Non-square Grid; electric Scooter and Its Rider Detection Framework Based on Deep Learning for Supporting Scooter-Related Injury Emergency Services; tracking Livestock Using a Fully Connected Network and Kalman Filter; traffic-Sign Recognition Using Deep Learning; a Comparison of Approaches for Synchronizing Events in Video Streams Using Audio; union-Retire: A New Paradigm for Single-Pass Connected Component Analysis; improving Object Detection in Real-World Traffic Scenes; comparison of Red versus Blue Laser Light for Accurate 3D Measurement of Highly Specular Surfaces in Ambient Lighting Conditions; fruit Detection from Digital Images Using CenterNet; a Graph-Regularized Non-local Hyperspectral Image Denoising Method; random Convolutional Network for Hyperspectral Image Classification; mamboNet: Adversarial Semantic Segmentation for Autonomous Driving; effective Pavement Crack Delineation Using a Cascaded Dilation Module and Fully Convolutional Networks; d-GaussianNet: Adaptive Distorted Gaussian Matched Filter with Convolutional Neural Network for Retinal Vessel Segmentation; tree Leaves Detection Based on Deep Learning; deep Learning in Medical Applications: Lesion Segmentation in Skin Cancer Images Using Modified and Improved Encoder-Decoder Architecture; apple Ripeness Identification Using Deep Learning; a Hand-Held Sensor System for Exploration and Thermal Mapping of Volcanic Fumarole Fields.","","Conference review","Final","","Scopus","2-s2.0-85104869502"
"","","","7th International Brain Lesion Workshop, BrainLes 2021, held in conjunction with the Medical Image Computing and Computer Assisted Intervention, MICCAI 2021","2022","Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","12963 LNCS","","","","","1079","0","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85135196018&partnerID=40&md5=bb63448e42e99376c60adc30f0a048dd","The proceedings contain 93 papers. The special focus in this conference is on Brain Lesion. The topics include: Predicting Isocitrate Dehydrogenase Mutation Status in Glioma Using Structural Brain Networks and Graph Neural Networks; Optimization of Deep Learning Based Brain Extraction in MRI for Low Resource Environments; Reciprocal Adversarial Learning for Brain Tumor Segmentation: A Solution to BraTS Challenge 2021 Segmentation Task; unet3D with Multiple Atrous Convolutions Attention Block for Brain Tumor Segmentation; BRATS2021: Exploring Each Sequence in Multi-modal Input for Baseline U-net Performance; combining Global Information with Topological Prior for Brain Tumor Segmentation; automatic Brain Tumor Segmentation Using Multi-scale Features and Attention Mechanism; Simple and Fast Convolutional Neural Network Applied to Median Cross Sections for Predicting the Presence of MGMT Promoter Methylation in FLAIR MRI Scans; Brain Tumor Segmentation Using Non-local Mask R-CNN and Single Model Ensemble; opportunities and Challenges for Deep Learning in Brain Lesions; efficientNet for Brain-Lesion Classification; HarDNet-BTS: A Harmonic Shortcut Network for Brain Tumor Segmentation; Swin UNETR: Swin Transformers for Semantic Segmentation of Brain Tumors in MRI Images; Multi-plane UNet++ Ensemble for Glioblastoma Segmentation; Multimodal Brain Tumor Segmentation Using Modified UNet Architecture; A Video Data Based Transfer Learning Approach for Classification of MGMT Status in Brain Tumor MR Images; Multimodal Brain Tumor Segmentation Using a 3D ResUNet in BraTS 2021; 3D MRI Brain Tumour Segmentation with Autoencoder Regularization and Hausdorff Distance Loss Function; 3D CMM-Net with Deeper Encoder for Semantic Segmentation of Brain Tumors in BraTS2021 Challenge; multi Modal Fusion for Radiogenomics Classification of Brain Tumor; EMSViT: Efficient Multi Scale Vision Transformer for Biomedical Image Segmentation; a Joint Graph and Image Convolution Network for Automatic Brain Tumor Segmentation; brain Tumor Segmentation Using Neural Network Topology Search; residual 3D U-Net with Localization for Brain Tumor Segmentation.","","Conference review","Final","","Scopus","2-s2.0-85135196018"
"","","","21st International Conference on Image Analysis and Processing, ICIAP 2022","2022","Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","13231 LNCS","","","","","2040","0","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85130983060&partnerID=40&md5=d63d6b5977d6a92e604774b6f4e43290","The proceedings contain 168 papers. The special focus in this conference is on Image Analysis and Processing. The topics include: Leveraging Road Area Semantic Segmentation with Auxiliary Steering Task; Pulmonary-Restricted COVID-19 Informative Visual Screening Using Chest X-ray Images from Portable Devices; unsupervised Multi-camera Domain Adaptation for Object Detection in Cultural Sites; automatic Classification of Fresco Fragments: A Machine and Deep Learning Study; The AIRES-CH Project: Artificial Intelligence for Digital REStoration of Cultural Heritages Using Nuclear Imaging and Multidimensional Adversarial Neural Networks; Grad 2 VAE: An Explainable Variational Autoencoder Model Based on Online Attentions Preserving Curvatures of Representations; pruning in the Face of Adversaries; Towards Latent Space Optimization of GANs Using Meta-Learning; DMSANet: Dual Multi Scale Attention Network; enhanced Data-Recalibration: Utilizing Validation Data to Mitigate Instance-Dependent Noise in Classification; a Comparison of Deep Learning Methods for Inebriation Recognition in Humans; contrastive Supervised Distillation for Continual Representation Learning; multiple Input Branches Shift Graph Convolutional Network with DropEdge for Skeleton-Based Action Recognition; Robust Object Detection with Multi-input Multi-output Faster R-CNN; don’t Wait Until the Accident Happens: Few-Shot Classification Framework for Car Accident Inspection in a Real World; CVGAN: Image Generation with Capsule Vector-VAE; Avalanche RL: A Continual Reinforcement Learning Library; towards an Efficient Facial Image Compression with Neural Networks; consistency Regularization for Unsupervised Domain Adaptation in Semantic Segmentation; medicinal Boxes Recognition on a Deep Transfer Learning Augmented Reality Mobile Application; towards Efficient and Data Agnostic Image Classification Training Pipeline for Embedded Systems; an Intelligent Scanning Vehicle for Waste Collection Monitoring; avoiding Shortcuts in Unpaired Image-to-Image Translation.","","Conference review","Final","","Scopus","2-s2.0-85130983060"
"","","","24th International Conference on Medical Image Computing and Computer Assisted Intervention, MICCAI 2021","2021","Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","12905 LNCS","","","","","5669","0","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85116483216&partnerID=40&md5=d8ea466f5569906b9013d7ac1ab9218e","The proceedings contain 531 papers. The special focus in this conference is on Medical Image Computing and Computer Assisted Intervention. The topics include: TransBTS: Multimodal Brain Tumor Segmentation Using Transformer; automatic Polyp Segmentation via Multi-scale Subtraction Network; patch-Free 3D Medical Image Segmentation Driven by Super-Resolution Technique and Self-Supervised Guidance; progressively Normalized Self-Attention Network for Video Polyp Segmentation; SGNet: Structure-Aware Graph-Based Network for Airway Semantic Segmentation; NucMM Dataset: 3D Neuronal Nuclei Instance Segmentation at Sub-Cubic Millimeter Scale; AxonEM Dataset: 3D Axon Instance Segmentation of Brain Cortical Regions; improved Brain Lesion Segmentation with Anatomical Priors from Healthy Subjects; carveMix: A Simple Data Augmentation Method for Brain Lesion Segmentation; TransFuse: Fusing Transformers and CNNs for Medical Image Segmentation; boundary-Aware Transformers for Skin Lesion Segmentation; A Topological-Attention ConvLSTM Network and Its Application to EM Images; BiX-NAS: Searching Efficient Bi-directional Architecture for Medical Image Segmentation; multi-task, Multi-domain Deep Segmentation with Shared Representations and Contrastive Regularization for Sparse Pediatric Datasets; TEDS-Net: Enforcing Diffeomorphisms in Spatial Transformers to Guarantee Topology Preservation in Segmentations; learning Consistency- and Discrepancy-Context for 2D Organ Segmentation; partially-Supervised Learning for Vessel Segmentation in Ocular Images; unsupervised Network Learning for Cell Segmentation; MT-UDA: Towards Unsupervised Cross-modality Medical Image Segmentation with Limited Source Labels; context-Aware Virtual Adversarial Training for Anatomically-Plausible Segmentation; Pancreas CT Segmentation by Predictive Phenotyping; interactive Segmentation via Deep Learning and B-Spline Explicit Active Surfaces; multi-compound Transformer for Accurate Biomedical Image Segmentation; kCBAC-Net: Deeply Supervised Complete Bipartite Networks with Asymmetric Convolutions for Medical Image Segmentation; Refined Local-imbalance-based Weight for Airway Segmentation in CT.","","Conference review","Final","","Scopus","2-s2.0-85116483216"
"","","","24th International Conference on Medical Image Computing and Computer Assisted Intervention, MICCAI 2021","2021","Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","12907 LNCS","","","","","5669","0","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85116422857&partnerID=40&md5=45821b76f85ad97f711a110a557847b2","The proceedings contain 531 papers. The special focus in this conference is on Medical Image Computing and Computer Assisted Intervention. The topics include: TransBTS: Multimodal Brain Tumor Segmentation Using Transformer; automatic Polyp Segmentation via Multi-scale Subtraction Network; patch-Free 3D Medical Image Segmentation Driven by Super-Resolution Technique and Self-Supervised Guidance; progressively Normalized Self-Attention Network for Video Polyp Segmentation; SGNet: Structure-Aware Graph-Based Network for Airway Semantic Segmentation; NucMM Dataset: 3D Neuronal Nuclei Instance Segmentation at Sub-Cubic Millimeter Scale; AxonEM Dataset: 3D Axon Instance Segmentation of Brain Cortical Regions; improved Brain Lesion Segmentation with Anatomical Priors from Healthy Subjects; carveMix: A Simple Data Augmentation Method for Brain Lesion Segmentation; TransFuse: Fusing Transformers and CNNs for Medical Image Segmentation; boundary-Aware Transformers for Skin Lesion Segmentation; A Topological-Attention ConvLSTM Network and Its Application to EM Images; BiX-NAS: Searching Efficient Bi-directional Architecture for Medical Image Segmentation; multi-task, Multi-domain Deep Segmentation with Shared Representations and Contrastive Regularization for Sparse Pediatric Datasets; TEDS-Net: Enforcing Diffeomorphisms in Spatial Transformers to Guarantee Topology Preservation in Segmentations; learning Consistency- and Discrepancy-Context for 2D Organ Segmentation; partially-Supervised Learning for Vessel Segmentation in Ocular Images; unsupervised Network Learning for Cell Segmentation; MT-UDA: Towards Unsupervised Cross-modality Medical Image Segmentation with Limited Source Labels; context-Aware Virtual Adversarial Training for Anatomically-Plausible Segmentation; Pancreas CT Segmentation by Predictive Phenotyping; interactive Segmentation via Deep Learning and B-Spline Explicit Active Surfaces; multi-compound Transformer for Accurate Biomedical Image Segmentation; kCBAC-Net: Deeply Supervised Complete Bipartite Networks with Asymmetric Convolutions for Medical Image Segmentation; Refined Local-imbalance-based Weight for Airway Segmentation in CT.","","Conference review","Final","","Scopus","2-s2.0-85116422857"
"","","","24th International Conference on Medical Image Computing and Computer Assisted Intervention, MICCAI 2021","2021","Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","12902 LNCS","","","","","5669","0","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85116420317&partnerID=40&md5=194f8fbb9ea8e318f74235f15df14ce3","The proceedings contain 531 papers. The special focus in this conference is on Medical Image Computing and Computer Assisted Intervention. The topics include: TransBTS: Multimodal Brain Tumor Segmentation Using Transformer; automatic Polyp Segmentation via Multi-scale Subtraction Network; patch-Free 3D Medical Image Segmentation Driven by Super-Resolution Technique and Self-Supervised Guidance; progressively Normalized Self-Attention Network for Video Polyp Segmentation; SGNet: Structure-Aware Graph-Based Network for Airway Semantic Segmentation; NucMM Dataset: 3D Neuronal Nuclei Instance Segmentation at Sub-Cubic Millimeter Scale; AxonEM Dataset: 3D Axon Instance Segmentation of Brain Cortical Regions; improved Brain Lesion Segmentation with Anatomical Priors from Healthy Subjects; carveMix: A Simple Data Augmentation Method for Brain Lesion Segmentation; TransFuse: Fusing Transformers and CNNs for Medical Image Segmentation; boundary-Aware Transformers for Skin Lesion Segmentation; A Topological-Attention ConvLSTM Network and Its Application to EM Images; BiX-NAS: Searching Efficient Bi-directional Architecture for Medical Image Segmentation; multi-task, Multi-domain Deep Segmentation with Shared Representations and Contrastive Regularization for Sparse Pediatric Datasets; TEDS-Net: Enforcing Diffeomorphisms in Spatial Transformers to Guarantee Topology Preservation in Segmentations; learning Consistency- and Discrepancy-Context for 2D Organ Segmentation; partially-Supervised Learning for Vessel Segmentation in Ocular Images; unsupervised Network Learning for Cell Segmentation; MT-UDA: Towards Unsupervised Cross-modality Medical Image Segmentation with Limited Source Labels; context-Aware Virtual Adversarial Training for Anatomically-Plausible Segmentation; Pancreas CT Segmentation by Predictive Phenotyping; interactive Segmentation via Deep Learning and B-Spline Explicit Active Surfaces; multi-compound Transformer for Accurate Biomedical Image Segmentation; kCBAC-Net: Deeply Supervised Complete Bipartite Networks with Asymmetric Convolutions for Medical Image Segmentation; Refined Local-imbalance-based Weight for Airway Segmentation in CT.","","Conference review","Final","","Scopus","2-s2.0-85116420317"
"Tian M.; Yang Q.; Gao Y.","Tian, Mu (57222712119); Yang, Qinzhu (57485894400); Gao, Yi (58039670200)","57222712119; 57485894400; 58039670200","Multi-scale Multi-task Distillation for Incremental 3D Medical Image Segmentation","2023","Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","13803 LNCS","","","369","384","15","3","10.1007/978-3-031-25066-8_20","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85151136392&doi=10.1007%2f978-3-031-25066-8_20&partnerID=40&md5=6e1c8a98c295a40a622b13eb9357fcd6","Automatic medical image segmentation is the core component for many clinical applications. Substantial number of deep learning based methods have been proposed in past years, but deploying such methods in practice faces certain difficulties, such as the acquisition of massive annotated data for training and the high latency of model iteration. In contrast to the conventional cycle of “data collection, offline training, model update”, developing a system that continuously generates robust predictions will be critical. Recently, incremental learning was widely investigated for classification and semantic segmentation on 2D natural images. Existing work showed the effectiveness of data rehearsal and knowledge distillation in counteracting catastrophic forgetting. Inspired by these advances, we propose a multi-scale multi-task distillation framework for incremental learning with 3D medical images. Different from the task-incremental scenario in literature, our proposed strategy focuses on improving robustness against implicit data distribution shift. We introduce knowledge distillation as multi-task regularization to resolve prediction confusions. At each step, the network is instructed to learn towards both the new ground truth and the uncertainty weighted predictions from the previous model. Simultaneously, image features at multiple scales in the segmentation network could participate in a contrastive learning scheme, aiming at more discriminant representations that inherit the past knowledge effectively. Experiments showed that our method improved overall continual learning robustness under the extremely challenging scenario of “seeing each image once in a batch of one” without any pre-training. In addition, the proposed method could work on top of any network architectures and existing incremental learning strategies. We also showed further improvements by combining our method with data rehearsal using a small buffer. © 2023, The Author(s), under exclusive license to Springer Nature Switzerland AG.","3D medical image segmentation; Distillation; Incremental learning; Multi-scale; Multi-task","Conference paper","Final","","Scopus","2-s2.0-85151136392"
"Chen Y.; Zhang L.; Chen B.; Zuo J.; Hu Y.","Chen, Yuyang (59383256600); Zhang, Li (58853352100); Chen, Bowei (56789262700); Zuo, Jian (58125423000); Hu, Yingwen (58984338500)","59383256600; 58853352100; 56789262700; 58125423000; 58984338500","MPG-Net: A Semantic Segmentation Model for Extracting Aquaculture Ponds in Coastal Areas from Sentinel-2 MSI and Planet SuperDove Images","2024","Remote Sensing","16","20","3760","","","","0","10.3390/rs16203760","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85207371299&doi=10.3390%2frs16203760&partnerID=40&md5=f56570702910282b693dda3e70afc756","Achieving precise and swift monitoring of aquaculture ponds in coastal regions is essential for the scientific planning of spatial layouts in aquaculture zones and the advancement of ecological sustainability in coastal areas. However, because the distribution of many land types in coastal areas and the complex spectral features of remote sensing images are prone to the phenomenon of ‘same spectrum heterogeneous objects’, the current deep learning model is susceptible to background noise interference in the face of complex backgrounds, resulting in poor model generalization ability. Moreover, with the image features of aquaculture ponds of different scales, the model has limited multi-scale feature extraction ability, making it difficult to extract effective edge features. To address these issues, this work suggests a novel semantic segmentation model for aquaculture ponds called MPG-Net, which is based on an enhanced version of the U-Net model and primarily comprises two structures: MS and PGC. The MS structure integrates the Inception module and the Dilated residual module in order to enhance the model’s ability to extract the features of aquaculture ponds and effectively solve the problem of gradient disappearance in the training of the model; the PGC structure integrates the Global Context module and the Polarized Self-Attention in order to enhance the model’s ability to understand the contextual semantic information and reduce the interference of redundant information. Using Sentinel-2 and Planet images as data sources, the effectiveness of the refined method is confirmed through ablation experiments conducted on the two structures. The experimental comparison using the FCN8S, SegNet, U-Net, and DeepLabV3 classical semantic segmentation models shows that the MPG-Net model outperforms the other four models in all four precision evaluation indicators; the average values of precision, recall, IoU, and F1-Score of the two image datasets with different resolutions are 94.95%, 92.95%, 88.57%, and 93.94%, respectively. These values prove that the MPG-Net model has better robustness and generalization ability, which can reduce the interference of irrelevant information, effectively improve the extraction precision of individual aquaculture ponds, and significantly reduce the edge adhesion of aquaculture ponds in the extraction results, thereby offering new technical support for the automatic extraction of aquaculture ponds in coastal areas. © 2024 by the authors.","aquaculture pond; deep learning; MPG-Net; remote sensing image; segmentation","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85207371299"
"","","","24th International Conference on Medical Image Computing and Computer Assisted Intervention, MICCAI 2021","2021","Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","12901 LNCS","","","","","5669","0","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85116441624&partnerID=40&md5=e7672c0f93dc15cd4ea5b09542cb770b","The proceedings contain 531 papers. The special focus in this conference is on Medical Image Computing and Computer Assisted Intervention. The topics include: TransBTS: Multimodal Brain Tumor Segmentation Using Transformer; automatic Polyp Segmentation via Multi-scale Subtraction Network; patch-Free 3D Medical Image Segmentation Driven by Super-Resolution Technique and Self-Supervised Guidance; progressively Normalized Self-Attention Network for Video Polyp Segmentation; SGNet: Structure-Aware Graph-Based Network for Airway Semantic Segmentation; NucMM Dataset: 3D Neuronal Nuclei Instance Segmentation at Sub-Cubic Millimeter Scale; AxonEM Dataset: 3D Axon Instance Segmentation of Brain Cortical Regions; improved Brain Lesion Segmentation with Anatomical Priors from Healthy Subjects; carveMix: A Simple Data Augmentation Method for Brain Lesion Segmentation; TransFuse: Fusing Transformers and CNNs for Medical Image Segmentation; boundary-Aware Transformers for Skin Lesion Segmentation; A Topological-Attention ConvLSTM Network and Its Application to EM Images; BiX-NAS: Searching Efficient Bi-directional Architecture for Medical Image Segmentation; multi-task, Multi-domain Deep Segmentation with Shared Representations and Contrastive Regularization for Sparse Pediatric Datasets; TEDS-Net: Enforcing Diffeomorphisms in Spatial Transformers to Guarantee Topology Preservation in Segmentations; learning Consistency- and Discrepancy-Context for 2D Organ Segmentation; partially-Supervised Learning for Vessel Segmentation in Ocular Images; unsupervised Network Learning for Cell Segmentation; MT-UDA: Towards Unsupervised Cross-modality Medical Image Segmentation with Limited Source Labels; context-Aware Virtual Adversarial Training for Anatomically-Plausible Segmentation; Pancreas CT Segmentation by Predictive Phenotyping; interactive Segmentation via Deep Learning and B-Spline Explicit Active Surfaces; multi-compound Transformer for Accurate Biomedical Image Segmentation; kCBAC-Net: Deeply Supervised Complete Bipartite Networks with Asymmetric Convolutions for Medical Image Segmentation; Refined Local-imbalance-based Weight for Airway Segmentation in CT.","","Conference review","Final","","Scopus","2-s2.0-85116441624"
"Li Y.; Yan Q.; Zhang K.; Xu H.","Li, Yuenan (55145899000); Yan, Qixin (57221106534); Zhang, Kuangshi (57221086457); Xu, Haoyu (57222574816)","55145899000; 57221106534; 57221086457; 57222574816","Image Reflection Removal via Contextual Feature Fusion Pyramid and Task-Driven Regularization","2022","IEEE Transactions on Circuits and Systems for Video Technology","32","2","","553","565","12","12","10.1109/TCSVT.2021.3067502","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85103242945&doi=10.1109%2fTCSVT.2021.3067502&partnerID=40&md5=c463498c5c72bfaf7072297383395104","In this paper, we propose a deep neural network for single image reflection removal. More specifically, we design a convolutional-grid module and take it as the building block of a feature fusion pyramid. The module leverages the combination effect of the grid topology to create a rich ensemble of receptive fields. Embedding the modules into a pyramidal architecture further expands the coverage of receptive fields. Another benefit of the pyramid is to fuse the multi-scale features learned by the modules locating at the ascending and descending pathways. The rich diversity of features helps the neural network analyze the contexts around overlapping objects at various spatial ranges and harvest the cues for layer separation. The proposed work also exploits useful semantic cues from the hyper-column descriptors generated by a pre-trained VGG-19 model to reduce the ambiguity of layer separation. In light of the low correlation between background and reflection layers, we design a channel-correlation based conditional discriminator to penalize residual reflection. The discriminator uses channel-wise attention to screen the features that can distinguish real background images from estimated ones. This paper also presents a task-driven regularization strategy. The high sensitivity of semantic segmentation to reflection is exploited for assessing the completeness of reflection removal. Training with this regularization strategy can boost the performance of both reflection removal and high-level task. The comparison against state-of-the-art algorithms on four public benchmark datasets demonstrates that this work exhibits superior performance in handling the complex reflections in wild scenarios. The proposed network architecture is also applicable to haze removal, which is another ill-posed layer separation problem, and has shown encouraging performance. © 2021 IEEE.","Feature fusion; Multi-scale feature extraction; Reflection removal; Semantic segmentation; Task-driven regularization","Article","Final","","Scopus","2-s2.0-85103242945"
"Dai Y.; Zhao K.; Shen L.; Liu S.; Yan X.; Li Z.","Dai, Yanshuai (57195476834); Zhao, Kongyang (57223180857); Shen, Li (36555006600); Liu, Shichuan (58303612400); Yan, Xin (57202610950); Li, Zhilin (7409077661)","57195476834; 57223180857; 36555006600; 58303612400; 57202610950; 7409077661","A Siamese Network Combining Multiscale Joint Supervision and Improved Consistency Regularization for Weakly Supervised Building Change Detection","2023","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","16","","","4963","4982","19","8","10.1109/JSTARS.2023.3279863","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85161040277&doi=10.1109%2fJSTARS.2023.3279863&partnerID=40&md5=b9870cfa8e377d24a855f5be4d3a52fd","Building change detection (BCD) from remote sensing images is essential in various practical applications. Recently, inspired by the achievement of deep learning in semantic segmentation (SS), methods that treat the BCD problem as a binary SS task using deep siamese networks have attracted increasing attention. However, similar to their counterparts, these approaches still face the challenge of collecting massive pixel-level annotations. To address this issue, this article presents a novel weakly supervised method for BCD from remote sensing images using image-level labels. The proposed method elaborately designs a siamese network to integrate a multiscale joint supervision (MJS) module and an improved consistency regularization (ICR) module into a unified framework to improve the so-called class activation maps (CAMs), which is vital for producing high-quality pseudomasks using image-level annotations to support pixel-level BCD. To be specific, the MSJ is used for generating refined multiscale CAMs to well capture changes at different scales corresponding to various buildings of varying sizes. The ICR contributes to improving the consistency of CAMs to highlight the boundaries of changed buildings. Extensive experiments on two public BCD datasets have demonstrated that the proposed method outperforms the current state-of-the-art approaches. Furthermore, the visual detection maps also indicate that the proposed method can achieve scale-adaptive change detection results and preserve object boundaries more effectively.  © 2008-2012 IEEE.","Building change detection (BCD); class activation maps (CAM); image-level weakly supervised; improved consistency regularization (ICR); multiscale joint supervision (MJS)","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85161040277"
"Liao Y.; Kang S.; Li J.; Liu Y.; Liu Y.; Dong Z.; Yang B.; Chen X.","Liao, Youqi (58186083600); Kang, Shuhao (58289190100); Li, Jianping (57191420252); Liu, Yang (57997357000); Liu, Yun (57191434431); Dong, Zhen (55578187900); Yang, Bisheng (55578807789); Chen, Xieyuanli (57193685774)","58186083600; 58289190100; 57191420252; 57997357000; 57191434431; 55578187900; 55578807789; 57193685774","Mobile-Seed: Joint Semantic Segmentation and Boundary Detection for Mobile Robots","2024","IEEE Robotics and Automation Letters","9","4","","3902","3909","7","4","10.1109/LRA.2024.3373235","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85187383375&doi=10.1109%2fLRA.2024.3373235&partnerID=40&md5=410e02773594da21c63fcedf7a75482f","Precise and rapid delineation of sharp boundaries and robust semantics is essential for numerous downstream robotic tasks, such as robot grasping and manipulation, real-time semantic mapping, and online sensor calibration performed on edge computing units. Although boundary detection and semantic segmentation are complementary tasks, most studies focus on lightweight models for semantic segmentation but overlook the critical role of boundary detection. In this work, we introduce Mobile-Seed, a lightweight, dual-task framework tailored for simultaneous semantic segmentation and boundary detection. Our framework features a two-stream encoder, an active fusion decoder (AFD) and a dual-task regularization approach. The encoder is divided into two pathways: one captures category-aware semantic information, while the other discerns boundaries from multi-scale features. The AFD module dynamically adapts the fusion of semantic and boundary information by learning channel-wise relationships, allowing for precise weight assignment of each channel. Furthermore, we introduce a regularization loss to mitigate the conflicts in dual-task learning and deep diversity supervision. Compared to existing methods, the proposed Mobile-Seed offers a lightweight framework to simultaneously improve semantic segmentation performance and accurately locate object boundaries. Experiments on the Cityscapes dataset have shown that Mobile-Seed achieves notable improvement over the state-of-the-art (SOTA) baseline by 2.2 percentage points (pp) in mIoU and 4.2 pp in mF-score, while maintaining an online inference speed of 23.9 frames-per-second (FPS) with 1024 × 2048 resolution input on an RTX 2080 Ti GPU. Additional experiments on CamVid and PASCAL Context datasets confirm our method's generalizability. © 2016 IEEE.","Deep learning for visual perception; deep learning methods; visual learning","Article","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85187383375"
"Yang L.; Gu X.; Sun J.","Yang, Liwei (57226598496); Gu, Xiang (57219182794); Sun, Jian (57650520100)","57226598496; 57219182794; 57650520100","Generalized Semantic Segmentation by Self-Supervised Source Domain Projection and Multi-Level Contrastive Learning","2023","Proceedings of the 37th AAAI Conference on Artificial Intelligence, AAAI 2023","37","","","10789","10797","8","3","10.1609/aaai.v37i9.26280","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85168250939&doi=10.1609%2faaai.v37i9.26280&partnerID=40&md5=8b280c090eb91e93dcaf629b1b240416","Deep networks trained on the source domain show degraded performance when tested on unseen target domain data. To enhance the model’s generalization ability, most existing domain generalization methods learn domain invariant features by suppressing domain sensitive features. Different from them, we propose a Domain Projection and Contrastive Learning (DPCL) approach for generalized semantic segmentation, which includes two modules: Self-supervised Source Domain Projection (SSDP) and Multi-Level Contrastive Learning (MLCL). SSDP aims to reduce domain gap by projecting data to the source domain, while MLCL is a learning scheme to learn discriminative and generalizable features on the projected data. During test time, we first project the target data by SSDP to mitigate domain shift, then generate the segmentation results by the learned segmentation network based on MLCL. At test time, we can update the projected data by minimizing our proposed pixel-to-pixel contrastive loss to obtain better results. Extensive experiments for semantic segmentation demonstrate the favorable generalization capability of our method on benchmark datasets. Copyright © 2023, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.","","Conference paper","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85168250939"
"Li B.; Gong A.; Zhang J.; Fu Z.","Li, Boyi (57394440600); Gong, Adu (24536867900); Zhang, Jiaming (59215645400); Fu, Zexin (59216159300)","57394440600; 24536867900; 59215645400; 59216159300","From image-level to pixel-level labeling: A weakly-supervised learning method for identifying aquaculture ponds using iterative anti-adversarial attacks guided by aquaculture features","2024","International Journal of Applied Earth Observation and Geoinformation","132","","104023","","","","0","10.1016/j.jag.2024.104023","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85198370124&doi=10.1016%2fj.jag.2024.104023&partnerID=40&md5=bed3802af6d90947f5d5cd112fef85d9","Aquaculture mapping is essential for monitoring and managing aquaculture resources. However, accurately geotargeting individual aquaculture ponds from medium-resolution remote sensing imagery remains challenging, and convolutional deep learning methods for identifying aquaculture ponds require labor-intensive pixel-level annotations. This paper presents a novel weakly-supervised learning method to derive pixel-level labels from image-level annotations for aquaculture ponds. Our approach uses iterative anti-adversarial attacks to refine localization results from multi-scale class activation maps (CAMs). The improved method integrates two regularization methods guided by aquaculture features to form a joint loss function for adversarial samples: discriminative water region suppression and non-aquaculture water class suppression. We also propose an aquaculture feature termed CFNDWI to constrain the localization results and generate high-quality pseudo-labels. As a result, the pseudo-labels are used to train semantic segmentation networks for accurately identifying aquaculture ponds. We evaluated the performance of our method using commonly-used backbones on 10 m Sentinel-2 imagery. Our method achieves Intersection over Union (IoU) values of 0.618–0.655 for pseudo-label generation, and IoU values of 0.664–0.708 for semantic segmentation, outperforming state-of-the-art weakly-supervised methods and public datasets. The effectiveness of each module of our method was also testified through ablation experiments. Our method leverages knowledge-driven aquaculture features to guide the data-driven adversarial learning process, addressing the lack of high-quality aquaculture datasets for model training. The code for implementing our method will be accessible at https://github.com/designer1024/WSLM-AQ. © 2024 The Authors","Adversarial learning; Aquaculture pond; Class activation map; Pseudo-label; Weakly-supervised learning","Article","Final","","Scopus","2-s2.0-85198370124"
"Deng R.; Liu Q.; Cui C.; Yao T.; Long J.; Asad Z.; Womick R.M.; Zhu Z.; Fogo A.B.; Zhao S.; Yang H.; Huo Y.","Deng, Ruining (57219448253); Liu, Quan (57336485300); Cui, Can (57219469236); Yao, Tianyuan (57222525765); Long, Jun (35756419000); Asad, Zuhayr (57249905900); Womick, R. Michael (57786748800); Zhu, Zheyu (57219448236); Fogo, Agnes B. (7102673654); Zhao, Shilin (56415384300); Yang, Haichun (8638224800); Huo, Yuankai (56830058500)","57219448253; 57336485300; 57219469236; 57222525765; 35756419000; 57249905900; 57786748800; 57219448236; 7102673654; 56415384300; 8638224800; 56830058500","Omni-Seg: A Scale-Aware Dynamic Network for Renal Pathological Image Segmentation","2023","IEEE Transactions on Biomedical Engineering","70","9","","2636","2644","8","12","10.1109/TBME.2023.3260739","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85151563110&doi=10.1109%2fTBME.2023.3260739&partnerID=40&md5=3cd97b671579b9f962bd5d7bc26c644c","Comprehensive semantic segmentation on renal pathological images is challenging due to the heterogeneous scales of the objects. For example, on a whole slide image (WSI), the cross-sectional areas of glomeruli can be 64 times larger than that of the peritubular capillaries, making it impractical to segment both objects on the same patch, at the same scale. To handle this scaling issue, prior studies have typically trained multiple segmentation networks in order to match the optimal pixel resolution of heterogeneous tissue types. This multi-network solution is resource-intensive and fails to model the spatial relationship between tissue types. In this article, we propose the Omni-Seg network, a scale-aware dynamic neural network that achieves multi-object (six tissue types) and multi-scale (5× to 40× scale) pathological image segmentation via a single neural network. The contribution of this article is three-fold: (1) a novel scale-aware controller is proposed to generalize the dynamic neural network from single-scale to multi-scale; (2) semi-supervised consistency regularization of pseudo-labels is introduced to model the inter-scale correlation of unannotated tissue types into a single end-to-end learning paradigm; and (3) superior scale-aware generalization is evidenced by directly applying a model trained on human kidney images to mouse kidney images, without retraining. By learning from 150,000 human pathological image patches from six tissue types at three different resolutions, our approach achieved superior segmentation performance according to human visual assessment and evaluation of image-omics (i.e., spatial transcriptomics).  © 1964-2012 IEEE.","image segmentation; multi-label; multi-scale; Renal pathology; semi-supervised learning","Article","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85151563110"
"","","","9th International Conference on Image and Graphics, ICIG 2017","2017","Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","10666 LNCS","","","","","1955","0","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85040234508&partnerID=40&md5=5488f2a2fc2df796c7de9a94e59a746b","The proceedings contain 172 papers. The special focus in this conference is on Image and Graphics. The topics include: New tikhonov regularization for blind image restoration; real-time multi-camera video stitching based on improved optimal stitch line and multi-resolution fusion; image quality assessment of enriched tonal levels images; a variational model to extract texture from noisy image data with local variance constraints; Joint visualization of UKF tractography data; semantic segmentation based automatic two-tone portrait synthesis; Parameters sharing multi-items non-parametric factor microfacet model for isotropic and anisotropic BRDFs; SRG and RMSE-based automated segmentation for volume data; shape recovery of endoscopic videos by shape from shading using mesh regularization; RGB-D saliency detection with multi-feature-fused optimization; lazy recoloring; Similar trademark image retrieval integrating LBP and convolutional neural network; adaptive learning compressive tracking based on Kalman filter; Online high-accurate calibration of RGB+3D-LiDAR for autonomous driving; run-based connected components labeling using double-row scan; a 3D tube-object centerline extraction algorithm based on steady fluid dynamics; Moving objects detection in video sequences captured by a PTZ camera; Fast grid-based fluid dynamics simulation with conservation of momentum and kinetic energy on GPU; adaptive density optimization of lattice structures sustaining the external multi-load; hyperspectral image classification based on deep forest and spectral-spatial cooperative feature; research on color image segmentation; hyperspectral image classification using multi vote strategy on convolutional neural network and sparse representation joint feature; efficient deep belief network based hyperspectral image classification; classification of hyperspectral imagery based on dictionary learning and extended multi-attribute profiles; sparse acquisition integral imaging system.","","Conference review","Final","","Scopus","2-s2.0-85040234508"
"Lyu Y.; Vosselman G.; Xia G.-S.; Yilmaz A.; Yang M.Y.","Lyu, Ye (57213191151); Vosselman, George (6602209960); Xia, Gui-Song (12781686200); Yilmaz, Alper (57067174700); Yang, Michael Ying (36015861500)","57213191151; 6602209960; 12781686200; 57067174700; 36015861500","UAVid: A semantic segmentation dataset for UAV imagery","2020","ISPRS Journal of Photogrammetry and Remote Sensing","165","","","108","119","11","237","10.1016/j.isprsjprs.2020.05.009","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85085523681&doi=10.1016%2fj.isprsjprs.2020.05.009&partnerID=40&md5=2b851fa177dfe99a67d846606c1d4e90","Semantic segmentation has been one of the leading research interests in computer vision recently. It serves as a perception foundation for many fields, such as robotics and autonomous driving. The fast development of semantic segmentation attributes enormously to the large scale datasets, especially for the deep learning related methods. There already exist several semantic segmentation datasets for comparison among semantic segmentation methods in complex urban scenes, such as the Cityscapes and CamVid datasets, where the side views of the objects are captured with a camera mounted on the driving car. There also exist semantic labeling datasets for the airborne images and the satellite images, where the nadir views of the objects are captured. However, only a few datasets capture urban scenes from an oblique Unmanned Aerial Vehicle (UAV) perspective, where both of the top view and the side view of the objects can be observed, providing more information for object recognition. In this paper, we introduce our UAVid dataset, a new high-resolution UAV semantic segmentation dataset as a complement, which brings new challenges, including large scale variation, moving object recognition and temporal consistency preservation. Our UAV dataset consists of 30 video sequences capturing high-resolution images in oblique views. In total, 300 images have been densely labeled with 8 classes for the semantic labeling task. We have provided several deep learning baseline methods with pre-training, among which the proposed Multi-Scale-Dilation net performs the best via multi-scale feature extraction, reaching a mean intersection-over-union (IoU) score around 50%. We have also explored the influence of spatial-temporal regularization for sequence data by leveraging on feature space optimization (FSO) and 3D conditional random field (CRF). Our UAVid website and the labeling tool have been published online (https://uavid.nl/). © 2020 International Society for Photogrammetry and Remote Sensing, Inc. (ISPRS)","Dataset; Deep learning; Semantic segmentation; UAV","Article","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85085523681"
"De Oliveira J.P.; Costa M.G.F.; Costa Filho C.F.F.","De Oliveira, Joel Parente (57222475915); Costa, Marly Guimarães Fernandes (55252953100); Costa Filho, Cícero Ferreira Fernandes (6506784770)","57222475915; 55252953100; 6506784770","Methodology of data fusion using deep learning for semantic segmentation of land types in the amazon","2020","IEEE Access","8","","","187864","187875","11","9","10.1109/ACCESS.2020.3031533","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85102839559&doi=10.1109%2fACCESS.2020.3031533&partnerID=40&md5=980d9343ed37eff4b3938711b96aae73","This study proposes a methodology using deep learning and a multi-resolution segmentation algorithm to perform the semantic segmentation of remote sensing images. Initially the image is segmented using a CNN, and then an image with homogeneous regions is generated using a multi-resolution segmentation algorithm. Finally, a data fusion process is performed with these two images, generating the final classified image. The field of study was the Brazilian Amazon region. The proposed methodology classifies images in the following classes: forest, pasture and agriculture. The input data used were LANDSAT-8/OLI images. The reference data were extracted from the results of the TerraClass project in 2014. Two datasets were evaluated: the first with six bands and the second with three bands. Three CNN architectures were evaluated together with three optimization methods: SGDM, ADAM, and RMSProp and the dropout and L2 regularization methods as methods for generalization improvement. The best model, CNN C optimization method C technique for generalization improvement, evaluated in the validation set, was submitted to a 5-fold cross validation methodology, and the results were compared with pre-trained networks using the learning transfer methodology; in this case the networks used for comparison were ResNet50, InceptionResnetv2, MobileNetv2 and Xception. The proposed methodology was evaluated through image segmentation of some regions of the Amazon. Finally, the proposed methodology was evaluated in regions used by other authors. The accuracy values obtained for the images evaluated were over 99%. © 2020 Institute of Electrical and Electronics Engineers Inc.. All rights reserved.","Convolutional neural networks; Deep learning; Image segmentation; Remote sensing","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85102839559"
"Boutillon A.; Conze P.-H.; Pons C.; Burdin V.; Borotikar B.","Boutillon, Arnaud (57211439501); Conze, Pierre-Henri (55195063200); Pons, Christelle (55752507200); Burdin, Valérie (55026725400); Borotikar, Bhushan (23092872300)","57211439501; 55195063200; 55752507200; 55026725400; 23092872300","Generalizable multi-task, multi-domain deep segmentation of sparse pediatric imaging datasets via multi-scale contrastive regularization and multi-joint anatomical priors","2022","Medical Image Analysis","81","","102556","","","","16","10.1016/j.media.2022.102556","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85136460725&doi=10.1016%2fj.media.2022.102556&partnerID=40&md5=5cbba9e2aab5576693c495aa4796c38b","Clinical diagnosis of the pediatric musculoskeletal system relies on the analysis of medical imaging examinations. In the medical image processing pipeline, semantic segmentation using deep learning algorithms enables an automatic generation of patient-specific three-dimensional anatomical models which are crucial for morphological evaluation. However, the scarcity of pediatric imaging resources may result in reduced accuracy and generalization performance of individual deep segmentation models. In this study, we propose to design a novel multi-task, multi-domain learning framework in which a single segmentation network is optimized over the union of multiple datasets arising from distinct parts of the anatomy. Unlike previous approaches, we simultaneously consider multiple intensity domains and segmentation tasks to overcome the inherent scarcity of pediatric data while leveraging shared features between imaging datasets. To further improve generalization capabilities, we employ a transfer learning scheme from natural image classification, along with a multi-scale contrastive regularization aimed at promoting domain-specific clusters in the shared representations, and multi-joint anatomical priors to enforce anatomically consistent predictions. We evaluate our contributions for performing bone segmentation using three scarce and pediatric imaging datasets of the ankle, knee, and shoulder joints. Our results demonstrate that the proposed approach outperforms individual, transfer, and shared segmentation schemes in Dice metric with statistically sufficient margins. The proposed model brings new perspectives towards intelligent use of imaging resources and better management of pediatric musculoskeletal disorders. © 2022 Elsevier B.V.","Attention models; Contrastive learning; Domain adaptation; Musculoskeletal system; Shape priors; Universal representations","Article","Final","All Open Access; Bronze Open Access; Green Open Access","Scopus","2-s2.0-85136460725"
